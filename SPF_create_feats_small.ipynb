{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m4_feats_polars import *\n",
    "from m5_sb_models import lgb_pipeline\n",
    "import polars as pl\n",
    "import os, warnings\n",
    "from m4_small_feats import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_STORE = 'feat_small'\n",
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n",
    "test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n",
    "train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')\n",
    "train_scores = train_scores.collect().to_pandas()\n",
    "\n",
    "param = {'n_estimators': 700,\n",
    "        'learning_rate': 0.005,\n",
    "        'metric': 'rmse',\n",
    "        'force_col_wise': True,\n",
    "        'verbosity': -1,}\n",
    "\n",
    "feat_list = os.listdir(FEAT_STORE)\n",
    "list_train_feats = [feat for feat in feat_list if feat.startswith('train_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'down_events_counts_three'\n",
    "# tr,ts = down_events_counts_three(train_logs, test_logs, n_events=20)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'down_events_counts_two'\n",
    "# tr,ts = down_events_counts_two(train_logs, test_logs, n_events=20)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'down_events_counts_three'\n",
    "# tr,ts = down_events_counts_three(train_logs, test_logs, n_events=20)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'down_events_counts_one'\n",
    "# tr,ts = down_events_counts_one(train_logs, test_logs, n_events=20)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'create_pauses'\n",
    "# tr,ts = create_pauses(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'cursor_pos_acceleration_basic'\n",
    "# tr,ts = cursor_pos_acceleration_basic(train_logs, test_logs, time_agg=8)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'cursor_pos_acceleration_adv'\n",
    "# tr,ts = cursor_pos_acceleration_adv(train_logs, test_logs, time_agg=8)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'p_burst_feats_basic'\n",
    "# tr,ts = p_burst_feats_basic(train_logs, test_logs, time_agg=2)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'p_burst_feats_adv'\n",
    "# tr,ts = p_burst_feats_adv(train_logs, test_logs, time_agg=2)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'r_burst_feats_basic'\n",
    "# tr,ts = r_burst_feats_basic(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'r_burst_feats_adv'\n",
    "# tr,ts = r_burst_feats_adv(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'events_counts_acceleration_basic'\n",
    "# tr,ts = events_counts_acceleration_basic(train_logs, test_logs, time_agg=4)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'events_counts_acceleration_adv'\n",
    "# tr,ts = events_counts_acceleration_adv(train_logs, test_logs, time_agg=4)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'word_count_acceleration_basic'\n",
    "# tr,ts = word_count_acceleration_basic(train_logs, test_logs, time_agg=8)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'word_count_acceleration_adv'\n",
    "# tr,ts = word_count_acceleration_adv(train_logs, test_logs, time_agg=8)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'add_word_pauses_basic'\n",
    "# tr,ts = add_word_pauses_basic(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'add_word_pauses_adv'\n",
    "# tr,ts = add_word_pauses_adv(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'remove_word_pauses_basic'\n",
    "# tr,ts = remove_word_pauses_basic(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'remove_word_pauses_adv'\n",
    "# tr,ts = remove_word_pauses_adv(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'word_timings_basic'\n",
    "# tr,ts = word_timings_basic(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'word_timings_adv'\n",
    "# tr,ts = word_timings_adv(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'categorical_nunique'\n",
    "# tr,ts = categorical_nunique(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "\n",
    "\n",
    "# train_essays = get_essay_df(train_logs.collect().to_pandas())\n",
    "# test_essays = get_essay_df(test_logs.collect().to_pandas())\n",
    "# file_name = 'essay_sent_words'\n",
    "# tr,ts = essay_sent_words(train_essays), essay_sent_words(test_essays)\n",
    "# tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'essay_sent_length'\n",
    "# tr,ts = essay_sent_length(train_essays), essay_sent_length(test_essays)\n",
    "# tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'essay_par_length'\n",
    "# tr,ts = essay_par_length(train_essays), essay_par_length(test_essays)\n",
    "# tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'essay_par_words'\n",
    "# tr,ts = essay_par_words(train_essays), essay_par_words(test_essays)\n",
    "# tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'essay_sents_per_par_basic'\n",
    "# tr,ts = essay_sents_per_par_basic(train_essays), essay_sents_per_par_basic(test_essays)\n",
    "# tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'essay_sents_per_par_adv'\n",
    "# tr,ts = essay_sents_per_par_adv(train_essays), essay_sents_per_par_adv(test_essays)\n",
    "# tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_one_one'\n",
    "# tr,ts = countvectorize_one_one(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_one_two'\n",
    "# tr,ts = countvectorize_one_two(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_one_three'\n",
    "# tr,ts = countvectorize_one_three(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_one_four'\n",
    "# tr,ts = countvectorize_one_four(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_two_one'\n",
    "# tr,ts = countvectorize_two_one(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_two_one'\n",
    "# tr,ts = countvectorize_two_one(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'countvectorize_two_two'\n",
    "# tr,ts = countvectorize_two_two(train_essays, test_essays)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting round 0 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.7341\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.8962\n",
      "Training... train_categorical_nunique.pkl. Score: 0.9037\n",
      "Training... train_word_timings_basic.pkl. Score: 0.7370\n",
      "Training... train_word_timings_adv.pkl. Score: 0.9494\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.7990\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.8430\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.8287\n",
      "Training... train_essay_sent_words.pkl. Score: 0.7155\n",
      "Training... train_create_pauses.pkl. Score: 0.7308\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.9331\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.8369\n",
      "Training... train_down_events_counts_two.pkl. Score: 0.8172\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.7927\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6776\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.9346\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.8933\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.8612\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.8059\n",
      "Training... train_countvectorize_one_three.pkl. Score: 1.0247\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.7291\n",
      "Training... train_down_events_counts_one.pkl. Score: 0.6679\n",
      "Training... train_countvectorize_one_four.pkl. Score: 1.0247\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.8029\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.9757\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.9402\n",
      "Training... train_cursor_pos_acceleration_basic.pkl. Score: 0.9153\n",
      "Training... train_essay_par_words.pkl. Score: 0.7523\n",
      "Training... train_countvectorize_one_two.pkl. Score: 0.8748\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.8834\n",
      "Training... train_essay_par_length.pkl. Score: 0.6962\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.9830\n",
      "Results improved!: Selected feat train_down_events_counts_one.pkl - score 0.6679\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_create_pauses.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_down_events_counts_two.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_par_words.pkl', 'train_countvectorize_one_two.pkl', 'train_p_burst_feats_adv.pkl', 'train_essay_par_length.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl']\n",
      "best feat: train_down_events_counts_one.pkl\n",
      "Starting round 1 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6675\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6616\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6644\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6675\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6680\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6621\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.6555\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6644\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6552\n",
      "Training... train_create_pauses.pkl. Score: 0.6521\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6550\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6570\n",
      "Training... train_down_events_counts_two.pkl. Score: 0.6564\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6669\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6496\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6677\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6620\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6631\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6633\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6673\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6603\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6675\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6618\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6623\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6661\n",
      "Training... train_cursor_pos_acceleration_basic.pkl. Score: 0.6623\n",
      "Training... train_essay_par_words.pkl. Score: 0.6585\n",
      "Training... train_countvectorize_one_two.pkl. Score: 0.6592\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6691\n",
      "Training... train_essay_par_length.pkl. Score: 0.6466\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6665\n",
      "Results improved!: Selected feat train_essay_par_length.pkl - score 0.6466\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_create_pauses.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_down_events_counts_two.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_par_words.pkl', 'train_countvectorize_one_two.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl']\n",
      "best feat: train_essay_par_length.pkl\n",
      "Starting round 2 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6462\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6430\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6433\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6477\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6483\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6444\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.6438\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6449\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6444\n",
      "Training... train_create_pauses.pkl. Score: 0.6339\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6438\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6374\n",
      "Training... train_down_events_counts_two.pkl. Score: 0.6404\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6450\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6408\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6453\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6425\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6416\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6449\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6475\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6399\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6467\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6410\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6448\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6450\n",
      "Training... train_cursor_pos_acceleration_basic.pkl. Score: 0.6424\n",
      "Training... train_essay_par_words.pkl. Score: 0.6446\n",
      "Training... train_countvectorize_one_two.pkl. Score: 0.6347\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6476\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6461\n",
      "Results improved!: Selected feat train_create_pauses.pkl - score 0.6339\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_down_events_counts_two.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_par_words.pkl', 'train_countvectorize_one_two.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl']\n",
      "best feat: train_create_pauses.pkl\n",
      "Starting round 3 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6347\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6331\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6317\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6358\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6359\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6344\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.6311\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6343\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6325\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6323\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6329\n",
      "Training... train_down_events_counts_two.pkl. Score: 0.6306\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6342\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6286\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6331\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6327\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6335\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6333\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6338\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6342\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6345\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6330\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6340\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6331\n",
      "Training... train_cursor_pos_acceleration_basic.pkl. Score: 0.6327\n",
      "Training... train_essay_par_words.pkl. Score: 0.6331\n",
      "Training... train_countvectorize_one_two.pkl. Score: 0.6212\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6355\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6336\n",
      "Results improved!: Selected feat train_countvectorize_one_two.pkl - score 0.6212\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_down_events_counts_two.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl']\n",
      "best feat: train_countvectorize_one_two.pkl\n",
      "Starting round 4 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6217\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6203\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6196\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6222\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6221\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6205\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.6190\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6202\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6190\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6194\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6199\n",
      "Training... train_down_events_counts_two.pkl. Score: 0.6155\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6199\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6179\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6207\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6189\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6198\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6196\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6207\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6206\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6211\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6198\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6207\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6200\n",
      "Training... train_cursor_pos_acceleration_basic.pkl. Score: 0.6181\n",
      "Training... train_essay_par_words.pkl. Score: 0.6198\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6214\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6208\n",
      "Results improved!: Selected feat train_down_events_counts_two.pkl - score 0.6155\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl']\n",
      "best feat: train_down_events_counts_two.pkl\n",
      "Starting round 5 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6147\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6162\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6144\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6165\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6168\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6146\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.6134\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6148\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6138\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6144\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6149\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6148\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6134\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6155\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6148\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6143\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6140\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6154\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6156\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6156\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6146\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6156\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6153\n",
      "Training... train_cursor_pos_acceleration_basic.pkl. Score: 0.6132\n",
      "Training... train_essay_par_words.pkl. Score: 0.6138\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6152\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6157\n",
      "Results improved!: Selected feat train_cursor_pos_acceleration_basic.pkl - score 0.6132\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl']\n",
      "best feat: train_cursor_pos_acceleration_basic.pkl\n",
      "Starting round 6 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6130\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6137\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6127\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6136\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6140\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6129\n",
      "Training... train_essay_sents_per_par_basic.pkl. Score: 0.6116\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6136\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6127\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6123\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6134\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6128\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6116\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6135\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6124\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6135\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6118\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6129\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6136\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6123\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6135\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6134\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6131\n",
      "Training... train_essay_par_words.pkl. Score: 0.6119\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6129\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6134\n",
      "Results improved!: Selected feat train_essay_sents_per_par_basic.pkl - score 0.6116\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl']\n",
      "best feat: train_essay_sents_per_par_basic.pkl\n",
      "Starting round 7 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6118\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6124\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6113\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6119\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6124\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6116\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6120\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6115\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6112\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6112\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6114\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6104\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6116\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6111\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6114\n",
      "Training... train_countvectorize_two_one.pkl. Score: 0.6094\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6115\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6118\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6113\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6121\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6119\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6116\n",
      "Training... train_essay_par_words.pkl. Score: 0.6110\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6123\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6117\n",
      "Results improved!: Selected feat train_countvectorize_two_one.pkl - score 0.6094\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_remove_word_pauses_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl']\n",
      "best feat: train_countvectorize_two_one.pkl\n",
      "Starting round 8 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6095\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6102\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6094\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6100\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6103\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6095\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6100\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6097\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6097\n",
      "Training... train_remove_word_pauses_adv.pkl. Score: 0.6089\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6103\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6093\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6100\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6093\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6095\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6093\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6100\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6094\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6101\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6102\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6100\n",
      "Training... train_essay_par_words.pkl. Score: 0.6094\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6102\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6105\n",
      "Results improved!: Selected feat train_remove_word_pauses_adv.pkl - score 0.6089\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_add_word_pauses_adv.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_remove_word_pauses_adv.pkl']\n",
      "best feat: train_remove_word_pauses_adv.pkl\n",
      "Starting round 9 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6088\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6100\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6091\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6094\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6099\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6089\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6098\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6092\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6099\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6092\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6089\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6092\n",
      "Training... train_add_word_pauses_adv.pkl. Score: 0.6084\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6090\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6092\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6100\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6093\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6092\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6096\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6097\n",
      "Training... train_essay_par_words.pkl. Score: 0.6088\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6098\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6098\n",
      "Results improved!: Selected feat train_add_word_pauses_adv.pkl - score 0.6084\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_word_count_acceleration_adv.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_remove_word_pauses_adv.pkl', 'train_add_word_pauses_adv.pkl']\n",
      "best feat: train_add_word_pauses_adv.pkl\n",
      "Starting round 10 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6082\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6094\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6077\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6087\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6087\n",
      "Training... train_word_count_acceleration_adv.pkl. Score: 0.6076\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6090\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6080\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6088\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6080\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6087\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6083\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6082\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6083\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6091\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6082\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6087\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6089\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6092\n",
      "Training... train_essay_par_words.pkl. Score: 0.6081\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6089\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6089\n",
      "Results improved!: Selected feat train_word_count_acceleration_adv.pkl - score 0.6076\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_remove_word_pauses_adv.pkl', 'train_add_word_pauses_adv.pkl', 'train_word_count_acceleration_adv.pkl']\n",
      "best feat: train_word_count_acceleration_adv.pkl\n",
      "Starting round 11 of training feats\n",
      "Training... train_p_burst_feats_basic.pkl. Score: 0.6086\n",
      "Training... train_add_word_pauses_basic.pkl. Score: 0.6086\n",
      "Training... train_categorical_nunique.pkl. Score: 0.6079\n",
      "Training... train_word_timings_basic.pkl. Score: 0.6087\n",
      "Training... train_word_timings_adv.pkl. Score: 0.6090\n",
      "Training... train_cursor_pos_acceleration_adv.pkl. Score: 0.6083\n",
      "Training... train_essay_sent_words.pkl. Score: 0.6080\n",
      "Training... train_essay_sents_per_par_adv.pkl. Score: 0.6087\n",
      "Training... train_events_counts_acceleration_adv.pkl. Score: 0.6077\n",
      "Training... train_essay_sent_length.pkl. Score: 0.6087\n",
      "Training... train_countvectorize_two_two.pkl. Score: 0.6082\n",
      "Training... train_events_counts_acceleration_basic.pkl. Score: 0.6083\n",
      "Training... train_countvectorize_one_three.pkl. Score: 0.6082\n",
      "Training... train_remove_word_pauses_basic.pkl. Score: 0.6086\n",
      "Training... train_countvectorize_one_four.pkl. Score: 0.6081\n",
      "Training... train_word_count_acceleration_basic.pkl. Score: 0.6079\n",
      "Training... train_down_events_counts_three.pkl. Score: 0.6083\n",
      "Training... train_r_burst_feats_basic.pkl. Score: 0.6087\n",
      "Training... train_essay_par_words.pkl. Score: 0.6077\n",
      "Training... train_p_burst_feats_adv.pkl. Score: 0.6091\n",
      "Training... train_r_burst_feats_adv.pkl. Score: 0.6085\n",
      "Training Over!\n",
      "list_train_feats: ['train_p_burst_feats_basic.pkl', 'train_add_word_pauses_basic.pkl', 'train_categorical_nunique.pkl', 'train_word_timings_basic.pkl', 'train_word_timings_adv.pkl', 'train_cursor_pos_acceleration_adv.pkl', 'train_essay_sent_words.pkl', 'train_essay_sents_per_par_adv.pkl', 'train_events_counts_acceleration_adv.pkl', 'train_essay_sent_length.pkl', 'train_countvectorize_two_two.pkl', 'train_events_counts_acceleration_basic.pkl', 'train_countvectorize_one_three.pkl', 'train_remove_word_pauses_basic.pkl', 'train_countvectorize_one_four.pkl', 'train_word_count_acceleration_basic.pkl', 'train_down_events_counts_three.pkl', 'train_r_burst_feats_basic.pkl', 'train_essay_par_words.pkl', 'train_p_burst_feats_adv.pkl', 'train_r_burst_feats_adv.pkl']\n",
      "added_feats_list: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_remove_word_pauses_adv.pkl', 'train_add_word_pauses_adv.pkl', 'train_word_count_acceleration_adv.pkl']\n",
      "best feat: train_word_count_acceleration_adv.pkl\n",
      "Best RMSE: 0.6076\n",
      "Best Feature Set: ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', 'train_remove_word_pauses_adv.pkl', 'train_add_word_pauses_adv.pkl', 'train_word_count_acceleration_adv.pkl']\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty DataFrames\n",
    "best_feats = []\n",
    "train_feats, test_feats = pd.DataFrame(), pd.DataFrame()\n",
    "best_rmse = float('inf')\n",
    "round = 0\n",
    "added_feats = []\n",
    "improved = True\n",
    "results = pd.DataFrame()\n",
    "list_train_feats = [feat for feat in feat_list if feat.startswith('train_')]\n",
    "\n",
    "while improved:\n",
    "    print(f'Starting round {round} of training feats')\n",
    "    improved = False\n",
    "    list_train_feats = [feat for feat in list_train_feats if feat not in added_feats]\n",
    "\n",
    "    for tr_feats_cand in list_train_feats:\n",
    "        ts_feats_cand = tr_feats_cand.replace('train', 'test')\n",
    "\n",
    "        tr_feats = pd.read_pickle(f'{FEAT_STORE}/{tr_feats_cand}')\n",
    "        ts_feats = pd.read_pickle(f'{FEAT_STORE}/{ts_feats_cand}')\n",
    "\n",
    "        existing_train_columns = set(train_feats.columns)\n",
    "        existing_test_columns = set(test_feats.columns)\n",
    "\n",
    "        if not(train_feats.empty & test_feats.empty):\n",
    "\n",
    "            train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "            test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "\n",
    "            if 'score' not in train_feats.columns:\n",
    "                train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "                \n",
    "            assert train_feats.shape[1] == test_feats.shape[1] + 1\n",
    "        else:\n",
    "            #print(f'feats empty - setting up train_feats')\n",
    "            train_feats = tr_feats\n",
    "            test_feats = ts_feats\n",
    "            train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "        \n",
    "        # print(f'Train feats cols {train_feats.columns}')\n",
    "        tr_cols = tr_feats.drop(columns=['id']).columns\n",
    "        ts_cols = ts_feats.drop(columns=['id']).columns\n",
    "        train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n",
    "        mean_rmse = []\n",
    "\n",
    "        for i in range(15):\n",
    "            train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n",
    "            test_preds, valid_preds, final_rmse, cv_rm = lgb_pipeline(train_feats, test_feats, param)\n",
    "            mean_rmse.append(final_rmse)\n",
    "\n",
    "        print(f'Training... {tr_feats_cand}. Score: {np.mean(mean_rmse):.4f}')\n",
    "        temp_res = {'feat_name': tr_feats_cand, 'RMSE': np.mean(mean_rmse)}\n",
    "        results = pd.concat([results, pd.DataFrame([temp_res])])\n",
    "\n",
    "        train_feats.drop(columns=tr_cols, inplace=True)\n",
    "        test_feats.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "    results = results.sort_values('RMSE', ascending=True)\n",
    "    top_score = results.head(1).RMSE.values[0]\n",
    "    top_feat = results.head(1).feat_name.values[0]\n",
    "\n",
    "    if top_score < best_rmse:\n",
    "        best_rmse = top_score\n",
    "        best_feat = top_feat\n",
    "        improved = True\n",
    "        print(f'Results improved!: Selected feat {top_feat} - score {top_score:.4f}')\n",
    "\n",
    "        ts_top_feat = top_feat.replace('train', 'test')\n",
    "        tr_feats = pd.read_pickle(f'{FEAT_STORE}/{top_feat}')\n",
    "        ts_feats = pd.read_pickle(f'{FEAT_STORE}/{ts_top_feat}')\n",
    "\n",
    "        if round > 0:\n",
    "            train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "            test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "        else:\n",
    "            train_feats = tr_feats\n",
    "            test_feats = ts_feats\n",
    "\n",
    "        added_feats.append(top_feat)\n",
    "        round += 1\n",
    "    else:\n",
    "        print('Training Over!')\n",
    "\n",
    "    list_train_feats = [feat for feat in list_train_feats if feat not in added_feats]\n",
    "    print(f'list_train_feats: {list_train_feats}')\n",
    "    print(f'added_feats_list: {added_feats}')\n",
    "    print(f'best feat: {top_feat}')\n",
    "\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Best Feature Set: {added_feats}\")\n",
    "best_feats.append(added_feats)\n",
    "added_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>down_event_one_1</th>\n",
       "      <th>down_event_one_2</th>\n",
       "      <th>down_event_one_3</th>\n",
       "      <th>down_event_one_4</th>\n",
       "      <th>down_event_one_5</th>\n",
       "      <th>down_event_one_6</th>\n",
       "      <th>down_event_one_7</th>\n",
       "      <th>down_event_one_8</th>\n",
       "      <th>down_event_one_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_per_par_max</th>\n",
       "      <th>sent_per_par_first</th>\n",
       "      <th>sent_per_par_last</th>\n",
       "      <th>sent_per_par_q1</th>\n",
       "      <th>sent_per_par_median_x</th>\n",
       "      <th>sent_per_par_q3</th>\n",
       "      <th>sent_per_par_count_y</th>\n",
       "      <th>sent_per_par_mean_y</th>\n",
       "      <th>sent_per_par_std</th>\n",
       "      <th>sent_per_par_median_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d9474286</td>\n",
       "      <td>363</td>\n",
       "      <td>7</td>\n",
       "      <td>205</td>\n",
       "      <td>1516</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>1.169045</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b5285c39</td>\n",
       "      <td>217</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1591</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>501</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5.75</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>4</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f17fa30e</td>\n",
       "      <td>500</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2791</td>\n",
       "      <td>46</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>537</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>3</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a0eaf617</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>1775</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1acb2b61</td>\n",
       "      <td>272</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>2385</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>522</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>3.714835</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>394c1342</td>\n",
       "      <td>937</td>\n",
       "      <td>9</td>\n",
       "      <td>63</td>\n",
       "      <td>2231</td>\n",
       "      <td>233</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>516</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>1.914854</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>cb24e2f0</td>\n",
       "      <td>231</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>2421</td>\n",
       "      <td>41</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>550</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>1.516575</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>f5d8ccf9</td>\n",
       "      <td>140</td>\n",
       "      <td>8</td>\n",
       "      <td>82</td>\n",
       "      <td>1827</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>401</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>1.788854</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>0432f117</td>\n",
       "      <td>272</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>2285</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>437</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.75</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>2.753785</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fcd51d95</td>\n",
       "      <td>521</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>2620</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>558</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>1.303840</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows  62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  down_event_one_1  down_event_one_2  down_event_one_3  \\\n",
       "0     d9474286               363                 7               205   \n",
       "1     b5285c39               217                11                 0   \n",
       "2     f17fa30e               500                 4                28   \n",
       "3     a0eaf617               112                 4                36   \n",
       "4     1acb2b61               272                 8                73   \n",
       "...        ...               ...               ...               ...   \n",
       "2466  394c1342               937                 9                63   \n",
       "2467  cb24e2f0               231                 8                70   \n",
       "2468  f5d8ccf9               140                 8                82   \n",
       "2469  0432f117               272                 3                34   \n",
       "2470  fcd51d95               521                31                 6   \n",
       "\n",
       "      down_event_one_4  down_event_one_5  down_event_one_6  down_event_one_7  \\\n",
       "0                 1516                17                26                 2   \n",
       "1                 1591                41                23                 0   \n",
       "2                 2791                46                25                 0   \n",
       "3                 1775                15                18                 0   \n",
       "4                 2385                29                35                 6   \n",
       "...                ...               ...               ...               ...   \n",
       "2466              2231               233                22                 0   \n",
       "2467              2421                41                26                 0   \n",
       "2468              1827                20                32                 0   \n",
       "2469              2285                45                23                 2   \n",
       "2470              2620                72                35                 5   \n",
       "\n",
       "      down_event_one_8  down_event_one_9  ...  sent_per_par_max  \\\n",
       "0                  315                 0  ...                 5   \n",
       "1                  501                 0  ...                 9   \n",
       "2                  537                 4  ...                12   \n",
       "3                  370                 0  ...                 7   \n",
       "4                  522                 0  ...                12   \n",
       "...                ...               ...  ...               ...   \n",
       "2466               516                 0  ...                 8   \n",
       "2467               550                 4  ...                 7   \n",
       "2468               401                 0  ...                 9   \n",
       "2469               437                 1  ...                10   \n",
       "2470               558                 0  ...                 9   \n",
       "\n",
       "      sent_per_par_first  sent_per_par_last  sent_per_par_q1  \\\n",
       "0                      5                  2             4.00   \n",
       "1                      9                  5             5.75   \n",
       "2                      4                  5             4.50   \n",
       "3                      6                  7             6.00   \n",
       "4                     10                 12            10.00   \n",
       "...                  ...                ...              ...   \n",
       "2466                   4                  8             4.00   \n",
       "2467                   7                  5             5.00   \n",
       "2468                   8                  5             5.00   \n",
       "2469                   5                  4             4.75   \n",
       "2470                   9                  8             6.00   \n",
       "\n",
       "      sent_per_par_median_x  sent_per_par_q3  sent_per_par_count_y  \\\n",
       "0                       4.5             5.00                     6   \n",
       "1                       6.0             6.75                     4   \n",
       "2                       5.0             8.50                     3   \n",
       "3                       6.0             6.50                     3   \n",
       "4                      10.0            12.00                     5   \n",
       "...                     ...              ...                   ...   \n",
       "2466                    5.0             6.50                     4   \n",
       "2467                    6.0             6.00                     5   \n",
       "2468                    7.0             8.00                     5   \n",
       "2469                    6.5             8.50                     4   \n",
       "2470                    7.0             8.00                     5   \n",
       "\n",
       "      sent_per_par_mean_y  sent_per_par_std  sent_per_par_median_y  \n",
       "0                4.166667          1.169045                    4.5  \n",
       "1                6.500000          1.732051                    6.0  \n",
       "2                7.000000          4.358899                    5.0  \n",
       "3                6.333333          0.577350                    6.0  \n",
       "4                9.400000          3.714835                   10.0  \n",
       "...                   ...               ...                    ...  \n",
       "2466             5.500000          1.914854                    5.0  \n",
       "2467             5.400000          1.516575                    6.0  \n",
       "2468             6.800000          1.788854                    7.0  \n",
       "2469             6.750000          2.753785                    6.5  \n",
       "2470             7.200000          1.303840                    7.0  \n",
       "\n",
       "[2471 rows x 62 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
