{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LightAutoML installation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:25:41.301074Z","iopub.status.busy":"2023-12-24T08:25:41.300533Z","iopub.status.idle":"2023-12-24T08:25:47.436051Z","shell.execute_reply":"2023-12-24T08:25:47.435015Z","shell.execute_reply.started":"2023-12-24T08:25:41.301027Z"},"papermill":{"duration":4.154316,"end_time":"2023-11-05T19:55:14.77427","exception":false,"start_time":"2023-11-05T19:55:10.619954","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["%matplotlib inline\n","import gc\n","import numpy as np\n","import pandas as pd\n","import os\n","import itertools\n","import pickle\n","import re\n","import time\n","from random import choice, choices\n","from functools import reduce\n","from tqdm import tqdm\n","from itertools import cycle\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from functools import reduce\n","from itertools import cycle\n","from scipy import stats\n","from scipy.stats import skew, kurtosis\n","from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n","import lightgbm as lgb\n","import torch"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007333,"end_time":"2023-11-05T19:55:14.789479","exception":false,"start_time":"2023-11-05T19:55:14.782146","status":"completed"},"tags":[]},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:25:47.438292Z","iopub.status.busy":"2023-12-24T08:25:47.437424Z","iopub.status.idle":"2023-12-24T08:26:03.139258Z","shell.execute_reply":"2023-12-24T08:26:03.137893Z","shell.execute_reply.started":"2023-12-24T08:25:47.438254Z"},"papermill":{"duration":17.101649,"end_time":"2023-11-05T19:55:31.898771","exception":false,"start_time":"2023-11-05T19:55:14.797122","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["INPUT_DIR = '../input/linking-writing-processes-to-writing-quality'\n","train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n","train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n","test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n","ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.14224Z","iopub.status.busy":"2023-12-24T08:26:03.141814Z","iopub.status.idle":"2023-12-24T08:26:03.261875Z","shell.execute_reply":"2023-12-24T08:26:03.260721Z","shell.execute_reply.started":"2023-12-24T08:26:03.142202Z"},"papermill":{"duration":0.155263,"end_time":"2023-11-05T19:55:32.062212","exception":false,"start_time":"2023-11-05T19:55:31.906949","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_essays = pd.read_csv('../input/writing-quality-challenge-constructed-essays/train_essays_02.csv')\n","train_essays.index = train_essays[\"Unnamed: 0\"]\n","train_essays.index.name = None\n","train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n","train_essays.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.263686Z","iopub.status.busy":"2023-12-24T08:26:03.263304Z","iopub.status.idle":"2023-12-24T08:26:03.280275Z","shell.execute_reply":"2023-12-24T08:26:03.278785Z","shell.execute_reply.started":"2023-12-24T08:26:03.263652Z"},"papermill":{"duration":0.029497,"end_time":"2023-11-05T19:55:32.116127","exception":false,"start_time":"2023-11-05T19:55:32.08663","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Function to construct essays copied from here (small adjustments): https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor\n","\n","def getEssays(df):\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']]\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","    lastIndex = 0\n","    essaySeries = pd.Series()\n","    for index, valCount in enumerate(valCountsArr):\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","        lastIndex += valCount\n","        essayText = \"\"\n","        for Input in currTextInput.values:\n","            if Input[0] == 'Replace':\n","                replaceTxt = Input[2].split(' => ')\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +\\\n","                essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","            if Input[0] == 'Paste':\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","            if Input[0] == 'Remove/Cut':\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","            if \"M\" in Input[0]:\n","                croppedTxt = Input[0][10:]\n","                splitTxt = croppedTxt.split(' To ')\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                moveData = (int(valueArr[0][0][1:]), \n","                            int(valueArr[0][1][:-1]), \n","                            int(valueArr[1][0][1:]), \n","                            int(valueArr[1][1][:-1]))\n","                if moveData[0] != moveData[2]:\n","                    if moveData[0] < moveData[2]:\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n","                        essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n","                        essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","        essaySeries[index] = essayText\n","    essaySeries.index =  textInputDf['id'].unique()\n","    return pd.DataFrame(essaySeries, columns=['essay'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.283004Z","iopub.status.busy":"2023-12-24T08:26:03.282372Z","iopub.status.idle":"2023-12-24T08:26:03.294528Z","shell.execute_reply":"2023-12-24T08:26:03.293425Z","shell.execute_reply.started":"2023-12-24T08:26:03.282953Z"},"papermill":{"duration":0.016764,"end_time":"2023-11-05T19:55:32.141238","exception":false,"start_time":"2023-11-05T19:55:32.124474","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Helper functions\n","\n","def q1(x):\n","    return x.quantile(0.25)\n","def q3(x):\n","    return x.quantile(0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.296441Z","iopub.status.busy":"2023-12-24T08:26:03.296115Z","iopub.status.idle":"2023-12-24T08:26:03.312637Z","shell.execute_reply":"2023-12-24T08:26:03.311372Z","shell.execute_reply.started":"2023-12-24T08:26:03.296413Z"},"papermill":{"duration":0.031457,"end_time":"2023-11-05T19:55:32.18096","exception":false,"start_time":"2023-11-05T19:55:32.149503","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n","\n","def split_essays_into_sentences(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    essay_df = essay_df.explode('sent')\n","    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_sentence_aggregations(df):\n","    sent_agg_df = pd.concat(\n","        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    )\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","def split_essays_into_paragraphs(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n","    essay_df = essay_df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_paragraph_aggregations(df):\n","    paragraph_agg_df = pd.concat(\n","        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    ) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.315313Z","iopub.status.busy":"2023-12-24T08:26:03.314892Z","iopub.status.idle":"2023-12-24T08:26:11.134343Z","shell.execute_reply":"2023-12-24T08:26:11.133072Z","shell.execute_reply.started":"2023-12-24T08:26:03.315279Z"},"papermill":{"duration":8.195247,"end_time":"2023-11-05T19:55:40.384252","exception":false,"start_time":"2023-11-05T19:55:32.189005","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Sentence features for train dataset\n","train_sent_df = split_essays_into_sentences(train_essays)\n","train_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n","# plt.figure(figsize=(15, 1.5))\n","# plt.boxplot(x=train_sent_df.sent_len, vert=False, labels=['Sentence length'])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:11.13701Z","iopub.status.busy":"2023-12-24T08:26:11.136459Z","iopub.status.idle":"2023-12-24T08:26:18.475339Z","shell.execute_reply":"2023-12-24T08:26:18.474424Z","shell.execute_reply.started":"2023-12-24T08:26:11.136962Z"},"papermill":{"duration":8.110471,"end_time":"2023-11-05T19:55:48.50333","exception":false,"start_time":"2023-11-05T19:55:40.392859","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Paragraph features for train dataset\n","train_paragraph_df = split_essays_into_paragraphs(train_essays)\n","train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n","# plt.figure(figsize=(15, 1.5))\n","# plt.boxplot(x=train_paragraph_df.paragraph_len, vert=False, labels=['Paragraph length'])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:18.482009Z","iopub.status.busy":"2023-12-24T08:26:18.481054Z","iopub.status.idle":"2023-12-24T08:26:18.560272Z","shell.execute_reply":"2023-12-24T08:26:18.558987Z","shell.execute_reply.started":"2023-12-24T08:26:18.481964Z"},"papermill":{"duration":0.09158,"end_time":"2023-11-05T19:55:48.603596","exception":false,"start_time":"2023-11-05T19:55:48.512016","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Features for test dataset\n","test_essays = getEssays(test_logs)\n","test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n","test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:18.562287Z","iopub.status.busy":"2023-12-24T08:26:18.561891Z","iopub.status.idle":"2023-12-24T08:26:18.608778Z","shell.execute_reply":"2023-12-24T08:26:18.60776Z","shell.execute_reply.started":"2023-12-24T08:26:18.562254Z"},"papermill":{"duration":0.070872,"end_time":"2023-11-05T19:55:48.683261","exception":false,"start_time":"2023-11-05T19:55:48.612389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# The following code comes almost Abdullah's notebook: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","# Abdullah's code is based on work shared in previous notebooks (e.g., https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline)\n","\n","from collections import defaultdict\n","\n","class Preprocessor:\n","    \n","    def __init__(self, seed):\n","        self.seed = seed\n","        \n","        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n","              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n","        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n","                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n","        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n","        \n","        self.idf = defaultdict(float)\n","    \n","    def activity_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['activity'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.activities:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","\n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","    def event_counts(self, df, colname):\n","        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df[colname].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.events:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","    def text_change_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['text_change'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.text_changes:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","            \n","        return ret\n","\n","    def match_punctuations(self, df):\n","        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['down_event'].values):\n","            cnt = 0\n","            items = list(Counter(li).items())\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in self.punctuations:\n","                    cnt += v\n","            ret.append(cnt)\n","        ret = pd.DataFrame({'punct_cnt': ret})\n","        return ret\n","\n","    def get_input_words(self, df):\n","        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n","        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n","        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n","        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df.drop(['text_change'], axis=1, inplace=True)\n","        return tmp_df\n","    \n","    def make_feats(self, df):\n","        \n","        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n","        \n","        print(\"Engineering time data\")\n","        for gap in self.gaps:\n","            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n","            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n","        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        print(\"Engineering cursor position data\")\n","        for gap in self.gaps:\n","            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n","            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n","            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n","        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        print(\"Engineering word count data\")\n","        for gap in self.gaps:\n","            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n","            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n","            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n","        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n","        \n","        print(\"Engineering statistical summaries for features\")\n","        feats_stat = [\n","            ('event_id', ['max']),\n","            ('up_time', ['max']),\n","            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","            ('activity', ['nunique']),\n","            ('down_event', ['nunique']),\n","            ('up_event', ['nunique']),\n","            ('text_change', ['nunique']),\n","            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n","            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n","        for gap in self.gaps:\n","            feats_stat.extend([\n","                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n","            ])\n","        \n","        pbar = tqdm(feats_stat)\n","        for item in pbar:\n","            colname, methods = item[0], item[1]\n","            for method in methods:\n","                pbar.set_postfix()\n","                if isinstance(method, str):\n","                    method_name = method\n","                else:\n","                    method_name = method.__name__\n","                pbar.set_postfix(column=colname, method=method_name)\n","                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n","                feats = feats.merge(tmp_df, on='id', how='left')\n","\n","        print(\"Engineering activity counts data\")\n","        tmp_df = self.activity_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering event counts data\")\n","        tmp_df = self.event_counts(df, 'down_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        tmp_df = self.event_counts(df, 'up_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering text change counts data\")\n","        tmp_df = self.text_change_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering punctuation counts data\")\n","        tmp_df = self.match_punctuations(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","\n","        print(\"Engineering input words data\")\n","        tmp_df = self.get_input_words(df)\n","        feats = pd.merge(feats, tmp_df, on='id', how='left')\n","\n","        print(\"Engineering ratios data\")\n","        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n","        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n","        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n","        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n","\n","        return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-24T08:26:18.611191Z","iopub.status.busy":"2023-12-24T08:26:18.610117Z","iopub.status.idle":"2023-12-24T08:30:55.672484Z","shell.execute_reply":"2023-12-24T08:30:55.671342Z","shell.execute_reply.started":"2023-12-24T08:26:18.611149Z"},"papermill":{"duration":370.664062,"end_time":"2023-11-05T20:01:59.356059","exception":false,"start_time":"2023-11-05T19:55:48.691997","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["preprocessor = Preprocessor(seed=42)\n","train_feats = preprocessor.make_feats(train_logs)\n","test_feats = preprocessor.make_feats(test_logs)\n","nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n","train_feats = train_feats.drop(columns=nan_cols)\n","test_feats = test_feats.drop(columns=nan_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:30:55.674891Z","iopub.status.busy":"2023-12-24T08:30:55.67421Z","iopub.status.idle":"2023-12-24T08:31:00.683022Z","shell.execute_reply":"2023-12-24T08:31:00.681615Z","shell.execute_reply.started":"2023-12-24T08:30:55.67485Z"},"papermill":{"duration":5.754153,"end_time":"2023-11-05T20:02:05.211536","exception":false,"start_time":"2023-11-05T20:01:59.457383","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Code for additional aggregations comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","\n","train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n","    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n","train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n","train_agg_fe_df.reset_index(inplace=True)\n","\n","test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n","    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n","test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n","test_agg_fe_df.reset_index(inplace=True)\n","\n","train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n","test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:00.686024Z","iopub.status.busy":"2023-12-24T08:31:00.685218Z","iopub.status.idle":"2023-12-24T08:31:09.846325Z","shell.execute_reply":"2023-12-24T08:31:09.845204Z","shell.execute_reply.started":"2023-12-24T08:31:00.685971Z"},"papermill":{"duration":11.523742,"end_time":"2023-11-05T20:02:16.835416","exception":false,"start_time":"2023-11-05T20:02:05.311674","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Code for creating these features comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","# Idea is based on features introduced in Section 3 of this research paper: https://files.eric.ed.gov/fulltext/ED592674.pdf\n","\n","data = []\n","\n","for logs in [train_logs, test_logs]:\n","    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n","    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n","\n","    group = logs.groupby('id')['time_diff']\n","    largest_lantency = group.max()\n","    smallest_lantency = group.min()\n","    median_lantency = group.median()\n","    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n","    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n","    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n","    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n","    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n","    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n","\n","    data.append(pd.DataFrame({\n","        'id': logs['id'].unique(),\n","        'largest_lantency': largest_lantency,\n","        'smallest_lantency': smallest_lantency,\n","        'median_lantency': median_lantency,\n","        'initial_pause': initial_pause,\n","        'pauses_half_sec': pauses_half_sec,\n","        'pauses_1_sec': pauses_1_sec,\n","        'pauses_1_half_sec': pauses_1_half_sec,\n","        'pauses_2_sec': pauses_2_sec,\n","        'pauses_3_sec': pauses_3_sec,\n","    }).reset_index(drop=True))\n","\n","train_eD592674, test_eD592674 = data\n","\n","train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n","test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n","train_feats = train_feats.merge(train_scores, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.848795Z","iopub.status.busy":"2023-12-24T08:31:09.847778Z","iopub.status.idle":"2023-12-24T08:31:09.888317Z","shell.execute_reply":"2023-12-24T08:31:09.887011Z","shell.execute_reply.started":"2023-12-24T08:31:09.848753Z"},"papermill":{"duration":0.139952,"end_time":"2023-11-05T20:02:17.075043","exception":false,"start_time":"2023-11-05T20:02:16.935091","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Adding the additional features to the original feature set\n","\n","train_feats = train_feats.merge(train_sent_agg_df, on='id', how='left')\n","train_feats = train_feats.merge(train_paragraph_agg_df, on='id', how='left')\n","test_feats = test_feats.merge(test_sent_agg_df, on='id', how='left')\n","test_feats = test_feats.merge(test_paragraph_agg_df, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.890627Z","iopub.status.busy":"2023-12-24T08:31:09.890075Z","iopub.status.idle":"2023-12-24T08:31:09.895923Z","shell.execute_reply":"2023-12-24T08:31:09.894991Z","shell.execute_reply.started":"2023-12-24T08:31:09.890586Z"},"papermill":{"duration":0.111533,"end_time":"2023-11-05T20:02:17.288094","exception":false,"start_time":"2023-11-05T20:02:17.176561","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["target_col = ['score']\n","drop_cols = ['id']\n","train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.898825Z","iopub.status.busy":"2023-12-24T08:31:09.897594Z","iopub.status.idle":"2023-12-24T08:31:09.911032Z","shell.execute_reply":"2023-12-24T08:31:09.909888Z","shell.execute_reply.started":"2023-12-24T08:31:09.89879Z"},"trusted":true},"outputs":[],"source":["len(test_feats)"]},{"cell_type":"markdown","metadata":{},"source":["# LightGBM train and predict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.913306Z","iopub.status.busy":"2023-12-24T08:31:09.912919Z","iopub.status.idle":"2023-12-24T08:31:09.920509Z","shell.execute_reply":"2023-12-24T08:31:09.919481Z","shell.execute_reply.started":"2023-12-24T08:31:09.913271Z"},"trusted":true},"outputs":[],"source":["OOF_PREDS = np.zeros((len(train_feats), 2))\n","TEST_PREDS = np.zeros((len(test_feats), 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-24T08:31:09.922966Z","iopub.status.busy":"2023-12-24T08:31:09.922543Z","iopub.status.idle":"2023-12-24T08:36:19.909905Z","shell.execute_reply":"2023-12-24T08:36:19.908642Z","shell.execute_reply.started":"2023-12-24T08:31:09.922933Z"},"papermill":{"duration":382.8295,"end_time":"2023-11-05T20:08:40.219879","exception":false,"start_time":"2023-11-05T20:02:17.390379","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["# Code comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","\n","models_dict = {}\n","scores = []\n","\n","test_predict_list = []\n","best_params = {'reg_alpha': 0.007678095440286993, \n","               'reg_lambda': 0.34230534302168353, \n","               'colsample_bytree': 0.627061253588415, \n","               'subsample': 0.854942238828458, \n","               'learning_rate': 0.04,   #0.038697981947473245, \n","               'num_leaves': 22, \n","               'max_depth': 37, \n","               'min_child_samples': 18,\n","               'n_jobs':4\n","              }\n","\n","for i in range(5): \n","    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n","    oof_valid_preds = np.zeros(train_feats.shape[0])\n","    X_test = test_feats[train_cols]\n","    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","        \n","        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","        params = {\n","            \"objective\": \"regression\",\n","            \"metric\": \"rmse\",\n","            'random_state': 42,\n","            \"n_estimators\" : 12001,\n","            \"verbosity\": -1,\n","            **best_params\n","        }\n","        model = lgb.LGBMRegressor(**params)\n","        early_stopping_callback = lgb.early_stopping(100, first_metric_only=True, verbose=False)\n","        \n","        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n","                  callbacks=[early_stopping_callback],\n","        )\n","        valid_predict = model.predict(X_valid)\n","        oof_valid_preds[valid_idx] = valid_predict\n","        OOF_PREDS[valid_idx, 0] += valid_predict / 5\n","        test_predict = model.predict(X_test)\n","        TEST_PREDS[:, 0] += test_predict / 5 / 10\n","        test_predict_list.append(test_predict)\n","        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n","        models_dict[f'{fold}_{i}'] = model\n","\n","    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n","    scores.append(oof_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:36:19.912105Z","iopub.status.busy":"2023-12-24T08:36:19.911732Z","iopub.status.idle":"2023-12-24T08:36:19.922465Z","shell.execute_reply":"2023-12-24T08:36:19.921199Z","shell.execute_reply.started":"2023-12-24T08:36:19.912072Z"},"trusted":true},"outputs":[],"source":["print('OOF metric LGBM = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], \n","                                                                   OOF_PREDS[:, 0], \n","                                                                   squared=False)))"]},{"cell_type":"markdown","metadata":{},"source":["# LightAutoML NN (DenseLight) prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:36:19.924501Z","iopub.status.busy":"2023-12-24T08:36:19.924137Z","iopub.status.idle":"2023-12-24T08:37:01.656868Z","shell.execute_reply":"2023-12-24T08:37:01.655462Z","shell.execute_reply.started":"2023-12-24T08:36:19.924467Z"},"trusted":true},"outputs":[],"source":["from lightautoml.automl.presets.tabular_presets import TabularAutoML\n","from lightautoml.tasks import Task\n","import joblib\n","\n","# def use_plr(USE_PLR):\n","#     if USE_PLR:\n","#         return \"plr\"\n","#     else:\n","#         return \"cont\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-24T08:37:01.660098Z","iopub.status.busy":"2023-12-24T08:37:01.659083Z","iopub.status.idle":"2023-12-24T08:38:00.259007Z","shell.execute_reply":"2023-12-24T08:38:00.257769Z","shell.execute_reply.started":"2023-12-24T08:37:01.660053Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["for i in range(3):\n","    oof_pred, automl = joblib.load('/kaggle/input/linkinglamamodels/oof_and_lama_denselight_{}.pkl'.format(i))\n","    OOF_PREDS[:, 1] += oof_pred / 3\n","    TEST_PREDS[:, 1] += automl.predict(test_feats[train_cols]).data[:, 0] / 3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:00.263176Z","iopub.status.busy":"2023-12-24T08:38:00.262664Z","iopub.status.idle":"2023-12-24T08:38:00.273721Z","shell.execute_reply":"2023-12-24T08:38:00.272602Z","shell.execute_reply.started":"2023-12-24T08:38:00.263126Z"},"trusted":true},"outputs":[],"source":["print('OOF metric LightAutoML_NN = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], \n","                                                                               OOF_PREDS[:, 1], \n","                                                                               squared=False)))"]},{"cell_type":"markdown","metadata":{},"source":["# Blending"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:00.276337Z","iopub.status.busy":"2023-12-24T08:38:00.275714Z","iopub.status.idle":"2023-12-24T08:38:02.308343Z","shell.execute_reply":"2023-12-24T08:38:02.30731Z","shell.execute_reply.started":"2023-12-24T08:38:00.276292Z"},"trusted":true},"outputs":[],"source":["best_sc = 1\n","for w in np.arange(0, 1.01, 0.001):\n","    sc = metrics.mean_squared_error(train_feats[target_col], \n","                                    w * OOF_PREDS[:, 0] + (1-w) * OOF_PREDS[:, 1], \n","                                    squared=False)\n","    if sc < best_sc:\n","        best_sc = sc\n","        best_w = w\n","        \n","print('Composition OOF score = {:.5f}'.format(best_sc))\n","print('Composition best W = {:.3f}'.format(best_w))"]},{"cell_type":"markdown","metadata":{},"source":["# Submission creation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.309951Z","iopub.status.busy":"2023-12-24T08:38:02.309629Z","iopub.status.idle":"2023-12-24T08:38:02.318528Z","shell.execute_reply":"2023-12-24T08:38:02.317543Z","shell.execute_reply.started":"2023-12-24T08:38:02.309922Z"},"trusted":true},"outputs":[],"source":["W = [best_w, 1 - best_w]\n","print(W)\n","test_preds = TEST_PREDS[:, 0] * W[0] + TEST_PREDS[:, 1] * W[1]\n","test_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.320281Z","iopub.status.busy":"2023-12-24T08:38:02.319931Z","iopub.status.idle":"2023-12-24T08:38:02.334213Z","shell.execute_reply":"2023-12-24T08:38:02.333261Z","shell.execute_reply.started":"2023-12-24T08:38:02.320249Z"},"trusted":true},"outputs":[],"source":["test_feats['score'] = test_preds\n","sub1 = test_feats[['id', 'score']]\n","#test_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.335955Z","iopub.status.busy":"2023-12-24T08:38:02.335502Z","iopub.status.idle":"2023-12-24T08:38:02.351253Z","shell.execute_reply":"2023-12-24T08:38:02.350495Z","shell.execute_reply.started":"2023-12-24T08:38:02.335924Z"},"trusted":true},"outputs":[],"source":["sub1"]},{"cell_type":"markdown","metadata":{},"source":["# Saving OOFs and test predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.360136Z","iopub.status.busy":"2023-12-24T08:38:02.359747Z","iopub.status.idle":"2023-12-24T08:38:02.370475Z","shell.execute_reply":"2023-12-24T08:38:02.369394Z","shell.execute_reply.started":"2023-12-24T08:38:02.360096Z"},"trusted":true},"outputs":[],"source":["joblib.dump((OOF_PREDS, TEST_PREDS), 'OOF_and_TEST_preds.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["# Public LGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.372781Z","iopub.status.busy":"2023-12-24T08:38:02.37176Z","iopub.status.idle":"2023-12-24T08:38:02.915232Z","shell.execute_reply":"2023-12-24T08:38:02.914284Z","shell.execute_reply.started":"2023-12-24T08:38:02.372746Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import gc\n","import ctypes\n","def clean_memory():\n","    gc.collect()\n","    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()\n","clean_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.917668Z","iopub.status.busy":"2023-12-24T08:38:02.917005Z","iopub.status.idle":"2023-12-24T08:38:03.030071Z","shell.execute_reply":"2023-12-24T08:38:03.029076Z","shell.execute_reply.started":"2023-12-24T08:38:02.917631Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","%matplotlib inline\n","import gc\n","import os\n","import itertools\n","import pickle\n","\n","from random import choice, choices\n","from functools import reduce\n","from tqdm import tqdm\n","from itertools import cycle\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from functools import reduce\n","from itertools import cycle\n","from scipy import stats\n","from scipy.stats import skew, kurtosis\n","from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n","from transformers import BertTokenizer\n","import warnings\n","\n","import os\n","import gc\n","import re\n","import random\n","from collections import Counter, defaultdict\n","import pprint\n","import time\n","import copy\n","\n","\n","import seaborn as sns\n","from tqdm.autonotebook import tqdm\n","\n","# from gensim.models import Word2Vec\n","from sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\n","from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD, PCA\n","from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n","from sklearn.svm import SVR\n","from sklearn.pipeline import Pipeline\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import Lasso, Ridge, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.impute import SimpleImputer\n","\n","import lightgbm as lgb\n","import xgboost as xgb\n","import catboost as cb\n","import optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:03.032294Z","iopub.status.busy":"2023-12-24T08:38:03.031494Z","iopub.status.idle":"2023-12-24T08:38:14.185902Z","shell.execute_reply":"2023-12-24T08:38:14.184806Z","shell.execute_reply.started":"2023-12-24T08:38:03.032257Z"},"trusted":true},"outputs":[],"source":["traindf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n","train_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n","testdf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:14.188071Z","iopub.status.busy":"2023-12-24T08:38:14.187459Z","iopub.status.idle":"2023-12-24T08:38:14.20611Z","shell.execute_reply":"2023-12-24T08:38:14.204917Z","shell.execute_reply.started":"2023-12-24T08:38:14.188036Z"},"trusted":true},"outputs":[],"source":["def getEssays(df):\n","    # Copy required columns\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n","    \n","    # Get rid of text inputs that make no change\n","    # Note: Shift was unpreditcable so ignored\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","\n","    # Get how much each Id there is\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","\n","    # Holds the final index of the previous Id\n","    lastIndex = 0\n","\n","    # Holds all the essays\n","    essaySeries = pd.Series()\n","\n","    # Fills essay series with essays\n","    for index, valCount in enumerate(valCountsArr):\n","\n","        # Indexes down_time at current Id\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","\n","        # Update the last index\n","        lastIndex += valCount\n","\n","        # Where the essay content will be stored\n","        essayText = \"\"\n","\n","        \n","        # Produces the essay\n","        for Input in currTextInput.values:\n","            \n","            # Input[0] = activity\n","            # Input[2] = cursor_position\n","            # Input[3] = text_change\n","            \n","            # If activity = Replace\n","            if Input[0] == 'Replace':\n","                # splits text_change at ' => '\n","                replaceTxt = Input[2].split(' => ')\n","                \n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","\n","                \n","            # If activity = Paste    \n","            if Input[0] == 'Paste':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Remove/Cut\n","            if Input[0] == 'Remove/Cut':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Move...\n","            if \"M\" in Input[0]:\n","                # Gets rid of the \"Move from to\" text\n","                croppedTxt = Input[0][10:]\n","                \n","                # Splits cropped text by ' To '\n","                splitTxt = croppedTxt.split(' To ')\n","                \n","                # Splits split text again by ', ' for each item\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                \n","                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n","                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","\n","                # Skip if someone manages to activiate this by moving to same place\n","                if moveData[0] != moveData[2]:\n","                    # Check if they move text forward in essay (they are different)\n","                    if moveData[0] < moveData[2]:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","                \n","                \n","            # If just input\n","            # DONT TOUCH\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","\n","            \n","        # Sets essay at index  \n","        essaySeries[index] = essayText\n","     \n","    \n","    # Sets essay series index to the ids\n","    essaySeries.index =  textInputDf['id'].unique()\n","    \n","    \n","    # Returns the essay series\n","    return essaySeries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:14.208367Z","iopub.status.busy":"2023-12-24T08:38:14.207919Z"},"trusted":true},"outputs":[],"source":["%%time\n","train_essays = getEssays(traindf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_essays = getEssays(testdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_essaysdf = pd.DataFrame({'id': train_essays.index, 'essay': train_essays.values})\n","test_essaysdf = pd.DataFrame({'id': test_essays.index, 'essay': test_essays.values})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["merged_data = train_essaysdf.merge(train_scores, on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n","X_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\n","X_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\n","count_vectorizer.get_feature_names_out() #ADDED\n","y = merged_data['score']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train = pd.DataFrame()\n","df_test = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_tokenizer_train = X_tokenizer_train.todense()\n","X_tokenizer_test = X_tokenizer_test.todense()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(X_tokenizer_train.shape[1]) : \n","    L = list(X_tokenizer_train[:,i])\n","    li = [int(x) for x in L ]\n","    df_train[f'feature {i}'] = li"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(X_tokenizer_test.shape[1]) : \n","    L = list(X_tokenizer_test[:,i])\n","    li = [int(x) for x in L ]\n","    df_test[f'feature {i}'] = li"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train_index = train_essaysdf['id']\n","df_test_index = test_essaysdf['id']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train.loc[:, 'id'] = df_train_index\n","df_test.loc[:, 'id'] = df_test_index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_agg_fe_df = traindf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n","train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n","train_agg_fe_df.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_agg_fe_df = testdf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n","test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n","test_agg_fe_df.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","class Preprocessor:\n","    \n","    def __init__(self, seed):\n","        self.seed = seed\n","        \n","        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n","              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n","        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n","                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n","        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n","        \n","        self.idf = defaultdict(float)\n","#         self.gaps = [1, 2]\n","    \n","    def activity_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['activity'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.activities:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","\n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","\n","    def event_counts(self, df, colname):\n","        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df[colname].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.events:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","\n","    def text_change_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['text_change'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.text_changes:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","            \n","        return ret\n","\n","    def match_punctuations(self, df):\n","        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['down_event'].values):\n","            cnt = 0\n","            items = list(Counter(li).items())\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in self.punctuations:\n","                    cnt += v\n","            ret.append(cnt)\n","        ret = pd.DataFrame({'punct_cnt': ret})\n","        return ret\n","\n","\n","    def get_input_words(self, df):\n","        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n","        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n","        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n","        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df.drop(['text_change'], axis=1, inplace=True)\n","        return tmp_df\n","    \n","    def make_feats(self, df):\n","        \n","        print(\"Starting to engineer features\")\n","        \n","        # initialize features dataframe\n","        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n","        \n","        # get shifted features\n","        # time shift\n","        print(\"Engineering time data\")\n","        for gap in self.gaps:\n","            print(f\"> for gap {gap}\")\n","            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n","            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n","        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        # cursor position shift\n","        print(\"Engineering cursor position data\")\n","        for gap in self.gaps:\n","            print(f\"> for gap {gap}\")\n","            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n","            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n","            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n","        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        # word count shift\n","        print(\"Engineering word count data\")\n","        for gap in self.gaps:\n","            print(f\"> for gap {gap}\")\n","            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n","            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n","            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n","        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n","        \n","        # get aggregate statistical features\n","        print(\"Engineering statistical summaries for features\")\n","        # [(feature name, [ stat summaries to add ])]\n","        feats_stat = [\n","            ('event_id', ['max']),\n","            ('up_time', ['max']),\n","            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","            ('activity', ['nunique']),\n","            ('down_event', ['nunique']),\n","            ('up_event', ['nunique']),\n","            ('text_change', ['nunique']),\n","            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n","            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n","        for gap in self.gaps:\n","            feats_stat.extend([\n","                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n","            ])\n","        \n","        pbar = tqdm(feats_stat)\n","        for item in pbar:\n","            colname, methods = item[0], item[1]\n","            for method in methods:\n","                pbar.set_postfix()\n","                if isinstance(method, str):\n","                    method_name = method\n","                else:\n","                    method_name = method.__name__\n","                    \n","                pbar.set_postfix(column=colname, method=method_name)\n","                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n","                feats = feats.merge(tmp_df, on='id', how='left')\n","\n","        # counts\n","        print(\"Engineering activity counts data\")\n","        tmp_df = self.activity_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering event counts data\")\n","        tmp_df = self.event_counts(df, 'down_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        tmp_df = self.event_counts(df, 'up_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering text change counts data\")\n","        tmp_df = self.text_change_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering punctuation counts data\")\n","        tmp_df = self.match_punctuations(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","\n","        # input words\n","        print(\"Engineering input words data\")\n","        tmp_df = self.get_input_words(df)\n","        feats = pd.merge(feats, tmp_df, on='id', how='left')\n","\n","        # compare feats\n","        print(\"Engineering ratios data\")\n","        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n","        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n","        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n","        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n","        \n","        print(\"Done!\")\n","        return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preprocessor = Preprocessor(seed=42)\n","\n","print(\"Engineering features for training data\")\n","\n","other_train_feats = preprocessor.make_feats(traindf)\n","\n","print()\n","print(\"-\"*25)\n","print(\"Engineering features for test data\")\n","print(\"-\"*25)\n","other_test_feats = preprocessor.make_feats(testdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train_all = pd.DataFrame()\n","df_test_all = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train_all = df_train.merge(train_agg_fe_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_test_all = df_test.merge(test_agg_fe_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def q1(x):\n","    return x.quantile(0.25)\n","def q3(x):\n","    return x.quantile(0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n","\n","def split_essays_into_sentences(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',str(x)))\n","    essay_df = essay_df.explode('sent')\n","    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.columns.tolist()].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_sentence_aggregations(df):\n","    sent_agg_df = pd.concat(\n","        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    )\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","def split_essays_into_paragraphs(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n","    essay_df = essay_df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_paragraph_aggregations(df):\n","    paragraph_agg_df = pd.concat(\n","        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    ) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sent_df = split_essays_into_sentences(train_essaysdf)\n","train_sent_agg_df = compute_sentence_aggregations(train_sent_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_paragraph_df = split_essays_into_paragraphs(train_essaysdf)\n","train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essaysdf))\n","test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essaysdf))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_paragraph_agg_df.loc[:, 'id'] = df_train_index\n","train_sent_agg_df.loc[:, 'id'] = df_train_index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_paragraph_agg_df.loc[:, 'id'] = df_test_index\n","test_sent_agg_df.loc[:, 'id'] = df_test_index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_train_feats = pd.DataFrame()\n","new_test_feats = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_train_feats = train_paragraph_agg_df.merge(df_train_all,on='id')\n","new_train_feats = new_train_feats.merge(train_sent_agg_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_test_feats = test_paragraph_agg_df.merge(df_test_all,on='id')\n","new_test_feats = new_test_feats.merge(test_sent_agg_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feats = pd.DataFrame()\n","test_feats = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feats = new_train_feats.merge(other_train_feats,on='id')\n","test_feats = new_test_feats.merge(other_test_feats,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = []\n","\n","for logs in [traindf, testdf]:\n","    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n","    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n","\n","    group = logs.groupby('id')['time_diff']\n","    largest_lantency = group.max()\n","    smallest_lantency = group.min()\n","    median_lantency = group.median()\n","    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n","    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n","    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n","    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n","    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n","    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n","\n","    data.append(pd.DataFrame({\n","        'id': logs['id'].unique(),\n","        'largest_lantency': largest_lantency,\n","        'smallest_lantency': smallest_lantency,\n","        'median_lantency': median_lantency,\n","        'initial_pause': initial_pause,\n","        'pauses_half_sec': pauses_half_sec,\n","        'pauses_1_sec': pauses_1_sec,\n","        'pauses_1_half_sec': pauses_1_half_sec,\n","        'pauses_2_sec': pauses_2_sec,\n","        'pauses_3_sec': pauses_3_sec,\n","    }).reset_index(drop=True))\n","\n","train_eD592674, test_eD592674 = data\n","\n","train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n","test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n","train_feats = train_feats.merge(train_scores, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","train_feats['score_class'] = le.fit_transform(train_feats['score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target_col = ['score']\n","\n","drop_cols = ['id', 'score_class']\n","train_cols = list()\n","\n","train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]\n","\n","train_cols.__len__(), target_col.__len__()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n","nan_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for col in nan_cols:\n","    mode_value_train = train_feats[col].mode()[0]  # In case there are multiple modes, choose the first one\n","    train_feats[col].fillna(mode_value_train, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for col in test_feats.columns[test_feats.isna().any()].tolist():\n","    # Find the most frequent value in the training set for the current feature\n","    most_frequent_value_train = train_feats[col].mode()[0]\n","    \n","    # Fill missing values in the test set with the most frequent value from the training set\n","    test_feats[col].fillna(most_frequent_value_train, inplace=True)\n","\n","train_feats.shape, test_feats.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feats.columns[train_feats.isna().any()].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nan_values_test = test_feats.columns[test_feats.isna().any()].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clean_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models_dict = {}\n","scores = []\n","\n","test_predict_list = []\n","best_params = {'boosting_type': 'gbdt', \n","               'metric': 'rmse',\n","               'reg_alpha': 0.003188447814669599, \n","               'reg_lambda': 0.0010228604507564066, \n","               'colsample_bytree': 0.5420247656839267, \n","               'subsample': 0.9778252382803456, \n","               'feature_fraction': 0.8,\n","               'bagging_freq': 1,\n","               'bagging_fraction': 0.75,\n","               'learning_rate': 0.01716485155812008, \n","               'num_leaves': 19, \n","               'min_child_samples': 46,\n","               'verbosity': -1,\n","               'random_state': 42,\n","               'n_estimators': 500,\n","               'device_type': 'cpu'}\n","\n","for i in range(5): \n","    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n","\n","    oof_valid_preds = np.zeros(train_feats.shape[0], )\n","\n","    X_test = test_feats[train_cols]\n","\n","\n","    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","\n","        print(\"==-\"* 50)\n","        print(\"Fold : \", fold)\n","\n","        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","\n","        print(\"Trian :\", X_train.shape, y_train.shape)\n","        print(\"Valid :\", X_valid.shape, y_valid.shape)\n","\n","        params = {\n","            \"objective\": \"regression\",\n","            \"metric\": \"rmse\",\n","            'random_state': 42,\n","            \"n_estimators\" : 12001,\n","            \"verbosity\": -1,\n","            \"device_type\": \"cpu\",\n","            **best_params\n","        }\n","\n","        model = lgb.LGBMRegressor(**params)\n","\n","        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n","        verbose_callback = lgb.callback.record_evaluation({})\n","\n","        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n","                  callbacks=[early_stopping_callback, verbose_callback],\n","        )\n","\n","        valid_predict = model.predict(X_valid)\n","        oof_valid_preds[valid_idx] = valid_predict\n","\n","        test_predict = model.predict(X_test)\n","        test_predict_list.append(test_predict)\n","\n","        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n","        print(\"Fold RMSE Score : \", score)\n","\n","        models_dict[f'{fold}_{i}'] = model\n","\n","\n","    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n","    scores.append(oof_score)\n","    print(\"OOF RMSE Score : \", oof_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_importances_values = np.asarray([model.feature_importances_ for model in models_dict.values()]).mean(axis=0)\n","feature_importance_df = pd.DataFrame({'name': train_cols, 'importance': feature_importances_values})\n","\n","feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["np.mean(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 6))\n","\n","ax = sns.barplot(data=feature_importance_df.head(30), x='name', y='importance')\n","ax.set_title(f\"Mean feature importances\")\n","ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_feats['score'] = np.mean(test_predict_list, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub2 = test_feats[['id', 'score']]"]},{"cell_type":"markdown","metadata":{},"source":["# Writing Quality(fusion_notebook)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd#csv\n","import numpy as np#\n","import polars as pl#pandas,.\n","#,,\n","from collections import Counter,defaultdict\n","import re#\n","from scipy.stats import skew, kurtosis#\n","import gc#\n","\n","#model\n","from lightgbm import LGBMRegressor#lgbm\n","from catboost import CatBoostRegressor#catboost\n","from sklearn.svm import SVR#\n","\n","#KFoldk,StratifiedKFold\n","from sklearn.model_selection import KFold,StratifiedKFold\n","from sklearn.preprocessing import MinMaxScaler#(x-min)/(max-min)\n","from sklearn.impute import SimpleImputer#\n","\n","#,\n","import random\n","seed=2023\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","import warnings#\n","warnings.filterwarnings('ignore')#filterwarnings()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#\n","num_folds = 10\n","#svr()\n","model_with_scaled_features = ['svr']\n","#\n","blending_weights = {\n","    'lgbm': 0.4,\n","    'catboost': 0.4,\n","    'svr': 0.2,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\")\n","print(f\"len(train_logs):{len(train_logs)}\")\n","train_logs=train_logs.sort_values(by=['id', 'down_time'])\n","# \n","train_logs = train_logs.reset_index(drop=True)\n","# 'id'\n","train_logs['event_id'] = train_logs.groupby('id').cumcount() + 1\n","\n","train_scores=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\")\n","\n","test_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\")\n","print(f\"len(test_logs):{len(test_logs)}\")\n","test_logs=test_logs.sort_values(by=['id', 'down_time'])\n","# \n","test_logs = test_logs.reset_index(drop=True)\n","# 'id'\n","test_logs['event_id'] = test_logs.groupby('id').cumcount() + 1\n","test_logs.to_csv(\"test_logs.csv\",index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#q '.'  .\n","def getEssays(df):\n","    #df.\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']]\n","    #activity'Nonproduction'\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","    #id,\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","    #\n","    lastIndex = 0\n","    #.\n","    essaySeries = pd.Series()\n","    #indexid,valCount\n","    for index, valCount in enumerate(valCountsArr):\n","        #iid['activity', 'cursor_position', 'text_change']\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","        #idindex\n","        lastIndex += valCount\n","        essayText = \"\"\n","        for Input in currTextInput.values:\n","            #input[0]idactivity\n","            if Input[0] == 'Replace':\n","                #text_change' => ' replaceTxt:[' qqq qqqqq ', ' ']\n","                replaceTxt = Input[2].split(' => ')#A=>B\n","                #input[1], -len()\n","                #,replaceTxt[0]replaceTxt[1] \n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","            if Input[0] == 'Paste':#\n","                #print(f\"input[2]:{Input[2]}\") #input[2]:qqqqqqqqqqq \n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","            if Input[0] == 'Remove/Cut':# Input[1]Input[2]\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","            #Move from\n","            if \"M\" in Input[0]:\n","                #[284, 292] To [282, 290] [284, 292]8[282,290]\n","                croppedTxt = Input[0][10:]\n","                #fromto4.\n","                splitTxt = croppedTxt.split(' To ')\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                moveData = (int(valueArr[0][0][1:]), \n","                            int(valueArr[0][1][:-1]), \n","                            int(valueArr[1][0][1:]), \n","                            int(valueArr[1][1][:-1]))\n","                #,,\n","                if moveData[0] != moveData[2]:\n","                    # \n","                    if moveData[0] < moveData[2]:\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n","                        essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    #\n","                    else:\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n","                        essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","            #check    \n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","        #id\n","        essaySeries[index] = essayText\n","    #id\n","    essaySeries.index =  textInputDf['id'].unique()\n","    return pd.DataFrame(essaySeries, columns=['essay']).reset_index().rename(columns={\"index\":'id'})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#25%\n","def q1(x):\n","    return x.quantile(0.25)\n","#75%\n","def q3(x):\n","    return x.quantile(0.75)\n","AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', kurtosis, 'sum']\n","\n","#\n","def split_essays_into_words(df):\n","    essay_df = df\n","    #,\\n,,.\n","    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n","    # essay1 [1,2,3] essay2[4,5] ->5 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n","    essay_df = essay_df.explode('word')\n","    #\n","    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n","    #0\n","    essay_df = essay_df[essay_df['word_len'] != 0]\n","    return essay_df\n","\n","#word_len,>=word_len\n","def compute_word_aggregations(word_df):\n","    #id\n","    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n","    #('mean','word_len')->'mean_word_len'\n","    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n","    word_agg_df['id'] = word_agg_df.index\n","    for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n","        #ge Latex>=,word_len>=word_l,id,0\n","        word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n","        #0\n","        word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n","    #\n","    word_agg_df = word_agg_df.reset_index(drop=True)\n","    return word_agg_df\n","\n","#df\n","def split_essays_into_sentences(df):\n","    essay_df = df#dfdf\n","    #. ? !. .\n","    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    # essay1 [1,2,3] essay2[4,5] ->5 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n","    essay_df = essay_df.explode('sent')\n","    #'\\n' strip .\n","    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # \n","    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n","    #.\n","    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n","    #0\n","    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_sentence_aggregations(df):\n","    #\n","    sent_agg_df = pd.concat(\n","        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    )\n","    #('mean','sent_len')->'mean_sent_len'\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","\n","    # New features intoduced here: https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline-v2\n","    for sent_l in [50, 60, 75, 100]:\n","        #ge Latex>=,sent_len>=sent_l,id,0\n","        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = df[df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n","        #0\n","        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n","    #\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    #count,,sent_lencount.,.\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    #sent_len_count,rename.\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","#.(?)\n","def split_essays_into_paragraphs(df):\n","    essay_df = df\n","    #'\\n' [1,2,3]\n","    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n","    #[1 [1 2,]->[1 1 // 1 2]\n","    essay_df = essay_df.explode('paragraph')\n","    #\n","    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n","    #\n","    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    #0.\n","    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","#,.\n","def compute_paragraph_aggregations(df):\n","    paragraph_agg_df = pd.concat(\n","        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    ) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"train_essays\")\n","train_essays = pd.read_csv('/kaggle/input/writing-quality-challenge-constructed-essays/train_essays_fast.csv')\n","print(\"train_word_agg_df\")\n","train_word_agg_df = compute_word_aggregations(split_essays_into_words(train_essays))\n","print(\"train_sent_agg_df\")\n","train_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(train_essays))\n","print(\"train_paragraph_agg_df\")\n","train_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(train_essays))\n","print(\"test_essays\")\n","test_essays = getEssays(test_logs)\n","test_essays_copy=test_essays.copy()\n","print(\"test_word_agg_df\")\n","test_word_agg_df = compute_word_aggregations(split_essays_into_words(test_essays))\n","print(\"test_sent_agg_df\")\n","test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n","print(\"test_paragraph_agg_df\")\n","test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Preprocessor:#\n","    \n","    def __init__(self):\n","        \n","        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste','Move From']#activity\n","        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n","              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']#down_event\n","        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']#text_change\n","        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n","                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']#down_event\n","        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]#\n","        \n","        #activityidf\n","        self.idf = defaultdict(float)#float,,0.0\n","    \n","    #dfactivitycount\n","    def activity_counts(self, df):\n","        #idactivity\n","        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n","        #\n","        ret = list()\n","        for li in tmp_df['activity'].values:#activity\n","            items = list(Counter(li).items())#[(activity1:count1),(activity2:count2),]\n","            di = dict()#\n","            #activity0\n","            for k in self.activities:\n","                di[k] = 0\n","            #activitycount\n","            for item in items:\n","                k, v = item[0], item[1]#k:activity v:count\n","                if k in di:\n","                    di[k] = v\n","            #activitycount\n","            ret.append(di)\n","        #pandas\n","        ret = pd.DataFrame(ret)\n","        #\n","        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        #,\n","        cnts = ret.sum(1)\n","\n","        #,tf-idf\n","        for col in cols:#activity_i_count\n","            if col in self.idf.keys():#key\n","                idf = self.idf[col]\n","            else:#\n","                #idf=log(/(+1))\n","                idf = np.log(df.shape[0] / (ret[col].sum() + 1))\n","                self.idf[col] = idf#colidf\n","            #ret[col] / cnts :/,log1\n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret#tf-idf\n","\n","    #eventtf-idf,down_eventup_event,colname\n","    def event_counts(self, df, colname):\n","        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n","        ret = list()\n","        for li in tmp_df[colname].values:\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.events:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","    #text_changetf-idf\n","    def text_change_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n","        ret = list()\n","        for li in tmp_df['text_change'].values:\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.text_changes:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","            \n","        return ret\n","    #,.(tf-idf?)\n","    def match_punctuations(self, df):\n","        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n","        ret = list()\n","        for li in tmp_df['down_event'].values:\n","            cnt = 0\n","            items = list(Counter(li).items())\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in self.punctuations:#,\n","                    cnt += v\n","            ret.append(cnt)\n","        ret = pd.DataFrame({'punct_cnt': ret})\n","        return ret\n","\n","\n","    def get_input_words(self, df):\n","        #~ text_change  => Nochange\n","        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n","        #drop => Nochange id\n","        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n","        #\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n","        #'q'\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n","        #len,text_changeq\n","        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n","        #,,,np.nan0\n","        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df.drop(['text_change'], axis=1, inplace=True)\n","        return tmp_df\n","    \n","    #df\n","    def make_feats(self, df):\n","        \n","        print(\"Starting to engineer features\")\n","        #id\n","        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n","        #\n","        print(\"Engineering time data\")\n","        for gap in self.gaps:\n","            print(f\"-> for gap {gap}\")\n","            #up_timeshiftaction_time_gap\n","            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n","            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n","        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        #cursor_position,-\n","        print(\"Engineering cursor position data\")\n","        for gap in self.gaps:\n","            print(f\"-> for gap {gap}\")\n","            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n","            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n","            #,.\n","            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n","        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        #word_count,.\n","        print(\"Engineering word count data\")\n","        for gap in self.gaps:\n","            print(f\"-> for gap {gap}\")\n","            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n","            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n","            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n","        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n","        \n","        print(\"Engineering statistical summaries for features\")\n","        #,,.\n","        feats_stat = [\n","            ('event_id', ['max']),\n","            ('down_time',['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']),\n","            ('up_time',['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']),\n","            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt,'last', 'first','median']),\n","            ('activity', ['nunique']),\n","            ('down_event', ['nunique']),\n","            ('up_event', ['nunique']),\n","            ('text_change', ['nunique']),\n","            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean', 'std', 'min','last', 'first',  'median', 'sum']),\n","            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean', 'std', 'min', 'last', 'first','median', 'sum'])]\n","        #for\n","        for gap in self.gaps:\n","            feats_stat.extend([\n","                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n","            ])\n","        \n","        pbar = feats_stat\n","        for item in pbar:\n","            colname, methods = item[0], item[1]#'max'\n","            for method in methods:\n","                #agg\n","                if isinstance(method, str):\n","                    method_name = method\n","                else:\n","                    method_name = method.__name__\n","                #feats.\n","                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n","                feats = feats.merge(tmp_df, on='id', how='left')\n","\n","        #activitytf-idf\n","        print(\"Engineering activity counts data\")\n","        tmp_df = self.activity_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        #down_eventup_eventtf-idf\n","        print(\"Engineering event counts data\")\n","        tmp_df = self.event_counts(df, 'down_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        tmp_df = self.event_counts(df, 'up_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering text change counts data\")\n","        tmp_df = self.text_change_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering punctuation counts data\")\n","        tmp_df = self.match_punctuations(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","\n","        # input words\n","        print(\"Engineering input words data\")\n","        tmp_df = self.get_input_words(df)\n","        feats = pd.merge(feats, tmp_df, on='id', how='left')\n","\n","        # compare feats\n","        print(\"Engineering ratios data\")\n","        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n","        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n","        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n","        #\n","        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n","        \n","        print(\"Done!\")\n","        return feats\n","\n","preprocessor = Preprocessor()\n","print(\"Engineering features for training data\")\n","\n","train_feats = preprocessor.make_feats(train_logs)\n","print(\"-\"*25)\n","print(\"Engineering features for test data\")\n","test_feats = preprocessor.make_feats(test_logs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = []\n","\n","for logs in [train_logs, test_logs]:\n","    #up_timedown_time\n","    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n","    #(down_timeup_time) /1000\n","    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n","\n","    #idtime_diff\n","    group = logs.groupby('id')['time_diff']\n","    #max,min,median\n","    largest_lantency = group.max()\n","    smallest_lantency = group.min()\n","    median_lantency = group.median()\n","    #down_timefirst /1000\n","    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n","    #\n","    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x <= 1)).sum())\n","    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x <= 1.5)).sum())\n","    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x <= 2)).sum())\n","    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x <= 3)).sum())\n","    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n","\n","    data.append(pd.DataFrame({\n","        'id': logs['id'].unique(),\n","         #\n","        'largest_lantency': largest_lantency,\n","        'smallest_lantency': smallest_lantency,\n","        'median_lantency': median_lantency,\n","        'initial_pause': initial_pause,\n","        'pauses_half_sec': pauses_half_sec,\n","        'pauses_1_sec': pauses_1_sec,\n","        'pauses_1_half_sec': pauses_1_half_sec,\n","        'pauses_2_sec': pauses_2_sec,\n","        'pauses_3_sec': pauses_3_sec,\n","    }).reset_index(drop=True))\n","\n","train_eD592674, test_eD592674 = data\n","\n","gc.collect()#,\n","\n","train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n","test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n","train_feats = train_feats.merge(train_scores, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#\n","train_feats=train_feats.merge(train_word_agg_df,on='id', how='left')\n","train_feats=train_feats.merge(train_sent_agg_df,on='id', how='left')\n","train_feats=train_feats.merge(train_paragraph_agg_df,on='id', how='left')\n","\n","#\n","test_feats=test_feats.merge(test_word_agg_df,on='id', how='left')\n","test_feats=test_feats.merge(test_sent_agg_df,on='id', how='left')\n","test_feats=test_feats.merge(test_paragraph_agg_df,on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#,\n","keys=train_feats.keys().values\n","unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n","print(f\"unique_cols:{unique_cols}\")\n","train_feats = train_feats.drop(columns=unique_cols)\n","test_feats = test_feats.drop(columns=unique_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#lgbm,cat,SVR\n","def make_model():\n","    \n","    #,\n","    params = {'reg_alpha': 0.007678095440286993, \n","               'reg_lambda': 0.34230534302168353, \n","               'colsample_bytree': 0.627061253588415, \n","               'subsample': 0.854942238828458, \n","               'learning_rate': 0.038697981947473245, \n","               'num_leaves': 22, \n","               'max_depth': 37, \n","               'min_child_samples': 18,\n","               'random_state': seed,\n","               'n_estimators': 150,\n","               \"objective\": \"regression\",\n","               \"metric\": \"rmse\",\n","               'force_col_wise': True,\n","               \"verbosity\": 0,\n","              }\n","    \n","    model1 = LGBMRegressor(**params)\n","    \n","    model2 = CatBoostRegressor(iterations=1000,\n","                                 learning_rate=0.1,\n","                                 depth=6,\n","                                 eval_metric='RMSE',\n","                                 random_seed = seed,\n","                                 bagging_temperature = 0.2,\n","                                 od_type='Iter',\n","                                 metric_period = 50,\n","                                 od_wait=20,\n","                                 verbose=False)\n","    \n","    model3 = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n","    \n","    models = []\n","    models.append((model1, 'lgbm'))\n","    models.append((model2, 'catboost'))\n","    models.append((model3, 'svr'))\n","    \n","    return models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_features =train_feats.drop(['score'],axis=1).keys().values \n","X_y = pd.merge(train_feats[best_features], train_scores, on='id', how='left')\n","\n","#\n","X_y.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","features = X_y.iloc[:,1:-1]#scorelabel\n","target = X_y.iloc[:,-1]#targetscore\n","\n","#RMSE\n","def RMSE(y_true,y_pred):\n","    return np.sqrt(np.mean((y_true-y_pred)**2))\n","\n","models_and_errors_dict = {}\n","\n","for model, model_type in make_model():\n","    \n","    oof_pred=np.zeros((len(features)))\n","        \n","    #10\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed + num_folds)\n","\n","    for fold, indexes in enumerate(kf.split(features), start=1):\n","\n","        # Get train and test indexes\n","        train_index, test_index = indexes\n","\n","        print(f'--- Fold #{fold} ---')       \n","\n","        # Split data into train and test sets\n","        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n","        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n","\n","        #,\n","        X_train_copy, X_test_copy = X_train.copy(), X_test.copy()\n","\n","        print(f'Training a {model_type} model on fold {fold}')\n","\n","        #\n","        if model_type in model_with_scaled_features:\n","            #\n","            imputer = SimpleImputer(strategy='mean')\n","            X_train_imputed = imputer.fit_transform(X_train.copy())\n","            X_test_imputed = imputer.transform(X_test.copy())\n","            #,[-1,1]\n","            scaler = MinMaxScaler(feature_range=(-1, 1))\n","            X_train_scaled = scaler.fit_transform(X_train_imputed)\n","            X_test_scaled = scaler.transform(X_test_imputed)\n","            #\n","            X_train_copy = X_train_scaled\n","            X_test_copy = X_test_scaled\n","        #\n","        if model_type == 'lgb':\n","            #200,,\n","            early_stopping_callback = LGBMRegressor.early_stopping(200, first_metric_only=True, verbose=False)\n","            verbose_callback = LGBMRegressor.log_evaluation(100)#100\n","\n","            model.fit(X_train_copy, y_train, eval_set=[(X_test_copy, y_test)],  \n","                      callbacks=[early_stopping_callback, verbose_callback],)\n","        else:\n","            model.fit(X_train_copy, y_train)\n","\n","        #\n","        y_hat = model.predict(X_test_copy)\n","        \n","        oof_pred[test_index]=y_hat\n","        #RMSE\n","        rmse = RMSE(y_test, y_hat)\n","        print(f'RMSE: {rmse} on fold {fold}')\n","\n","        #\n","        if model_type not in models_and_errors_dict:\n","            models_and_errors_dict[model_type] = []\n","        #,,\n","        if model_type in model_with_scaled_features:\n","            models_and_errors_dict[model_type].append((model, rmse, imputer, scaler,oof_pred))\n","        else:\n","            models_and_errors_dict[model_type].append((model, rmse, None, None,oof_pred))  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_oof_pred=models_and_errors_dict['lgbm'][9][4]\n","cat_oof_pred=models_and_errors_dict['catboost'][9][4]\n","svr_oof_pred=models_and_errors_dict['svr'][9][4]\n","margin=1000\n","target=target.values\n","current_RMSE=RMSE(target,(lgb_oof_pred+cat_oof_pred+svr_oof_pred)/3)\n","best_i=0\n","best_j=0\n","for i in range(0,margin):\n","    for j in range(0,margin-i):\n","        #k=1000-i-j\n","        blend_oof_pred=(i*lgb_oof_pred+j*cat_oof_pred+(margin-i-j)*svr_oof_pred)/margin\n","        if RMSE(target,blend_oof_pred)<current_RMSE:\n","            print(f\"current_RMSE:{current_RMSE}\")\n","            current_RMSE=RMSE(target,blend_oof_pred)\n","            best_i=i\n","            best_j=j\n","#\n","blending_weights['lgbm']=best_i/margin\n","blending_weights['catboost']=best_j/margin\n","blending_weights['svr']=(margin-best_i-best_j)/margin\n","print(f\"blending_weights:{blending_weights}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_hats = dict()\n","\n","#submission_df,idscore\n","submission_df = pd.DataFrame(test_feats['id'])\n","submission_df['score'] = 3.5#,3.5\n","\n","#test_feats\n","X_unseen = test_feats.copy()[best_features]\n","#id.\n","X_unseen.drop(columns=['id'], inplace=True)\n","#np.inf-np.infnp.nan\n","X_unseen.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","for model_name, model_info in models_and_errors_dict.items():\n","    print(f'\\n--- {model_name} ---\\n')\n","    \n","    #\n","    X_unseen_copy = X_unseen.copy()\n","    y_hats[model_name] = []#model\n","\n","    for ix, (trained_model, error, imputer, scaler,oof_pred) in enumerate(model_info, start=1):\n","        print(f\"Using model {ix} with error {error}\")\n","\n","        #,\n","        if model_name in model_with_scaled_features:\n","            #,\n","            X_unseen_imputed = imputer.transform(X_unseen_copy)\n","            X_unseen_scaled = scaler.transform(X_unseen_imputed)\n","            #\n","            y_hats[model_name].append(trained_model.predict(X_unseen_scaled))\n","        else:#\n","            y_hats[model_name].append(trained_model.predict(X_unseen_copy))\n","    #,,submission_df\n","    if y_hats[model_name]:\n","        y_hat_avg = np.mean(y_hats[model_name], axis=0)\n","        submission_df['score_' + model_name] = y_hat_avg\n","    print(\"Done.\")\n","print(\"blending\")\n","blended_score=np.zeros((len(test_essays_copy)))\n","for k, v in blending_weights.items():\n","    blended_score += submission_df['score_' + k] * v\n","print(f\"blended_score:{blended_score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#\n","num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n","activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n","\n","#dfcolnamevaluescount.\n","def count_by_values(df, colname, values):\n","    #maintain_order=True\n","    fts = df.select(pl.col('id').unique(maintain_order=True))\n","    for i, value in enumerate(values):\n","        #idcolnamevalue,renamecolname_i_cnt\n","        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n","        #\n","        fts  = fts.join(tmp_df, on='id', how='left') \n","    return fts\n","\n","def dev_feats(df):\n","    \n","    print(\"< Count by values features >\")\n","    \n","    #activity,text_change,down_event,up_eventcount\n","    feats = count_by_values(df, 'activity', activities)\n","    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n","    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n","    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n","\n","    print(\"< Input words stats features >\")\n","    #'=>'\n","    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n","    #idtext_change,'q+'\n","    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n","    #,,,,,.\n","    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n","                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n","    #text_change,.\n","    temp = temp.drop('text_change')\n","    feats = feats.join(temp, on='id', how='left') \n","\n","    print(\"< Numerical columns features >\")\n","\n","    #action_time,,,,,,50%\n","    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n","                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n","                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n","    feats = feats.join(temp, on='id', how='left') \n","\n","\n","    print(\"< Categorical columns features >\")\n","    #n_unique,.\n","    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n","    feats = feats.join(temp, on='id', how='left') \n","\n","    print(\"< Idle time features >\")\n","    #.(https://files.eric.ed.gov/fulltext/ED592674.pdf)\n","    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n","    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n","                                   inter_key_median_lantency = pl.median('time_diff'),\n","                                   mean_pause_time = pl.mean('time_diff'),\n","                                   std_pause_time = pl.std('time_diff'),\n","                                   total_pause_time = pl.sum('time_diff'),\n","                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n","                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n","                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n","                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n","                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n","    feats = feats.join(temp, on='id', how='left') \n","    \n","    print(\"< P-bursts features >\")\n","    #dfactivityInputRemove/cut,time_diff<2\n","    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n","    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.with_columns(pl.col('time_diff')<2)\n","    #()\n","    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n","    temp = temp.drop_nulls()#\n","    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n","                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n","                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n","    feats = feats.join(temp, on='id', how='left') \n","\n","    print(\"< R-bursts features >\")\n","    #'Remove/cut'\n","    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n","    #'Remove/cut'()\n","    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n","    temp = temp.drop_nulls()#\n","    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n","                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n","                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n","    feats = feats.join(temp, on='id', how='left')\n","    \n","    return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n","\n","def word_feats(df):\n","    essay_df = df\n","    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('word')\n","    df['word_len'] = df['word'].apply(lambda x: len(x))\n","    df = df[df['word_len'] != 0]\n","\n","    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n","    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n","    word_agg_df['id'] = word_agg_df.index\n","    word_agg_df = word_agg_df.reset_index(drop=True)\n","    return word_agg_df\n","\n","\n","def sent_feats(df):\n","    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('sent')\n","    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.sent_len!=0].reset_index(drop=True)\n","\n","    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n","                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","def parag_feats(df):\n","    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n","    df = df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.paragraph_len!=0].reset_index(drop=True)\n","    \n","    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n","                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df\n","\n","#(InputRemove/Cut).\n","def product_to_keys(logs, essays):\n","    essays['product_len'] = essays.essay.str.len()#\n","    #logsid InputRemove/Cut\n","    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n","    essays = essays.merge(tmp_df, on='id', how='left')\n","    #(InputRemove/Cut).\n","    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n","    return essays[['id', 'product_to_keys']]\n","#['Input', 'Remove/Cut'].\n","def get_keys_pressed_per_second(logs):\n","    #logs['Input', 'Remove/Cut']event_id\n","    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n","    #iddown_timeup_time\n","    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n","    #id\n","    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n","    #event_id\n","    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n","    return temp_df[['id', 'keys_per_second']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#dataXdatay,model,test_X\n","#k,15..\n","def evaluate(data_x, data_y, model, random_state=seed, n_splits=5, test_x=None):\n","    #StratifiedKFold\n","    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n","    test_y = np.zeros((len(test_x), n_splits))#5\n","    for i, (train_idx, valid_idx) in enumerate(skf.split(data_x, data_y.astype(str))):\n","        train_x = data_x.iloc[train_idx]\n","        train_y = data_y[train_idx]\n","        valid_x = data_x.iloc[valid_idx]\n","        valid_y = data_y[valid_idx]\n","        model.fit(train_x, train_y)\n","        test_y[:, i] = model.predict(test_x)\n","    return np.mean(test_y, axis=1)\n","\n","\n","train_logs    = pl.scan_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n","train_feats   = dev_feats(train_logs)\n","train_feats   = train_feats.collect().to_pandas()\n","\n","print('< Essay Reconstruction >')\n","train_logs             = train_logs.collect().to_pandas()\n","train_essays           = pd.read_csv('/kaggle/input/writing-quality-challenge-constructed-essays/train_essays_fast.csv')\n","train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n","train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n","train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n","train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n","train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n","\n","#,\n","keys=train_feats.keys().values\n","unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n","print(f\"unique_cols:{unique_cols}\")\n","train_feats = train_feats.drop(columns=unique_cols)\n","\n","print('< Mapping >')\n","train_scores   = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n","data           = train_feats.merge(train_scores, on='id', how='left')\n","x              = data.drop(['id', 'score'], axis=1)\n","y              = data['score'].values\n","\n","print(f'Number of features: {len(x.columns)}')\n","\n","print('< Testing Data >')\n","test_logs   = pl.scan_csv('/kaggle/working/test_logs.csv')\n","test_feats  = dev_feats(test_logs)\n","test_feats  = test_feats.collect().to_pandas()\n","\n","test_logs             = test_logs.collect().to_pandas()\n","test_essays           = test_essays_copy\n","test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n","test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n","test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n","test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n","test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n","\n","test_feats = test_feats.drop(columns=unique_cols)\n","\n","test_ids = test_feats['id'].values\n","testin_x = test_feats.drop(['id'], axis=1)\n","\n","print('< Learning and Evaluation >')\n","lgbm_params = {'n_estimators': 1024,\n","         'learning_rate': 0.006,\n","         'metric': 'rmse',\n","         'random_state': seed,\n","         'force_col_wise': True,\n","         'verbosity': 0,}\n","solution = LGBMRegressor(**lgbm_params)\n","y_pred_lgb   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n","y_pred_lgb"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_pred = blended_score*0.4+ y_pred_lgb*0.6#\n","\n","sub3 = pd.DataFrame({'id': test_ids, 'score': y_pred})\n","sub3.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clean_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub1.rename(columns={'score': 'score_1'}, inplace=True)\n","sub2.rename(columns={'score': 'score_2'}, inplace=True)\n","sub3.rename(columns={'score': 'score_3'}, inplace=True)\n","\n","submission = pd.merge(sub1, sub2, on='id')\n","submission = pd.merge(submission, sub3, on='id')\n","\n","submission['score'] = (submission['score_1']*0.2 +  #LGBM + NN (Weighted search for \"print(W)\")\n","                       submission['score_2']*0.3 +  #LGBM Public\n","                       submission['score_3']*0.5)   #Fusion\n","\n","submission_final = submission[['id', 'score']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_final.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_final"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"},{"datasetId":3992884,"sourceId":6971449,"sourceType":"datasetVersion"},{"datasetId":3949123,"sourceId":6973319,"sourceType":"datasetVersion"},{"sourceId":150384981,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
