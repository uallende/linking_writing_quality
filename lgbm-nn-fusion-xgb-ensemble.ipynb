{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LightAutoML installation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:25:41.301074Z","iopub.status.busy":"2023-12-24T08:25:41.300533Z","iopub.status.idle":"2023-12-24T08:25:47.436051Z","shell.execute_reply":"2023-12-24T08:25:47.435015Z","shell.execute_reply.started":"2023-12-24T08:25:41.301027Z"},"papermill":{"duration":4.154316,"end_time":"2023-11-05T19:55:14.77427","exception":false,"start_time":"2023-11-05T19:55:10.619954","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["%matplotlib inline\n","import gc\n","import numpy as np\n","import pandas as pd\n","import os\n","import itertools\n","import pickle\n","import re\n","import time\n","from random import choice, choices\n","from functools import reduce\n","from tqdm import tqdm\n","from itertools import cycle\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from functools import reduce\n","from itertools import cycle\n","from scipy import stats\n","from scipy.stats import skew, kurtosis\n","from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n","import lightgbm as lgb\n","import torch"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007333,"end_time":"2023-11-05T19:55:14.789479","exception":false,"start_time":"2023-11-05T19:55:14.782146","status":"completed"},"tags":[]},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:25:47.438292Z","iopub.status.busy":"2023-12-24T08:25:47.437424Z","iopub.status.idle":"2023-12-24T08:26:03.139258Z","shell.execute_reply":"2023-12-24T08:26:03.137893Z","shell.execute_reply.started":"2023-12-24T08:25:47.438254Z"},"papermill":{"duration":17.101649,"end_time":"2023-11-05T19:55:31.898771","exception":false,"start_time":"2023-11-05T19:55:14.797122","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["INPUT_DIR = '../input/linking-writing-processes-to-writing-quality'\n","train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n","train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n","test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n","ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.14224Z","iopub.status.busy":"2023-12-24T08:26:03.141814Z","iopub.status.idle":"2023-12-24T08:26:03.261875Z","shell.execute_reply":"2023-12-24T08:26:03.260721Z","shell.execute_reply.started":"2023-12-24T08:26:03.142202Z"},"papermill":{"duration":0.155263,"end_time":"2023-11-05T19:55:32.062212","exception":false,"start_time":"2023-11-05T19:55:31.906949","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_essays = pd.read_csv('../input/writing-quality-challenge-constructed-essays/train_essays_02.csv')\n","train_essays.index = train_essays[\"Unnamed: 0\"]\n","train_essays.index.name = None\n","train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n","train_essays.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.263686Z","iopub.status.busy":"2023-12-24T08:26:03.263304Z","iopub.status.idle":"2023-12-24T08:26:03.280275Z","shell.execute_reply":"2023-12-24T08:26:03.278785Z","shell.execute_reply.started":"2023-12-24T08:26:03.263652Z"},"papermill":{"duration":0.029497,"end_time":"2023-11-05T19:55:32.116127","exception":false,"start_time":"2023-11-05T19:55:32.08663","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Function to construct essays copied from here (small adjustments): https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor\n","\n","def getEssays(df):\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']]\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","    lastIndex = 0\n","    essaySeries = pd.Series()\n","    for index, valCount in enumerate(valCountsArr):\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","        lastIndex += valCount\n","        essayText = \"\"\n","        for Input in currTextInput.values:\n","            if Input[0] == 'Replace':\n","                replaceTxt = Input[2].split(' => ')\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +\\\n","                essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","            if Input[0] == 'Paste':\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","            if Input[0] == 'Remove/Cut':\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","            if \"M\" in Input[0]:\n","                croppedTxt = Input[0][10:]\n","                splitTxt = croppedTxt.split(' To ')\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                moveData = (int(valueArr[0][0][1:]), \n","                            int(valueArr[0][1][:-1]), \n","                            int(valueArr[1][0][1:]), \n","                            int(valueArr[1][1][:-1]))\n","                if moveData[0] != moveData[2]:\n","                    if moveData[0] < moveData[2]:\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n","                        essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n","                        essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","        essaySeries[index] = essayText\n","    essaySeries.index =  textInputDf['id'].unique()\n","    return pd.DataFrame(essaySeries, columns=['essay'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.283004Z","iopub.status.busy":"2023-12-24T08:26:03.282372Z","iopub.status.idle":"2023-12-24T08:26:03.294528Z","shell.execute_reply":"2023-12-24T08:26:03.293425Z","shell.execute_reply.started":"2023-12-24T08:26:03.282953Z"},"papermill":{"duration":0.016764,"end_time":"2023-11-05T19:55:32.141238","exception":false,"start_time":"2023-11-05T19:55:32.124474","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Helper functions\n","\n","def q1(x):\n","    return x.quantile(0.25)\n","def q3(x):\n","    return x.quantile(0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.296441Z","iopub.status.busy":"2023-12-24T08:26:03.296115Z","iopub.status.idle":"2023-12-24T08:26:03.312637Z","shell.execute_reply":"2023-12-24T08:26:03.311372Z","shell.execute_reply.started":"2023-12-24T08:26:03.296413Z"},"papermill":{"duration":0.031457,"end_time":"2023-11-05T19:55:32.18096","exception":false,"start_time":"2023-11-05T19:55:32.149503","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n","\n","def split_essays_into_sentences(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    essay_df = essay_df.explode('sent')\n","    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_sentence_aggregations(df):\n","    sent_agg_df = pd.concat(\n","        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    )\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","def split_essays_into_paragraphs(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n","    essay_df = essay_df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_paragraph_aggregations(df):\n","    paragraph_agg_df = pd.concat(\n","        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    ) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:03.315313Z","iopub.status.busy":"2023-12-24T08:26:03.314892Z","iopub.status.idle":"2023-12-24T08:26:11.134343Z","shell.execute_reply":"2023-12-24T08:26:11.133072Z","shell.execute_reply.started":"2023-12-24T08:26:03.315279Z"},"papermill":{"duration":8.195247,"end_time":"2023-11-05T19:55:40.384252","exception":false,"start_time":"2023-11-05T19:55:32.189005","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Sentence features for train dataset\n","train_sent_df = split_essays_into_sentences(train_essays)\n","train_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n","# plt.figure(figsize=(15, 1.5))\n","# plt.boxplot(x=train_sent_df.sent_len, vert=False, labels=['Sentence length'])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:11.13701Z","iopub.status.busy":"2023-12-24T08:26:11.136459Z","iopub.status.idle":"2023-12-24T08:26:18.475339Z","shell.execute_reply":"2023-12-24T08:26:18.474424Z","shell.execute_reply.started":"2023-12-24T08:26:11.136962Z"},"papermill":{"duration":8.110471,"end_time":"2023-11-05T19:55:48.50333","exception":false,"start_time":"2023-11-05T19:55:40.392859","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Paragraph features for train dataset\n","train_paragraph_df = split_essays_into_paragraphs(train_essays)\n","train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n","# plt.figure(figsize=(15, 1.5))\n","# plt.boxplot(x=train_paragraph_df.paragraph_len, vert=False, labels=['Paragraph length'])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:18.482009Z","iopub.status.busy":"2023-12-24T08:26:18.481054Z","iopub.status.idle":"2023-12-24T08:26:18.560272Z","shell.execute_reply":"2023-12-24T08:26:18.558987Z","shell.execute_reply.started":"2023-12-24T08:26:18.481964Z"},"papermill":{"duration":0.09158,"end_time":"2023-11-05T19:55:48.603596","exception":false,"start_time":"2023-11-05T19:55:48.512016","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Features for test dataset\n","test_essays = getEssays(test_logs)\n","test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n","test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:26:18.562287Z","iopub.status.busy":"2023-12-24T08:26:18.561891Z","iopub.status.idle":"2023-12-24T08:26:18.608778Z","shell.execute_reply":"2023-12-24T08:26:18.60776Z","shell.execute_reply.started":"2023-12-24T08:26:18.562254Z"},"papermill":{"duration":0.070872,"end_time":"2023-11-05T19:55:48.683261","exception":false,"start_time":"2023-11-05T19:55:48.612389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# The following code comes almost Abdullah's notebook: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","# Abdullah's code is based on work shared in previous notebooks (e.g., https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline)\n","\n","from collections import defaultdict\n","\n","class Preprocessor:\n","    \n","    def __init__(self, seed):\n","        self.seed = seed\n","        \n","        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n","              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n","        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n","                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n","        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n","        \n","        self.idf = defaultdict(float)\n","    \n","    def activity_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['activity'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.activities:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","\n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","    def event_counts(self, df, colname):\n","        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df[colname].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.events:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","    def text_change_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['text_change'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.text_changes:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","            \n","        return ret\n","\n","    def match_punctuations(self, df):\n","        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['down_event'].values):\n","            cnt = 0\n","            items = list(Counter(li).items())\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in self.punctuations:\n","                    cnt += v\n","            ret.append(cnt)\n","        ret = pd.DataFrame({'punct_cnt': ret})\n","        return ret\n","\n","    def get_input_words(self, df):\n","        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n","        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n","        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n","        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df.drop(['text_change'], axis=1, inplace=True)\n","        return tmp_df\n","    \n","    def make_feats(self, df):\n","        \n","        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n","        \n","        print(\"Engineering time data\")\n","        for gap in self.gaps:\n","            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n","            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n","        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        print(\"Engineering cursor position data\")\n","        for gap in self.gaps:\n","            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n","            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n","            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n","        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        print(\"Engineering word count data\")\n","        for gap in self.gaps:\n","            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n","            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n","            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n","        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n","        \n","        print(\"Engineering statistical summaries for features\")\n","        feats_stat = [\n","            ('event_id', ['max']),\n","            ('up_time', ['max']),\n","            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","            ('activity', ['nunique']),\n","            ('down_event', ['nunique']),\n","            ('up_event', ['nunique']),\n","            ('text_change', ['nunique']),\n","            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n","            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n","        for gap in self.gaps:\n","            feats_stat.extend([\n","                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n","            ])\n","        \n","        pbar = tqdm(feats_stat)\n","        for item in pbar:\n","            colname, methods = item[0], item[1]\n","            for method in methods:\n","                pbar.set_postfix()\n","                if isinstance(method, str):\n","                    method_name = method\n","                else:\n","                    method_name = method.__name__\n","                pbar.set_postfix(column=colname, method=method_name)\n","                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n","                feats = feats.merge(tmp_df, on='id', how='left')\n","\n","        print(\"Engineering activity counts data\")\n","        tmp_df = self.activity_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering event counts data\")\n","        tmp_df = self.event_counts(df, 'down_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        tmp_df = self.event_counts(df, 'up_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering text change counts data\")\n","        tmp_df = self.text_change_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering punctuation counts data\")\n","        tmp_df = self.match_punctuations(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","\n","        print(\"Engineering input words data\")\n","        tmp_df = self.get_input_words(df)\n","        feats = pd.merge(feats, tmp_df, on='id', how='left')\n","\n","        print(\"Engineering ratios data\")\n","        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n","        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n","        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n","        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n","\n","        return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-24T08:26:18.611191Z","iopub.status.busy":"2023-12-24T08:26:18.610117Z","iopub.status.idle":"2023-12-24T08:30:55.672484Z","shell.execute_reply":"2023-12-24T08:30:55.671342Z","shell.execute_reply.started":"2023-12-24T08:26:18.611149Z"},"papermill":{"duration":370.664062,"end_time":"2023-11-05T20:01:59.356059","exception":false,"start_time":"2023-11-05T19:55:48.691997","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["preprocessor = Preprocessor(seed=42)\n","train_feats = preprocessor.make_feats(train_logs)\n","test_feats = preprocessor.make_feats(test_logs)\n","nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n","train_feats = train_feats.drop(columns=nan_cols)\n","test_feats = test_feats.drop(columns=nan_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:30:55.674891Z","iopub.status.busy":"2023-12-24T08:30:55.67421Z","iopub.status.idle":"2023-12-24T08:31:00.683022Z","shell.execute_reply":"2023-12-24T08:31:00.681615Z","shell.execute_reply.started":"2023-12-24T08:30:55.67485Z"},"papermill":{"duration":5.754153,"end_time":"2023-11-05T20:02:05.211536","exception":false,"start_time":"2023-11-05T20:01:59.457383","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Code for additional aggregations comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","\n","train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n","    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n","train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n","train_agg_fe_df.reset_index(inplace=True)\n","\n","test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n","    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n","test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n","test_agg_fe_df.reset_index(inplace=True)\n","\n","train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n","test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:00.686024Z","iopub.status.busy":"2023-12-24T08:31:00.685218Z","iopub.status.idle":"2023-12-24T08:31:09.846325Z","shell.execute_reply":"2023-12-24T08:31:09.845204Z","shell.execute_reply.started":"2023-12-24T08:31:00.685971Z"},"papermill":{"duration":11.523742,"end_time":"2023-11-05T20:02:16.835416","exception":false,"start_time":"2023-11-05T20:02:05.311674","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Code for creating these features comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","# Idea is based on features introduced in Section 3 of this research paper: https://files.eric.ed.gov/fulltext/ED592674.pdf\n","\n","data = []\n","\n","for logs in [train_logs, test_logs]:\n","    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n","    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n","\n","    group = logs.groupby('id')['time_diff']\n","    largest_lantency = group.max()\n","    smallest_lantency = group.min()\n","    median_lantency = group.median()\n","    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n","    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n","    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n","    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n","    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n","    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n","\n","    data.append(pd.DataFrame({\n","        'id': logs['id'].unique(),\n","        'largest_lantency': largest_lantency,\n","        'smallest_lantency': smallest_lantency,\n","        'median_lantency': median_lantency,\n","        'initial_pause': initial_pause,\n","        'pauses_half_sec': pauses_half_sec,\n","        'pauses_1_sec': pauses_1_sec,\n","        'pauses_1_half_sec': pauses_1_half_sec,\n","        'pauses_2_sec': pauses_2_sec,\n","        'pauses_3_sec': pauses_3_sec,\n","    }).reset_index(drop=True))\n","\n","train_eD592674, test_eD592674 = data\n","\n","train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n","test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n","train_feats = train_feats.merge(train_scores, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.848795Z","iopub.status.busy":"2023-12-24T08:31:09.847778Z","iopub.status.idle":"2023-12-24T08:31:09.888317Z","shell.execute_reply":"2023-12-24T08:31:09.887011Z","shell.execute_reply.started":"2023-12-24T08:31:09.848753Z"},"papermill":{"duration":0.139952,"end_time":"2023-11-05T20:02:17.075043","exception":false,"start_time":"2023-11-05T20:02:16.935091","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Adding the additional features to the original feature set\n","\n","train_feats = train_feats.merge(train_sent_agg_df, on='id', how='left')\n","train_feats = train_feats.merge(train_paragraph_agg_df, on='id', how='left')\n","test_feats = test_feats.merge(test_sent_agg_df, on='id', how='left')\n","test_feats = test_feats.merge(test_paragraph_agg_df, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.890627Z","iopub.status.busy":"2023-12-24T08:31:09.890075Z","iopub.status.idle":"2023-12-24T08:31:09.895923Z","shell.execute_reply":"2023-12-24T08:31:09.894991Z","shell.execute_reply.started":"2023-12-24T08:31:09.890586Z"},"papermill":{"duration":0.111533,"end_time":"2023-11-05T20:02:17.288094","exception":false,"start_time":"2023-11-05T20:02:17.176561","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["target_col = ['score']\n","drop_cols = ['id']\n","train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.898825Z","iopub.status.busy":"2023-12-24T08:31:09.897594Z","iopub.status.idle":"2023-12-24T08:31:09.911032Z","shell.execute_reply":"2023-12-24T08:31:09.909888Z","shell.execute_reply.started":"2023-12-24T08:31:09.89879Z"},"trusted":true},"outputs":[],"source":["len(test_feats)"]},{"cell_type":"markdown","metadata":{},"source":["# LightGBM train and predict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:31:09.913306Z","iopub.status.busy":"2023-12-24T08:31:09.912919Z","iopub.status.idle":"2023-12-24T08:31:09.920509Z","shell.execute_reply":"2023-12-24T08:31:09.919481Z","shell.execute_reply.started":"2023-12-24T08:31:09.913271Z"},"trusted":true},"outputs":[],"source":["OOF_PREDS = np.zeros((len(train_feats), 2))\n","TEST_PREDS = np.zeros((len(test_feats), 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-24T08:31:09.922966Z","iopub.status.busy":"2023-12-24T08:31:09.922543Z","iopub.status.idle":"2023-12-24T08:36:19.909905Z","shell.execute_reply":"2023-12-24T08:36:19.908642Z","shell.execute_reply.started":"2023-12-24T08:31:09.922933Z"},"papermill":{"duration":382.8295,"end_time":"2023-11-05T20:08:40.219879","exception":false,"start_time":"2023-11-05T20:02:17.390379","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["# Code comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n","\n","models_dict = {}\n","scores = []\n","\n","test_predict_list = []\n","best_params = {'reg_alpha': 0.007678095440286993, \n","               'reg_lambda': 0.34230534302168353, \n","               'colsample_bytree': 0.627061253588415, \n","               'subsample': 0.854942238828458, \n","               'learning_rate': 0.04,   #0.038697981947473245, \n","               'num_leaves': 22, \n","               'max_depth': 37, \n","               'min_child_samples': 18,\n","               'n_jobs':4\n","              }\n","\n","for i in range(5): \n","    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n","    oof_valid_preds = np.zeros(train_feats.shape[0])\n","    X_test = test_feats[train_cols]\n","    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","        \n","        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","        params = {\n","            \"objective\": \"regression\",\n","            \"metric\": \"rmse\",\n","            'random_state': 42,\n","            \"n_estimators\" : 12001,\n","            \"verbosity\": -1,\n","            **best_params\n","        }\n","        model = lgb.LGBMRegressor(**params)\n","        early_stopping_callback = lgb.early_stopping(100, first_metric_only=True, verbose=False)\n","        \n","        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n","                  callbacks=[early_stopping_callback],\n","        )\n","        valid_predict = model.predict(X_valid)\n","        oof_valid_preds[valid_idx] = valid_predict\n","        OOF_PREDS[valid_idx, 0] += valid_predict / 5\n","        test_predict = model.predict(X_test)\n","        TEST_PREDS[:, 0] += test_predict / 5 / 10\n","        test_predict_list.append(test_predict)\n","        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n","        models_dict[f'{fold}_{i}'] = model\n","\n","    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n","    scores.append(oof_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:36:19.912105Z","iopub.status.busy":"2023-12-24T08:36:19.911732Z","iopub.status.idle":"2023-12-24T08:36:19.922465Z","shell.execute_reply":"2023-12-24T08:36:19.921199Z","shell.execute_reply.started":"2023-12-24T08:36:19.912072Z"},"trusted":true},"outputs":[],"source":["print('OOF metric LGBM = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], \n","                                                                   OOF_PREDS[:, 0], \n","                                                                   squared=False)))"]},{"cell_type":"markdown","metadata":{},"source":["# LightAutoML NN (DenseLight) prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:36:19.924501Z","iopub.status.busy":"2023-12-24T08:36:19.924137Z","iopub.status.idle":"2023-12-24T08:37:01.656868Z","shell.execute_reply":"2023-12-24T08:37:01.655462Z","shell.execute_reply.started":"2023-12-24T08:36:19.924467Z"},"trusted":true},"outputs":[],"source":["from lightautoml.automl.presets.tabular_presets import TabularAutoML\n","from lightautoml.tasks import Task\n","import joblib\n","\n","# def use_plr(USE_PLR):\n","#     if USE_PLR:\n","#         return \"plr\"\n","#     else:\n","#         return \"cont\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-24T08:37:01.660098Z","iopub.status.busy":"2023-12-24T08:37:01.659083Z","iopub.status.idle":"2023-12-24T08:38:00.259007Z","shell.execute_reply":"2023-12-24T08:38:00.257769Z","shell.execute_reply.started":"2023-12-24T08:37:01.660053Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["for i in range(3):\n","    oof_pred, automl = joblib.load('/kaggle/input/linkinglamamodels/oof_and_lama_denselight_{}.pkl'.format(i))\n","    OOF_PREDS[:, 1] += oof_pred / 3\n","    TEST_PREDS[:, 1] += automl.predict(test_feats[train_cols]).data[:, 0] / 3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:00.263176Z","iopub.status.busy":"2023-12-24T08:38:00.262664Z","iopub.status.idle":"2023-12-24T08:38:00.273721Z","shell.execute_reply":"2023-12-24T08:38:00.272602Z","shell.execute_reply.started":"2023-12-24T08:38:00.263126Z"},"trusted":true},"outputs":[],"source":["print('OOF metric LightAutoML_NN = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], \n","                                                                               OOF_PREDS[:, 1], \n","                                                                               squared=False)))"]},{"cell_type":"markdown","metadata":{},"source":["# Blending"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:00.276337Z","iopub.status.busy":"2023-12-24T08:38:00.275714Z","iopub.status.idle":"2023-12-24T08:38:02.308343Z","shell.execute_reply":"2023-12-24T08:38:02.30731Z","shell.execute_reply.started":"2023-12-24T08:38:00.276292Z"},"trusted":true},"outputs":[],"source":["best_sc = 1\n","for w in np.arange(0, 1.01, 0.001):\n","    sc = metrics.mean_squared_error(train_feats[target_col], \n","                                    w * OOF_PREDS[:, 0] + (1-w) * OOF_PREDS[:, 1], \n","                                    squared=False)\n","    if sc < best_sc:\n","        best_sc = sc\n","        best_w = w\n","        \n","print('Composition OOF score = {:.5f}'.format(best_sc))\n","print('Composition best W = {:.3f}'.format(best_w))"]},{"cell_type":"markdown","metadata":{},"source":["# Submission creation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.309951Z","iopub.status.busy":"2023-12-24T08:38:02.309629Z","iopub.status.idle":"2023-12-24T08:38:02.318528Z","shell.execute_reply":"2023-12-24T08:38:02.317543Z","shell.execute_reply.started":"2023-12-24T08:38:02.309922Z"},"trusted":true},"outputs":[],"source":["W = [best_w, 1 - best_w]\n","print(W)\n","test_preds = TEST_PREDS[:, 0] * W[0] + TEST_PREDS[:, 1] * W[1]\n","test_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.320281Z","iopub.status.busy":"2023-12-24T08:38:02.319931Z","iopub.status.idle":"2023-12-24T08:38:02.334213Z","shell.execute_reply":"2023-12-24T08:38:02.333261Z","shell.execute_reply.started":"2023-12-24T08:38:02.320249Z"},"trusted":true},"outputs":[],"source":["test_feats['score'] = test_preds\n","sub1 = test_feats[['id', 'score']]\n","#test_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.335955Z","iopub.status.busy":"2023-12-24T08:38:02.335502Z","iopub.status.idle":"2023-12-24T08:38:02.351253Z","shell.execute_reply":"2023-12-24T08:38:02.350495Z","shell.execute_reply.started":"2023-12-24T08:38:02.335924Z"},"trusted":true},"outputs":[],"source":["sub1"]},{"cell_type":"markdown","metadata":{},"source":["# Saving OOFs and test predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.360136Z","iopub.status.busy":"2023-12-24T08:38:02.359747Z","iopub.status.idle":"2023-12-24T08:38:02.370475Z","shell.execute_reply":"2023-12-24T08:38:02.369394Z","shell.execute_reply.started":"2023-12-24T08:38:02.360096Z"},"trusted":true},"outputs":[],"source":["joblib.dump((OOF_PREDS, TEST_PREDS), 'OOF_and_TEST_preds.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["# Public LGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.372781Z","iopub.status.busy":"2023-12-24T08:38:02.37176Z","iopub.status.idle":"2023-12-24T08:38:02.915232Z","shell.execute_reply":"2023-12-24T08:38:02.914284Z","shell.execute_reply.started":"2023-12-24T08:38:02.372746Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import gc\n","import ctypes\n","def clean_memory():\n","    gc.collect()\n","    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()\n","clean_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:02.917668Z","iopub.status.busy":"2023-12-24T08:38:02.917005Z","iopub.status.idle":"2023-12-24T08:38:03.030071Z","shell.execute_reply":"2023-12-24T08:38:03.029076Z","shell.execute_reply.started":"2023-12-24T08:38:02.917631Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","%matplotlib inline\n","import gc\n","import os\n","import itertools\n","import pickle\n","\n","from random import choice, choices\n","from functools import reduce\n","from tqdm import tqdm\n","from itertools import cycle\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from functools import reduce\n","from itertools import cycle\n","from scipy import stats\n","from scipy.stats import skew, kurtosis\n","from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n","from transformers import BertTokenizer\n","import warnings\n","\n","import os\n","import gc\n","import re\n","import random\n","from collections import Counter, defaultdict\n","import pprint\n","import time\n","import copy\n","\n","\n","import seaborn as sns\n","from tqdm.autonotebook import tqdm\n","\n","# from gensim.models import Word2Vec\n","from sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\n","from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD, PCA\n","from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n","from sklearn.svm import SVR\n","from sklearn.pipeline import Pipeline\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import Lasso, Ridge, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.impute import SimpleImputer\n","\n","import lightgbm as lgb\n","import xgboost as xgb\n","import catboost as cb\n","import optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:03.032294Z","iopub.status.busy":"2023-12-24T08:38:03.031494Z","iopub.status.idle":"2023-12-24T08:38:14.185902Z","shell.execute_reply":"2023-12-24T08:38:14.184806Z","shell.execute_reply.started":"2023-12-24T08:38:03.032257Z"},"trusted":true},"outputs":[],"source":["traindf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n","train_scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n","testdf = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:14.188071Z","iopub.status.busy":"2023-12-24T08:38:14.187459Z","iopub.status.idle":"2023-12-24T08:38:14.20611Z","shell.execute_reply":"2023-12-24T08:38:14.204917Z","shell.execute_reply.started":"2023-12-24T08:38:14.188036Z"},"trusted":true},"outputs":[],"source":["def getEssays(df):\n","    # Copy required columns\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']].copy()\n","    \n","    # Get rid of text inputs that make no change\n","    # Note: Shift was unpreditcable so ignored\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","\n","    # Get how much each Id there is\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","\n","    # Holds the final index of the previous Id\n","    lastIndex = 0\n","\n","    # Holds all the essays\n","    essaySeries = pd.Series()\n","\n","    # Fills essay series with essays\n","    for index, valCount in enumerate(valCountsArr):\n","\n","        # Indexes down_time at current Id\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","\n","        # Update the last index\n","        lastIndex += valCount\n","\n","        # Where the essay content will be stored\n","        essayText = \"\"\n","\n","        \n","        # Produces the essay\n","        for Input in currTextInput.values:\n","            \n","            # Input[0] = activity\n","            # Input[2] = cursor_position\n","            # Input[3] = text_change\n","            \n","            # If activity = Replace\n","            if Input[0] == 'Replace':\n","                # splits text_change at ' => '\n","                replaceTxt = Input[2].split(' => ')\n","                \n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","\n","                \n","            # If activity = Paste    \n","            if Input[0] == 'Paste':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Remove/Cut\n","            if Input[0] == 'Remove/Cut':\n","                # DONT TOUCH\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","\n","                \n","            # If activity = Move...\n","            if \"M\" in Input[0]:\n","                # Gets rid of the \"Move from to\" text\n","                croppedTxt = Input[0][10:]\n","                \n","                # Splits cropped text by ' To '\n","                splitTxt = croppedTxt.split(' To ')\n","                \n","                # Splits split text again by ', ' for each item\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                \n","                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n","                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","\n","                # Skip if someone manages to activiate this by moving to same place\n","                if moveData[0] != moveData[2]:\n","                    # Check if they move text forward in essay (they are different)\n","                    if moveData[0] < moveData[2]:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    else:\n","                        # DONT TOUCH\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","                \n","                \n","            # If just input\n","            # DONT TOUCH\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","\n","            \n","        # Sets essay at index  \n","        essaySeries[index] = essayText\n","     \n","    \n","    # Sets essay series index to the ids\n","    essaySeries.index =  textInputDf['id'].unique()\n","    \n","    \n","    # Returns the essay series\n","    return essaySeries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T08:38:14.208367Z","iopub.status.busy":"2023-12-24T08:38:14.207919Z"},"trusted":true},"outputs":[],"source":["%%time\n","train_essays = getEssays(traindf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_essays = getEssays(testdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_essaysdf = pd.DataFrame({'id': train_essays.index, 'essay': train_essays.values})\n","test_essaysdf = pd.DataFrame({'id': test_essays.index, 'essay': test_essays.values})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["merged_data = train_essaysdf.merge(train_scores, on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n","X_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\n","X_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\n","count_vectorizer.get_feature_names_out() #ADDED\n","y = merged_data['score']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train = pd.DataFrame()\n","df_test = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_tokenizer_train = X_tokenizer_train.todense()\n","X_tokenizer_test = X_tokenizer_test.todense()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(X_tokenizer_train.shape[1]) : \n","    L = list(X_tokenizer_train[:,i])\n","    li = [int(x) for x in L ]\n","    df_train[f'feature {i}'] = li"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(X_tokenizer_test.shape[1]) : \n","    L = list(X_tokenizer_test[:,i])\n","    li = [int(x) for x in L ]\n","    df_test[f'feature {i}'] = li"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train_index = train_essaysdf['id']\n","df_test_index = test_essaysdf['id']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train.loc[:, 'id'] = df_train_index\n","df_test.loc[:, 'id'] = df_test_index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_agg_fe_df = traindf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n","train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n","train_agg_fe_df.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_agg_fe_df = testdf.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n","test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n","test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n","test_agg_fe_df.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","class Preprocessor:\n","    \n","    def __init__(self, seed):\n","        self.seed = seed\n","        \n","        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n","              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n","        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n","                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n","        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n","        \n","        self.idf = defaultdict(float)\n","#         self.gaps = [1, 2]\n","    \n","    def activity_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['activity'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.activities:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","\n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","\n","    def event_counts(self, df, colname):\n","        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df[colname].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.events:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","\n","    def text_change_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['text_change'].values):\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.text_changes:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","            \n","        return ret\n","\n","    def match_punctuations(self, df):\n","        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n","        ret = list()\n","        for li in tqdm(tmp_df['down_event'].values):\n","            cnt = 0\n","            items = list(Counter(li).items())\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in self.punctuations:\n","                    cnt += v\n","            ret.append(cnt)\n","        ret = pd.DataFrame({'punct_cnt': ret})\n","        return ret\n","\n","\n","    def get_input_words(self, df):\n","        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n","        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n","        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n","        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df.drop(['text_change'], axis=1, inplace=True)\n","        return tmp_df\n","    \n","    def make_feats(self, df):\n","        \n","        print(\"Starting to engineer features\")\n","        \n","        # initialize features dataframe\n","        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n","        \n","        # get shifted features\n","        # time shift\n","        print(\"Engineering time data\")\n","        for gap in self.gaps:\n","            print(f\"> for gap {gap}\")\n","            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n","            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n","        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        # cursor position shift\n","        print(\"Engineering cursor position data\")\n","        for gap in self.gaps:\n","            print(f\"> for gap {gap}\")\n","            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n","            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n","            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n","        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        # word count shift\n","        print(\"Engineering word count data\")\n","        for gap in self.gaps:\n","            print(f\"> for gap {gap}\")\n","            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n","            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n","            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n","        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n","        \n","        # get aggregate statistical features\n","        print(\"Engineering statistical summaries for features\")\n","        # [(feature name, [ stat summaries to add ])]\n","        feats_stat = [\n","            ('event_id', ['max']),\n","            ('up_time', ['max']),\n","            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","            ('activity', ['nunique']),\n","            ('down_event', ['nunique']),\n","            ('up_event', ['nunique']),\n","            ('text_change', ['nunique']),\n","            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n","            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n","        for gap in self.gaps:\n","            feats_stat.extend([\n","                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n","            ])\n","        \n","        pbar = tqdm(feats_stat)\n","        for item in pbar:\n","            colname, methods = item[0], item[1]\n","            for method in methods:\n","                pbar.set_postfix()\n","                if isinstance(method, str):\n","                    method_name = method\n","                else:\n","                    method_name = method.__name__\n","                    \n","                pbar.set_postfix(column=colname, method=method_name)\n","                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n","                feats = feats.merge(tmp_df, on='id', how='left')\n","\n","        # counts\n","        print(\"Engineering activity counts data\")\n","        tmp_df = self.activity_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering event counts data\")\n","        tmp_df = self.event_counts(df, 'down_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        tmp_df = self.event_counts(df, 'up_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering text change counts data\")\n","        tmp_df = self.text_change_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering punctuation counts data\")\n","        tmp_df = self.match_punctuations(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","\n","        # input words\n","        print(\"Engineering input words data\")\n","        tmp_df = self.get_input_words(df)\n","        feats = pd.merge(feats, tmp_df, on='id', how='left')\n","\n","        # compare feats\n","        print(\"Engineering ratios data\")\n","        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n","        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n","        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n","        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n","        \n","        print(\"Done!\")\n","        return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preprocessor = Preprocessor(seed=42)\n","\n","print(\"Engineering features for training data\")\n","\n","other_train_feats = preprocessor.make_feats(traindf)\n","\n","print()\n","print(\"-\"*25)\n","print(\"Engineering features for test data\")\n","print(\"-\"*25)\n","other_test_feats = preprocessor.make_feats(testdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train_all = pd.DataFrame()\n","df_test_all = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train_all = df_train.merge(train_agg_fe_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_test_all = df_test.merge(test_agg_fe_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def q1(x):\n","    return x.quantile(0.25)\n","def q3(x):\n","    return x.quantile(0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n","\n","def split_essays_into_sentences(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',str(x)))\n","    essay_df = essay_df.explode('sent')\n","    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.columns.tolist()].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_sentence_aggregations(df):\n","    sent_agg_df = pd.concat(\n","        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    )\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","def split_essays_into_paragraphs(df):\n","    essay_df = df\n","    essay_df['id'] = essay_df.index\n","    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: str(x).split('\\n'))\n","    essay_df = essay_df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_paragraph_aggregations(df):\n","    paragraph_agg_df = pd.concat(\n","        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    ) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sent_df = split_essays_into_sentences(train_essaysdf)\n","train_sent_agg_df = compute_sentence_aggregations(train_sent_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_paragraph_df = split_essays_into_paragraphs(train_essaysdf)\n","train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essaysdf))\n","test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essaysdf))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_paragraph_agg_df.loc[:, 'id'] = df_train_index\n","train_sent_agg_df.loc[:, 'id'] = df_train_index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_paragraph_agg_df.loc[:, 'id'] = df_test_index\n","test_sent_agg_df.loc[:, 'id'] = df_test_index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_train_feats = pd.DataFrame()\n","new_test_feats = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_train_feats = train_paragraph_agg_df.merge(df_train_all,on='id')\n","new_train_feats = new_train_feats.merge(train_sent_agg_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_test_feats = test_paragraph_agg_df.merge(df_test_all,on='id')\n","new_test_feats = new_test_feats.merge(test_sent_agg_df,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feats = pd.DataFrame()\n","test_feats = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feats = new_train_feats.merge(other_train_feats,on='id')\n","test_feats = new_test_feats.merge(other_test_feats,on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = []\n","\n","for logs in [traindf, testdf]:\n","    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n","    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n","\n","    group = logs.groupby('id')['time_diff']\n","    largest_lantency = group.max()\n","    smallest_lantency = group.min()\n","    median_lantency = group.median()\n","    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n","    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n","    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n","    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n","    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n","    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n","\n","    data.append(pd.DataFrame({\n","        'id': logs['id'].unique(),\n","        'largest_lantency': largest_lantency,\n","        'smallest_lantency': smallest_lantency,\n","        'median_lantency': median_lantency,\n","        'initial_pause': initial_pause,\n","        'pauses_half_sec': pauses_half_sec,\n","        'pauses_1_sec': pauses_1_sec,\n","        'pauses_1_half_sec': pauses_1_half_sec,\n","        'pauses_2_sec': pauses_2_sec,\n","        'pauses_3_sec': pauses_3_sec,\n","    }).reset_index(drop=True))\n","\n","train_eD592674, test_eD592674 = data\n","\n","train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n","test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n","train_feats = train_feats.merge(train_scores, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","\n","train_feats['score_class'] = le.fit_transform(train_feats['score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target_col = ['score']\n","\n","drop_cols = ['id', 'score_class']\n","train_cols = list()\n","\n","train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]\n","\n","train_cols.__len__(), target_col.__len__()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n","nan_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for col in nan_cols:\n","    mode_value_train = train_feats[col].mode()[0]  # In case there are multiple modes, choose the first one\n","    train_feats[col].fillna(mode_value_train, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for col in test_feats.columns[test_feats.isna().any()].tolist():\n","    # Find the most frequent value in the training set for the current feature\n","    most_frequent_value_train = train_feats[col].mode()[0]\n","    \n","    # Fill missing values in the test set with the most frequent value from the training set\n","    test_feats[col].fillna(most_frequent_value_train, inplace=True)\n","\n","train_feats.shape, test_feats.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feats.columns[train_feats.isna().any()].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nan_values_test = test_feats.columns[test_feats.isna().any()].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clean_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models_dict = {}\n","scores = []\n","\n","test_predict_list = []\n","best_params = {'boosting_type': 'gbdt', \n","               'metric': 'rmse',\n","               'reg_alpha': 0.003188447814669599, \n","               'reg_lambda': 0.0010228604507564066, \n","               'colsample_bytree': 0.5420247656839267, \n","               'subsample': 0.9778252382803456, \n","               'feature_fraction': 0.8,\n","               'bagging_freq': 1,\n","               'bagging_fraction': 0.75,\n","               'learning_rate': 0.01716485155812008, \n","               'num_leaves': 19, \n","               'min_child_samples': 46,\n","               'verbosity': -1,\n","               'random_state': 42,\n","               'n_estimators': 500,\n","               'device_type': 'cpu'}\n","\n","for i in range(5): \n","    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n","\n","    oof_valid_preds = np.zeros(train_feats.shape[0], )\n","\n","    X_test = test_feats[train_cols]\n","\n","\n","    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","\n","        print(\"==-\"* 50)\n","        print(\"Fold : \", fold)\n","\n","        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","\n","        print(\"Trian :\", X_train.shape, y_train.shape)\n","        print(\"Valid :\", X_valid.shape, y_valid.shape)\n","\n","        params = {\n","            \"objective\": \"regression\",\n","            \"metric\": \"rmse\",\n","            'random_state': 42,\n","            \"n_estimators\" : 12001,\n","            \"verbosity\": -1,\n","            \"device_type\": \"cpu\",\n","            **best_params\n","        }\n","\n","        model = lgb.LGBMRegressor(**params)\n","\n","        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n","        verbose_callback = lgb.callback.record_evaluation({})\n","\n","        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n","                  callbacks=[early_stopping_callback, verbose_callback],\n","        )\n","\n","        valid_predict = model.predict(X_valid)\n","        oof_valid_preds[valid_idx] = valid_predict\n","\n","        test_predict = model.predict(X_test)\n","        test_predict_list.append(test_predict)\n","\n","        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n","        print(\"Fold RMSE Score : \", score)\n","\n","        models_dict[f'{fold}_{i}'] = model\n","\n","\n","    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n","    scores.append(oof_score)\n","    print(\"OOF RMSE Score : \", oof_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_importances_values = np.asarray([model.feature_importances_ for model in models_dict.values()]).mean(axis=0)\n","feature_importance_df = pd.DataFrame({'name': train_cols, 'importance': feature_importances_values})\n","\n","feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["np.mean(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 6))\n","\n","ax = sns.barplot(data=feature_importance_df.head(30), x='name', y='importance')\n","ax.set_title(f\"Mean feature importances\")\n","ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_feats['score'] = np.mean(test_predict_list, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub2 = test_feats[['id', 'score']]"]},{"cell_type":"markdown","metadata":{},"source":["# Writing Quality(fusion_notebook)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd#导入csv文件的库\n","import numpy as np#进行矩阵运算的库\n","import polars as pl#和pandas类似,但是处理大型数据集有更好的性能.\n","#用于对一组元素计数,一个存在默认值的字典,访问不存在的值时抛出的是默认值\n","from collections import Counter,defaultdict\n","import re#用于正则表达式提取\n","from scipy.stats import skew, kurtosis#统计分析和概率分布导入偏度和峰度\n","import gc#垃圾回收模块\n","\n","#model\n","from lightgbm import LGBMRegressor#导入lgbm回归器\n","from catboost import CatBoostRegressor#catboost回归器\n","from sklearn.svm import SVR#支持向量回归\n","\n","#KFold是直接分成k折,StratifiedKFold还要考虑每种类别的占比\n","from sklearn.model_selection import KFold,StratifiedKFold\n","from sklearn.preprocessing import MinMaxScaler#用最大值和最小值进行归一化操作(x-min)/(max-min)\n","from sklearn.impute import SimpleImputer#用于处理数据的缺失值\n","\n","#设置随机种子,保证模型可以复现\n","import random\n","seed=2023\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","import warnings#避免一些可以忽略的报错\n","warnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#交叉验证的折数\n","num_folds = 10\n","#需要使用归一化后特征的模型是svr(支持向量回归模型)\n","model_with_scaled_features = ['svr']\n","#融合模型的权重\n","blending_weights = {\n","    'lgbm': 0.4,\n","    'catboost': 0.4,\n","    'svr': 0.2,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\")\n","print(f\"len(train_logs):{len(train_logs)}\")\n","train_logs=train_logs.sort_values(by=['id', 'down_time'])\n","# 重置索引\n","train_logs = train_logs.reset_index(drop=True)\n","# 根据'id'列进行分组，并为每个分组添加一个递增的序列\n","train_logs['event_id'] = train_logs.groupby('id').cumcount() + 1\n","\n","train_scores=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\")\n","\n","test_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\")\n","print(f\"len(test_logs):{len(test_logs)}\")\n","test_logs=test_logs.sort_values(by=['id', 'down_time'])\n","# 重置索引\n","test_logs = test_logs.reset_index(drop=True)\n","# 根据'id'列进行分组，并为每个分组添加一个递增的序列\n","test_logs['event_id'] = test_logs.groupby('id').cumcount() + 1\n","test_logs.to_csv(\"test_logs.csv\",index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#统计‘q’ '.' ‘ ’的一个函数.\n","def getEssays(df):\n","    #获取传入的df的这几列.\n","    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']]\n","    #取出activity不等于'Nonproduction'的那些数据\n","    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n","    #统计每个id的出现次数,不排序\n","    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n","    #最后的下标\n","    lastIndex = 0\n","    #创建一个新的序列对象.\n","    essaySeries = pd.Series()\n","    #index是第几个id,valCount是出现次数\n","    for index, valCount in enumerate(valCountsArr):\n","        #取出第i个id的['activity', 'cursor_position', 'text_change']\n","        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n","        #跳到下一个id的index\n","        lastIndex += valCount\n","        essayText = \"\"\n","        for Input in currTextInput.values:\n","            #input[0]是这个id的activity\n","            if Input[0] == 'Replace':\n","                #text_change按照' => '分开 replaceTxt:[' qqq qqqqq ', ' ']\n","                replaceTxt = Input[2].split(' => ')#应该是A=>B的操作\n","                #input[1]是鼠标位置,是一个数字 鼠标位置-len()\n","                #这是一个字符串的转换操作,由replaceTxt[0]转成replaceTxt[1] \n","                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","                continue\n","            if Input[0] == 'Paste':#粘贴\n","                #print(f\"input[2]:{Input[2]}\") #input[2]:qqqqqqqqqqq \n","                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","                continue\n","            if Input[0] == 'Remove/Cut':#删除剪切 在Input[1]的位置删除Input[2]\n","                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","                continue\n","            #如果是Move from\n","            if \"M\" in Input[0]:\n","                #[284, 292] To [282, 290] 把[284, 292]这8行移动到[282,290]\n","                croppedTxt = Input[0][10:]\n","                #from和to的4个数字分开.\n","                splitTxt = croppedTxt.split(' To ')\n","                valueArr = [item.split(', ') for item in splitTxt]\n","                moveData = (int(valueArr[0][0][1:]), \n","                            int(valueArr[0][1][:-1]), \n","                            int(valueArr[1][0][1:]), \n","                            int(valueArr[1][1][:-1]))\n","                #行号不相等,如果相等,等于什么都没有做\n","                if moveData[0] != moveData[2]:\n","                    #行号小于 \n","                    if moveData[0] < moveData[2]:\n","                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n","                        essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                    #行号大于\n","                    else:\n","                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n","                        essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","                continue\n","            #相当于是个check    \n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","        #对应id对应论文\n","        essaySeries[index] = essayText\n","    #id\n","    essaySeries.index =  textInputDf['id'].unique()\n","    return pd.DataFrame(essaySeries, columns=['essay']).reset_index().rename(columns={\"index\":'id'})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#获取数据中第25%的数值\n","def q1(x):\n","    return x.quantile(0.25)\n","#获取数据中第75%的数值\n","def q3(x):\n","    return x.quantile(0.75)\n","AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', kurtosis, 'sum']\n","\n","#将论文转成单词\n","def split_essays_into_words(df):\n","    essay_df = df\n","    #对空格,\\n,句号问号感叹号进行匹配,得到一个拆分后的列表.\n","    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n","    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n","    essay_df = essay_df.explode('word')\n","    #求出每个单词的长度\n","    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n","    #去掉单词长度为0的数据\n","    essay_df = essay_df[essay_df['word_len'] != 0]\n","    return essay_df\n","\n","#计算word_len的统计学变量,并计算>=word_len的词数\n","def compute_word_aggregations(word_df):\n","    #根据id计算单词长度的统计学变量\n","    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n","    #比如('mean','word_len')->'mean_word_len'\n","    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n","    word_agg_df['id'] = word_agg_df.index\n","    for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n","        #ge 就是Latex里>=的符号,筛选出word_len>=word_l的行,根据id进行统计,提取每个计数的第0行\n","        word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n","        #如果有缺失值就填充为0\n","        word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n","    #重置索引\n","    word_agg_df = word_agg_df.reset_index(drop=True)\n","    return word_agg_df\n","\n","#将传入的论文df转成句子\n","def split_essays_into_sentences(df):\n","    essay_df = df#传入的df就是论文的df\n","    #对句子按照. ? !进行拆分. 得到一个拆分后的列表.\n","    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n","    essay_df = essay_df.explode('sent')\n","    #将换行符'\\n'变成空白字符 strip 去除行头和行尾的空白字符.\n","    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    #统计一下每个句子的长度 \n","    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n","    #求一下每个句子单词的个数.\n","    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n","    #去掉那些句子长度为0的数据\n","    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","def compute_sentence_aggregations(df):\n","    #统计句子长度的统计学变量和每个句子词数的统计学变量\n","    sent_agg_df = pd.concat(\n","        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    )\n","    #比如('mean','sent_len')->'mean_sent_len'\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","\n","    # New features intoduced here: https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline-v2\n","    for sent_l in [50, 60, 75, 100]:\n","        #ge 就是Latex里>=的符号,筛选出sent_len>=sent_l的行,根据id进行统计,提取每个计数的第0行\n","        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = df[df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n","        #如果有缺失值就填充为0\n","        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n","    #重置索引\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    #一句话里词的个数的count,其实就是有多少句话,也就是sent_len的count.重复了,故去掉.\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    #sent_len_count其实就是有多少句话,故rename.\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","#将论文根据换行符划分为段落.(每段有多少句话为什么没有统计?)\n","def split_essays_into_paragraphs(df):\n","    essay_df = df\n","    #按照'\\n'划分成段落 [1,2,3]\n","    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n","    #[论文1 [段落1 段落2,……]->[论文1 段落1 // 论文1 段落2]\n","    essay_df = essay_df.explode('paragraph')\n","    #统计段落的长度\n","    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n","    #统计每个段落的词数\n","    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    #将段落长度为0的数据去掉.\n","    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n","    return essay_df\n","\n","#对段落的长度和词数用统计学变量,和上面句子的代码一致.\n","def compute_paragraph_aggregations(df):\n","    paragraph_agg_df = pd.concat(\n","        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n","    ) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"train_essays\")\n","train_essays = pd.read_csv('/kaggle/input/writing-quality-challenge-constructed-essays/train_essays_fast.csv')\n","print(\"train_word_agg_df\")\n","train_word_agg_df = compute_word_aggregations(split_essays_into_words(train_essays))\n","print(\"train_sent_agg_df\")\n","train_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(train_essays))\n","print(\"train_paragraph_agg_df\")\n","train_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(train_essays))\n","print(\"test_essays\")\n","test_essays = getEssays(test_logs)\n","test_essays_copy=test_essays.copy()\n","print(\"test_word_agg_df\")\n","test_word_agg_df = compute_word_aggregations(split_essays_into_words(test_essays))\n","print(\"test_sent_agg_df\")\n","test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n","print(\"test_paragraph_agg_df\")\n","test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Preprocessor:#数据预处理的一个类\n","    \n","    def __init__(self):\n","        \n","        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste','Move From']#这是activity的一列\n","        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n","              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']#down_event中选出一些重要的\n","        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']#text_change中选出一些重要的\n","        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n","                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']#down_event中的一些标点符号\n","        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]#滞后项\n","        \n","        #这里是用于存储每个activity的idf值\n","        self.idf = defaultdict(float)#创建了一个float类型的字典,如果访问不存在,默认值为0.0\n","    \n","    #统计df对象中activity的count\n","    def activity_counts(self, df):\n","        #对每个id的所有activity组合成一个列表\n","        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n","        #创建一个空列表\n","        ret = list()\n","        for li in tmp_df['activity'].values:#取出一个人的activity列表\n","            items = list(Counter(li).items())#转成[(activity1:count1),(activity2:count2),……]\n","            di = dict()#一个空字典\n","            #每个activity初始化为0\n","            for k in self.activities:\n","                di[k] = 0\n","            #统计每个activity的count\n","            for item in items:\n","                k, v = item[0], item[1]#k:activity v:count\n","                if k in di:\n","                    di[k] = v\n","            #加上这个人的每个activity的count\n","            ret.append(di)\n","        #转成pandas类型\n","        ret = pd.DataFrame(ret)\n","        #给表格的每列换个名字\n","        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        #每列元素求和,文章中出现的总次数\n","        cnts = ret.sum(1)\n","\n","        #前面是词袋模型,这里转成tf-idf模型\n","        for col in cols:#activity_i_count\n","            if col in self.idf.keys():#字典里如果已经有这个key了\n","                idf = self.idf[col]\n","            else:#不在这个字典里\n","                #计算idf=log(数据量/(某列和+1))\n","                idf = np.log(df.shape[0] / (ret[col].sum() + 1))\n","                self.idf[col] = idf#将col的idf加入字典\n","            #ret[col] / cnts :给定文章的次数/在文章中出现的总次数,为什么取log再加1不知道\n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret#tf-idf\n","\n","    #这个是event的tf-idf模型,这里可能有down_event和up_event,故colname单独设置\n","    def event_counts(self, df, colname):\n","        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n","        ret = list()\n","        for li in tmp_df[colname].values:\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.events:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","\n","        return ret\n","\n","    #text_change的tf-idf模型\n","    def text_change_counts(self, df):\n","        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n","        ret = list()\n","        for li in tmp_df['text_change'].values:\n","            items = list(Counter(li).items())\n","            di = dict()\n","            for k in self.text_changes:\n","                di[k] = 0\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in di:\n","                    di[k] = v\n","            ret.append(di)\n","        ret = pd.DataFrame(ret)\n","        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n","        ret.columns = cols\n","\n","        cnts = ret.sum(1)\n","\n","        for col in cols:\n","            if col in self.idf.keys():\n","                idf = self.idf[col]\n","            else:\n","                idf = df.shape[0] / (ret[col].sum() + 1)\n","                idf = np.log(idf)\n","                self.idf[col] = idf\n","            \n","            ret[col] = 1 + np.log(ret[col] / cnts)\n","            ret[col] *= idf\n","            \n","        return ret\n","    #统计标点之类的出现的次数,不过这次是直接将它们相加做统计的.(可能这样比tf-idf好?)\n","    def match_punctuations(self, df):\n","        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n","        ret = list()\n","        for li in tmp_df['down_event'].values:\n","            cnt = 0\n","            items = list(Counter(li).items())\n","            for item in items:\n","                k, v = item[0], item[1]\n","                if k in self.punctuations:#只要在这张表里,就相加\n","                    cnt += v\n","            ret.append(cnt)\n","        ret = pd.DataFrame({'punct_cnt': ret})\n","        return ret\n","\n","\n","    def get_input_words(self, df):\n","        #~是取反的布尔值 取出text_change 中不包含 => 且不是Nochange的\n","        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n","        #在drop掉包含 => 和Nochange之后 按id打包成列表\n","        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n","        #将列表连接成一个整体\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n","        #用正则表达式子匹配一个或者多个'q'字符\n","        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n","        #统计len,也就是统计text_change中有多少个有q的字符\n","        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n","        #求均值,方差,最大值,取到np.nan就设置为0\n","        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n","        tmp_df.drop(['text_change'], axis=1, inplace=True)\n","        return tmp_df\n","    \n","    #对df做特征工程\n","    def make_feats(self, df):\n","        \n","        print(\"Starting to engineer features\")\n","        #创建一个只有id一列的表格\n","        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n","        #做时序上的特征工程\n","        print(\"Engineering time data\")\n","        for gap in self.gaps:\n","            print(f\"-> for gap {gap}\")\n","            #利用up_time的shift创造action_time_gap\n","            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n","            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n","        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        #对cursor_position做特征工程,这个就是自己-自己\n","        print(\"Engineering cursor position data\")\n","        for gap in self.gaps:\n","            print(f\"-> for gap {gap}\")\n","            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n","            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n","            #取了绝对值,鼠标向前移动也是移动了.\n","            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n","        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n","\n","        #对word_count做类似的特征工程,词数减少也是移动了.\n","        print(\"Engineering word count data\")\n","        for gap in self.gaps:\n","            print(f\"-> for gap {gap}\")\n","            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n","            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n","            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n","        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n","        \n","        print(\"Engineering statistical summaries for features\")\n","        #需要对哪些特征做哪些统计变量,这些都是大佬统计好的,就不做修改了.\n","        feats_stat = [\n","            ('event_id', ['max']),\n","            ('down_time',['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']),\n","            ('up_time',['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']),\n","            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt,'last', 'first','median']),\n","            ('activity', ['nunique']),\n","            ('down_event', ['nunique']),\n","            ('up_event', ['nunique']),\n","            ('text_change', ['nunique']),\n","            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean', 'std', 'min','last', 'first',  'median', 'sum']),\n","            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean', 'std', 'min', 'last', 'first','median', 'sum'])]\n","        #滞后特征的统计变量用for循环进行添加\n","        for gap in self.gaps:\n","            feats_stat.extend([\n","                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n","                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n","            ])\n","        \n","        pbar = feats_stat\n","        for item in pbar:\n","            colname, methods = item[0], item[1]#取出某列特征和需要进行的统计学的量'max'\n","            for method in methods:\n","                #转成能放入agg的方法\n","                if isinstance(method, str):\n","                    method_name = method\n","                else:\n","                    method_name = method.__name__\n","                #添加到feats里.\n","                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n","                feats = feats.merge(tmp_df, on='id', how='left')\n","\n","        #调用方法求activity的tf-idf\n","        print(\"Engineering activity counts data\")\n","        tmp_df = self.activity_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        #调用方法求down_event和up_event的tf-idf\n","        print(\"Engineering event counts data\")\n","        tmp_df = self.event_counts(df, 'down_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        tmp_df = self.event_counts(df, 'up_event')\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering text change counts data\")\n","        tmp_df = self.text_change_counts(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","        \n","        print(\"Engineering punctuation counts data\")\n","        tmp_df = self.match_punctuations(df)\n","        feats = pd.concat([feats, tmp_df], axis=1)\n","\n","        # input words\n","        print(\"Engineering input words data\")\n","        tmp_df = self.get_input_words(df)\n","        feats = pd.merge(feats, tmp_df, on='id', how='left')\n","\n","        # compare feats\n","        print(\"Engineering ratios data\")\n","        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n","        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n","        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n","        #休息时间的占比\n","        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n","        \n","        print(\"Done!\")\n","        return feats\n","\n","preprocessor = Preprocessor()\n","print(\"Engineering features for training data\")\n","\n","train_feats = preprocessor.make_feats(train_logs)\n","print(\"-\"*25)\n","print(\"Engineering features for test data\")\n","test_feats = preprocessor.make_feats(test_logs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = []\n","\n","for logs in [train_logs, test_logs]:\n","    #up_time向后移动并且用down_time填充缺失的位置\n","    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n","    #(down_time减上一个时刻的up_time) /1000是单位转换\n","    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n","\n","    #按照id打包time_diff\n","    group = logs.groupby('id')['time_diff']\n","    #延迟时间的max,min,median\n","    largest_lantency = group.max()\n","    smallest_lantency = group.min()\n","    median_lantency = group.median()\n","    #down_time的first /1000是做单位转换吧\n","    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n","    #分层次求和\n","    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x <= 1)).sum())\n","    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x <= 1.5)).sum())\n","    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x <= 2)).sum())\n","    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x <= 3)).sum())\n","    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n","\n","    data.append(pd.DataFrame({\n","        'id': logs['id'].unique(),\n","         #延迟\n","        'largest_lantency': largest_lantency,\n","        'smallest_lantency': smallest_lantency,\n","        'median_lantency': median_lantency,\n","        'initial_pause': initial_pause,\n","        'pauses_half_sec': pauses_half_sec,\n","        'pauses_1_sec': pauses_1_sec,\n","        'pauses_1_half_sec': pauses_1_half_sec,\n","        'pauses_2_sec': pauses_2_sec,\n","        'pauses_3_sec': pauses_3_sec,\n","    }).reset_index(drop=True))\n","\n","train_eD592674, test_eD592674 = data\n","\n","gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n","\n","train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n","test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n","train_feats = train_feats.merge(train_scores, on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#将论文的特征加上\n","train_feats=train_feats.merge(train_word_agg_df,on='id', how='left')\n","train_feats=train_feats.merge(train_sent_agg_df,on='id', how='left')\n","train_feats=train_feats.merge(train_paragraph_agg_df,on='id', how='left')\n","\n","#将论文的特征加上\n","test_feats=test_feats.merge(test_word_agg_df,on='id', how='left')\n","test_feats=test_feats.merge(test_sent_agg_df,on='id', how='left')\n","test_feats=test_feats.merge(test_paragraph_agg_df,on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#找到只有唯一值的列,删掉\n","keys=train_feats.keys().values\n","unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n","print(f\"unique_cols:{unique_cols}\")\n","train_feats = train_feats.drop(columns=unique_cols)\n","test_feats = test_feats.drop(columns=unique_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#这里创建了lgbm,cat,SVR模型\n","def make_model():\n","    \n","    #大佬找好的参数,这里不做改动\n","    params = {'reg_alpha': 0.007678095440286993, \n","               'reg_lambda': 0.34230534302168353, \n","               'colsample_bytree': 0.627061253588415, \n","               'subsample': 0.854942238828458, \n","               'learning_rate': 0.038697981947473245, \n","               'num_leaves': 22, \n","               'max_depth': 37, \n","               'min_child_samples': 18,\n","               'random_state': seed,\n","               'n_estimators': 150,\n","               \"objective\": \"regression\",\n","               \"metric\": \"rmse\",\n","               'force_col_wise': True,\n","               \"verbosity\": 0,\n","              }\n","    \n","    model1 = LGBMRegressor(**params)\n","    \n","    model2 = CatBoostRegressor(iterations=1000,\n","                                 learning_rate=0.1,\n","                                 depth=6,\n","                                 eval_metric='RMSE',\n","                                 random_seed = seed,\n","                                 bagging_temperature = 0.2,\n","                                 od_type='Iter',\n","                                 metric_period = 50,\n","                                 od_wait=20,\n","                                 verbose=False)\n","    \n","    model3 = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n","    \n","    models = []\n","    models.append((model1, 'lgbm'))\n","    models.append((model2, 'catboost'))\n","    models.append((model3, 'svr'))\n","    \n","    return models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_features =train_feats.drop(['score'],axis=1).keys().values \n","X_y = pd.merge(train_feats[best_features], train_scores, on='id', how='left')\n","\n","#将里面正无穷和负无穷的值替换成缺失值\n","X_y.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","features = X_y.iloc[:,1:-1]#特征是除了score和label的其他列\n","target = X_y.iloc[:,-1]#target是score\n","\n","#评估指标是RMSE\n","def RMSE(y_true,y_pred):\n","    return np.sqrt(np.mean((y_true-y_pred)**2))\n","\n","models_and_errors_dict = {}\n","\n","for model, model_type in make_model():\n","    \n","    oof_pred=np.zeros((len(features)))\n","        \n","    #10折交叉验证\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed + num_folds)\n","\n","    for fold, indexes in enumerate(kf.split(features), start=1):\n","\n","        # Get train and test indexes\n","        train_index, test_index = indexes\n","\n","        print(f'--- Fold #{fold} ---')       \n","\n","        # Split data into train and test sets\n","        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n","        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n","\n","        #因为有的模型需要归一化,不能动到原始数据\n","        X_train_copy, X_test_copy = X_train.copy(), X_test.copy()\n","\n","        print(f'Training a {model_type} model on fold {fold}')\n","\n","        #如果是需要归一化的模型\n","        if model_type in model_with_scaled_features:\n","            #用平均值来填充缺失值\n","            imputer = SimpleImputer(strategy='mean')\n","            X_train_imputed = imputer.fit_transform(X_train.copy())\n","            X_test_imputed = imputer.transform(X_test.copy())\n","            #然后用最大最小值来归一化,归一化的范围为[-1,1]\n","            scaler = MinMaxScaler(feature_range=(-1, 1))\n","            X_train_scaled = scaler.fit_transform(X_train_imputed)\n","            X_test_scaled = scaler.transform(X_test_imputed)\n","            #使用归一化好的数据\n","            X_train_copy = X_train_scaled\n","            X_test_copy = X_test_scaled\n","        #模型的训练\n","        if model_type == 'lgb':\n","            #早停200次,只考虑第一个度量指标,不显示训练日志\n","            early_stopping_callback = LGBMRegressor.early_stopping(200, first_metric_only=True, verbose=False)\n","            verbose_callback = LGBMRegressor.log_evaluation(100)#每隔100次打印一次训练日志\n","\n","            model.fit(X_train_copy, y_train, eval_set=[(X_test_copy, y_test)],  \n","                      callbacks=[early_stopping_callback, verbose_callback],)\n","        else:\n","            model.fit(X_train_copy, y_train)\n","\n","        #做预测\n","        y_hat = model.predict(X_test_copy)\n","        \n","        oof_pred[test_index]=y_hat\n","        #计算RMSE\n","        rmse = RMSE(y_test, y_hat)\n","        print(f'RMSE: {rmse} on fold {fold}')\n","\n","        #在字典里没有就创建一个\n","        if model_type not in models_and_errors_dict:\n","            models_and_errors_dict[model_type] = []\n","        #如果是需要归一化的模型,除了保存模型和损失,还需要将填充缺失值和归一化的模型保存\n","        if model_type in model_with_scaled_features:\n","            models_and_errors_dict[model_type].append((model, rmse, imputer, scaler,oof_pred))\n","        else:\n","            models_and_errors_dict[model_type].append((model, rmse, None, None,oof_pred))  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_oof_pred=models_and_errors_dict['lgbm'][9][4]\n","cat_oof_pred=models_and_errors_dict['catboost'][9][4]\n","svr_oof_pred=models_and_errors_dict['svr'][9][4]\n","margin=1000\n","target=target.values\n","current_RMSE=RMSE(target,(lgb_oof_pred+cat_oof_pred+svr_oof_pred)/3)\n","best_i=0\n","best_j=0\n","for i in range(0,margin):\n","    for j in range(0,margin-i):\n","        #k=1000-i-j\n","        blend_oof_pred=(i*lgb_oof_pred+j*cat_oof_pred+(margin-i-j)*svr_oof_pred)/margin\n","        if RMSE(target,blend_oof_pred)<current_RMSE:\n","            print(f\"current_RMSE:{current_RMSE}\")\n","            current_RMSE=RMSE(target,blend_oof_pred)\n","            best_i=i\n","            best_j=j\n","#找到最好的参数之后\n","blending_weights['lgbm']=best_i/margin\n","blending_weights['catboost']=best_j/margin\n","blending_weights['svr']=(margin-best_i-best_j)/margin\n","print(f\"blending_weights:{blending_weights}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_hats = dict()\n","\n","#设置submission_df,id和score\n","submission_df = pd.DataFrame(test_feats['id'])\n","submission_df['score'] = 3.5#如果报错,将预测结果设置为3.5\n","\n","#取出test_feats中所有列\n","X_unseen = test_feats.copy()[best_features]\n","#去掉id这一列.\n","X_unseen.drop(columns=['id'], inplace=True)\n","#测试集中把np.inf和-np.inf代替为np.nan\n","X_unseen.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","for model_name, model_info in models_and_errors_dict.items():\n","    print(f'\\n--- {model_name} ---\\n')\n","    \n","    #复制是因为有的要归一化\n","    X_unseen_copy = X_unseen.copy()\n","    y_hats[model_name] = []#某个model的预测结果\n","\n","    for ix, (trained_model, error, imputer, scaler,oof_pred) in enumerate(model_info, start=1):\n","        print(f\"Using model {ix} with error {error}\")\n","\n","        #如果需要归一化的话,也就是支持向量回归模型\n","        if model_name in model_with_scaled_features:\n","            #先填充缺失值,再用归一化\n","            X_unseen_imputed = imputer.transform(X_unseen_copy)\n","            X_unseen_scaled = scaler.transform(X_unseen_imputed)\n","            #得到预测结果\n","            y_hats[model_name].append(trained_model.predict(X_unseen_scaled))\n","        else:#如果是树模型直接得到预测结果\n","            y_hats[model_name].append(trained_model.predict(X_unseen_copy))\n","    #如果有值的话,求平均,赋值给submission_df\n","    if y_hats[model_name]:\n","        y_hat_avg = np.mean(y_hats[model_name], axis=0)\n","        submission_df['score_' + model_name] = y_hat_avg\n","    print(\"Done.\")\n","print(\"blending\")\n","blended_score=np.zeros((len(test_essays_copy)))\n","for k, v in blending_weights.items():\n","    blended_score += submission_df['score_' + k] * v\n","print(f\"blended_score:{blended_score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#数值型变量的几列\n","num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n","activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n","\n","#df表格的colname列统计values的count.\n","def count_by_values(df, colname, values):\n","    #maintain_order=True保持原有顺序\n","    fts = df.select(pl.col('id').unique(maintain_order=True))\n","    for i, value in enumerate(values):\n","        #根据每个id判断colname是不是value并统计个数,rename成colname_i_cnt\n","        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n","        #加上这个特征\n","        fts  = fts.join(tmp_df, on='id', how='left') \n","    return fts\n","\n","def dev_feats(df):\n","    \n","    print(\"< Count by values features >\")\n","    \n","    #统计activity,text_change,down_event,up_event这几个类别型变量的count\n","    feats = count_by_values(df, 'activity', activities)\n","    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n","    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n","    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n","\n","    print(\"< Input words stats features >\")\n","    #不含有'=>'且有变化的行\n","    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n","    #按照id将text_change连接成一个长字符串,然后匹配'q+'的字符串\n","    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n","    #统计输入的词数,词长度的均值,最大值,方差,中位数,偏斜度.\n","    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n","                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n","                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n","    #将text_change这列去掉,加入特征.\n","    temp = temp.drop('text_change')\n","    feats = feats.join(temp, on='id', how='left') \n","\n","    print(\"< Numerical columns features >\")\n","\n","    #对action_time求和,对数值型变量求均值,方差,中位数,最小值,最大值,50%的数字\n","    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n","                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n","                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n","    feats = feats.join(temp, on='id', how='left') \n","\n","\n","    print(\"< Categorical columns features >\")\n","    #类别型变量求了n_unique,加入特征.\n","    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n","    feats = feats.join(temp, on='id', how='left') \n","\n","    print(\"< Idle time features >\")\n","    #这里就是论文中的特征.(https://files.eric.ed.gov/fulltext/ED592674.pdf)\n","    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n","    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n","                                   inter_key_median_lantency = pl.median('time_diff'),\n","                                   mean_pause_time = pl.mean('time_diff'),\n","                                   std_pause_time = pl.std('time_diff'),\n","                                   total_pause_time = pl.sum('time_diff'),\n","                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n","                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n","                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n","                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n","                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n","    feats = feats.join(temp, on='id', how='left') \n","    \n","    print(\"< P-bursts features >\")\n","    #找到df中activity为‘Input’和‘Remove/cut’的行,并且是time_diff<2的行\n","    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n","    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.with_columns(pl.col('time_diff')<2)\n","    #然后统计连续出现的个数的(统计学变量)\n","    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n","    temp = temp.drop_nulls()#删除包含缺失值的行\n","    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n","                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n","                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n","    feats = feats.join(temp, on='id', how='left') \n","\n","    print(\"< R-bursts features >\")\n","    #取出数据中为'Remove/cut'\n","    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n","    #统计'Remove/cut'连续出现的次数(的统计学变量)\n","    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n","    temp = temp.drop_nulls()#删除包含缺失值的行\n","    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n","                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n","                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n","    feats = feats.join(temp, on='id', how='left')\n","    \n","    return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n","\n","def word_feats(df):\n","    essay_df = df\n","    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('word')\n","    df['word_len'] = df['word'].apply(lambda x: len(x))\n","    df = df[df['word_len'] != 0]\n","\n","    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n","    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n","    word_agg_df['id'] = word_agg_df.index\n","    word_agg_df = word_agg_df.reset_index(drop=True)\n","    return word_agg_df\n","\n","\n","def sent_feats(df):\n","    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('sent')\n","    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.sent_len!=0].reset_index(drop=True)\n","\n","    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n","                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","def parag_feats(df):\n","    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n","    df = df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.paragraph_len!=0].reset_index(drop=True)\n","    \n","    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n","                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df\n","\n","#论文的长度除以(‘Input’和‘Remove/Cut’)的按键个数.\n","def product_to_keys(logs, essays):\n","    essays['product_len'] = essays.essay.str.len()#论文的长度\n","    #logs中每个id ‘Input’和‘Remove/Cut’的数据\n","    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n","    essays = essays.merge(tmp_df, on='id', how='left')\n","    #论文的长度除以(‘Input’和‘Remove/Cut’)的按键个数.\n","    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n","    return essays[['id', 'product_to_keys']]\n","#统计每秒有几个['Input', 'Remove/Cut']的行为.\n","def get_keys_pressed_per_second(logs):\n","    #logs中为['Input', 'Remove/Cut']的event_id的个数\n","    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n","    #每个id最小的down_time和最大的up_time\n","    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n","    #按照id融合在一起\n","    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n","    #每秒有几个event_id\n","    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n","    return temp_df[['id', 'keys_per_second']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#传入训练数据dataX和datay,model,测试数据test_X\n","#就是一个简单的k折交叉验证,不过模型只有1个训练5次.训练完就得到测试集的预测结果.\n","def evaluate(data_x, data_y, model, random_state=seed, n_splits=5, test_x=None):\n","    #StratifiedKFold还要考虑每种类别的占比\n","    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n","    test_y = np.zeros((len(test_x), n_splits))#5折的预测结果\n","    for i, (train_idx, valid_idx) in enumerate(skf.split(data_x, data_y.astype(str))):\n","        train_x = data_x.iloc[train_idx]\n","        train_y = data_y[train_idx]\n","        valid_x = data_x.iloc[valid_idx]\n","        valid_y = data_y[valid_idx]\n","        model.fit(train_x, train_y)\n","        test_y[:, i] = model.predict(test_x)\n","    return np.mean(test_y, axis=1)\n","\n","\n","train_logs    = pl.scan_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n","train_feats   = dev_feats(train_logs)\n","train_feats   = train_feats.collect().to_pandas()\n","\n","print('< Essay Reconstruction >')\n","train_logs             = train_logs.collect().to_pandas()\n","train_essays           = pd.read_csv('/kaggle/input/writing-quality-challenge-constructed-essays/train_essays_fast.csv')\n","train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n","train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n","train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n","train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n","train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n","\n","#找到只有唯一值的列,删掉\n","keys=train_feats.keys().values\n","unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n","print(f\"unique_cols:{unique_cols}\")\n","train_feats = train_feats.drop(columns=unique_cols)\n","\n","print('< Mapping >')\n","train_scores   = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n","data           = train_feats.merge(train_scores, on='id', how='left')\n","x              = data.drop(['id', 'score'], axis=1)\n","y              = data['score'].values\n","\n","print(f'Number of features: {len(x.columns)}')\n","\n","print('< Testing Data >')\n","test_logs   = pl.scan_csv('/kaggle/working/test_logs.csv')\n","test_feats  = dev_feats(test_logs)\n","test_feats  = test_feats.collect().to_pandas()\n","\n","test_logs             = test_logs.collect().to_pandas()\n","test_essays           = test_essays_copy\n","test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n","test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n","test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n","test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n","test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n","\n","test_feats = test_feats.drop(columns=unique_cols)\n","\n","test_ids = test_feats['id'].values\n","testin_x = test_feats.drop(['id'], axis=1)\n","\n","print('< Learning and Evaluation >')\n","lgbm_params = {'n_estimators': 1024,\n","         'learning_rate': 0.006,\n","         'metric': 'rmse',\n","         'random_state': seed,\n","         'force_col_wise': True,\n","         'verbosity': 0,}\n","solution = LGBMRegressor(**lgbm_params)\n","y_pred_lgb   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n","y_pred_lgb"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_pred = blended_score*0.4+ y_pred_lgb*0.6#将两种预测结果进行一个加权融合\n","\n","sub3 = pd.DataFrame({'id': test_ids, 'score': y_pred})\n","sub3.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clean_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub1.rename(columns={'score': 'score_1'}, inplace=True)\n","sub2.rename(columns={'score': 'score_2'}, inplace=True)\n","sub3.rename(columns={'score': 'score_3'}, inplace=True)\n","\n","submission = pd.merge(sub1, sub2, on='id')\n","submission = pd.merge(submission, sub3, on='id')\n","\n","submission['score'] = (submission['score_1']*0.2 +  #LGBM + NN (Weighted search for \"print(W)\")\n","                       submission['score_2']*0.3 +  #LGBM Public\n","                       submission['score_3']*0.5)   #Fusion\n","\n","submission_final = submission[['id', 'score']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_final.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_final"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"},{"datasetId":3992884,"sourceId":6971449,"sourceType":"datasetVersion"},{"datasetId":3949123,"sourceId":6973319,"sourceType":"datasetVersion"},{"sourceId":150384981,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
