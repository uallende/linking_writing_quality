{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m4_feats_functions import *\n",
    "from m7_utils import *\n",
    "from m5_models import *\n",
    "from m3_model_params import lgb_params_2 as lgbm_params\n",
    "from m3_model_params import xgb_params_2 as xgb_params\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, StandardScaler\n",
    "from m3_model_params import non_important_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\n",
    "FEAT_STORE_DIR = 'feat_store_combined'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = pd.read_pickle('feature_store/base_feats/train_base_feats_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:25<00:00,  2.60s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13702.42it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12859.51it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 13860.51it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13626.27it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13305.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:358: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:360: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 24.44it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 40329.85it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 48026.38it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 41255.45it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 36472.21it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 36792.14it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:358: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:360: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:04<00:00, 585.71it/s]\n",
      "100%|██████████| 2474/2474 [00:04<00:00, 590.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2474, 27) (2474,)\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)\n",
    "logs = normalise_up_down_times(logs)\n",
    "\n",
    "train_logs = normalise_up_down_times(train_logs)\n",
    "test_logs = normalise_up_down_times(test_logs)\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "\n",
    "essays = getEssays(logs)\n",
    "sent_feats = compute_sentence_aggregations(essays)\n",
    "par_feats = compute_paragraph_aggregations(essays)\n",
    "word_feats = create_word_length_features(essays, 'essay', 'id', 'essay_words')\n",
    "\n",
    "train_sent = sent_feats[sent_feats['id'].isin(train_ids)]\n",
    "train_par = par_feats[par_feats['id'].isin(train_ids)]\n",
    "train_words = word_feats[word_feats['id'].isin(train_ids)]\n",
    "test_sent = sent_feats[sent_feats['id'].isin(test_ids)]\n",
    "test_par = par_feats[par_feats['id'].isin(test_ids)]\n",
    "test_words = word_feats[word_feats['id'].isin(test_ids)]\n",
    "\n",
    "train_vector, test_vector = countvectorize_one_one(train_logs, test_logs, train_feats, test_feats)\n",
    "\n",
    "train_feats = train_feats.merge(train_sent, how='left', on='id')\n",
    "train_feats = train_feats.merge(train_par, how='left', on='id')\n",
    "train_feats = train_feats.merge(train_words, how='left', on='id')\n",
    "train_feats = train_feats.merge(train_vector, how='left', on='id')\n",
    "\n",
    "test_feats = test_feats.merge(test_sent, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_par, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_words, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_vector, on='id', how='left')\n",
    "\n",
    "train_feats.to_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats.to_pickle('feat_store_hybrid/test_super.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/numpy/core/_methods.py:239: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 450\n",
      "Ridge Average RMSE over 50 folds: 0.621465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 0/5 [00:00<?, ?it/s]/root/miniconda3/envs/lrp/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:00:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Iterations: 100%|██████████| 5/5 [01:44<00:00, 20.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Average RMSE over 50 folds: 0.600903\n",
      "LGBM Average RMSE over 50 folds: 0.601095\n",
      "Mean RMSE of all iterations: 0.612733\n",
      "Blend RMSE 0.597671\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "train_feats = pd.read_pickle('feature_store/base_feats/train_base_feats_2.pkl')\n",
    "test_feats = pd.read_pickle('feature_store/base_feats/test_base_feats_2.pkl')\n",
    "\n",
    "\n",
    "# train_feats = train_feats.drop(columns=non_important_feats)\n",
    "# test_feats = test_feats.drop(columns=test_feats.columns.intersection(non_important_feats))\n",
    "train_feats = preprocess_feats(train_feats, PowerTransformer('yeo-johnson'))\n",
    "test_feats = preprocess_feats(test_feats, PowerTransformer('yeo-johnson'))\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "n_repeats = 5\n",
    "n_splits = 10\n",
    "alpha = 450\n",
    "\n",
    "ridge_params = {'alpha': alpha}  # Create a dictionary with alpha\n",
    "print(f'Alpha {alpha}')\n",
    "_, _, ridge_oof_preds, _ = ridge_cv_pipeline(train_feats, test_feats, ridge_params, seed=42, n_repeats=n_repeats, n_splits=n_splits)\n",
    "\n",
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "train_feats = pd.read_pickle('feature_store/base_feats/train_base_feats_2.pkl')\n",
    "test_feats = pd.read_pickle('feature_store/base_feats/test_base_feats_2.pkl')\n",
    "\n",
    "#('action_time_gap', 'count_vectorized', 'action_time_gap_by_acti', 'wc_chage')\n",
    "# action_gap = pd.read_pickle('feature_store/train/train_action_time_gap.pkl')\n",
    "# action_gap_act = pd.read_pickle('feature_store/train/train_action_time_gap_by_acti.pkl')\n",
    "# wc_change = pd.read_pickle('feature_store/train/train_wc_chage.pkl')\n",
    "# \n",
    "# ts_action_gap = pd.read_pickle('feature_store/test/test_action_time_gap.pkl')\n",
    "# ts_action_gap_act = pd.read_pickle('feature_store/test/test_action_time_gap_by_acti.pkl')\n",
    "# ts_wc_change = pd.read_pickle('feature_store/test/test_wc_chage.pkl')\n",
    "\n",
    "# train_feats = train_feats.merge(action_gap, on='id', how='left')\n",
    "# train_feats = train_feats.merge(action_gap_act, on='id', how='left')\n",
    "# train_feats = train_feats.merge(wc_change, on='id', how='left')\n",
    "# \n",
    "# test_feats = test_feats.merge(ts_action_gap, on='id', how='left')\n",
    "# test_feats = test_feats.merge(ts_action_gap_act, on='id', how='left')\n",
    "# test_feats = test_feats.merge(ts_wc_change, on='id', how='left')\n",
    "\n",
    "tr_vector = pd.read_pickle('feature_store/train/train_count_vectorized.pkl')\n",
    "ts_vector = pd.read_pickle('feature_store/test/test_count_vectorized.pkl')\n",
    "train_feats = train_feats.merge(tr_vector, on='id', how='left')\n",
    "test_feats = test_feats.merge(ts_vector, on='id', how='left')\n",
    "\n",
    "\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "_, oof_1, rmse, model1 = xgb_cv_pipeline(train_feats=train_feats, \n",
    "                                        test_feats=test_feats, \n",
    "                                        xgb_params=xgb_params, \n",
    "                                        seed=42, \n",
    "                                        n_repeats=n_repeats, \n",
    "                                        n_splits=n_splits)\n",
    "                                        \n",
    "_, oof_2, rmse, model1 = cv_pipeline(train_feats, \n",
    "                                     test_feats, \n",
    "                                     lgbm_params, \n",
    "                                     lgbm_params['boosting_type'],\n",
    "                                     seed = 42,\n",
    "                                     n_repeats= n_repeats,\n",
    "                                     n_splits = n_splits)\n",
    "\n",
    "blend = pd.concat([oof_1, oof_2, ridge_oof_preds], axis=0)\n",
    "blend_scores = blend.groupby(['id','score'])['prediction'].mean().reset_index()\n",
    "blend_rmse = mean_squared_error(blend_scores['score'], blend_scores['prediction'], squared=False)\n",
    "print(f'Blend RMSE {blend_rmse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 0/5 [00:00<?, ?it/s]/root/miniconda3/envs/lrp/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:59:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Iterations: 100%|██████████| 5/5 [00:54<00:00, 10.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Average RMSE over 30 folds: 0.605156\n",
      "LGBM Average RMSE over 30 folds: 0.610728\n",
      "Blend RMSE 0.605737\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "train_feats = pd.read_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats = pd.read_pickle('feat_store_hybrid/test_feats.pkl')\n",
    "\n",
    "train_feats = train_feats.drop(columns=non_important_feats)\n",
    "test_feats = test_feats.drop(columns=test_feats.columns.intersection(non_important_feats))\n",
    "\n",
    "# train_feats = preprocess_feats(train_feats, PowerTransformer('yeo-johnson'))\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "n_repeats=5\n",
    "n_splits=6\n",
    "\n",
    "_, oof_1, rmse, model1 = xgb_cv_pipeline(train_feats=train_feats, \n",
    "                                        test_feats=test_feats, \n",
    "                                        xgb_params=xgb_params, \n",
    "                                        seed=42, \n",
    "                                        n_repeats=n_repeats, \n",
    "                                        n_splits=n_splits)\n",
    "                                        \n",
    "_, oof_2, rmse, model1 = cv_pipeline(train_feats, \n",
    "                                     test_feats, \n",
    "                                     lgbm_params, \n",
    "                                     lgbm_params['boosting_type'],\n",
    "                                     seed = 42,\n",
    "                                     n_repeats= n_repeats,\n",
    "                                     n_splits = n_splits)\n",
    "\n",
    "blend = pd.concat([oof_1, oof_2], axis=0)\n",
    "blend_scores = blend.groupby(['id','score'])['prediction'].mean().reset_index()\n",
    "blend_rmse = mean_squared_error(blend_scores['score'], blend_scores['prediction'], squared=False)\n",
    "print(f'Blend RMSE {blend_rmse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Average RMSE over 50 folds: 0.608614\n",
      "Number of models: 50\n",
      "Train columns: ['event_id_max', 'up_time_max', 'action_time_max', 'action_time_min', 'action_time_mean', 'action_time_std', 'action_time_quantile', 'action_time_sem', 'action_time_sum', 'action_time_skew', 'action_time_kurt', 'activity_nunique', 'down_event_nunique', 'up_event_nunique', 'text_change_nunique', 'cursor_position_nunique', 'cursor_position_max', 'cursor_position_quantile', 'cursor_position_sem', 'cursor_position_mean', 'word_count_nunique', 'word_count_max', 'word_count_quantile', 'word_count_sem', 'word_count_mean', 'action_time_gap1_max', 'action_time_gap1_min', 'action_time_gap1_mean', 'action_time_gap1_std', 'action_time_gap1_quantile', 'action_time_gap1_sem', 'action_time_gap1_sum', 'action_time_gap1_skew', 'action_time_gap1_kurt', 'cursor_position_change1_max', 'cursor_position_change1_mean', 'cursor_position_change1_std', 'cursor_position_change1_quantile', 'cursor_position_change1_sem', 'cursor_position_change1_sum', 'cursor_position_change1_skew', 'cursor_position_change1_kurt', 'word_count_change1_max', 'word_count_change1_mean', 'word_count_change1_std', 'word_count_change1_quantile', 'word_count_change1_sem', 'word_count_change1_sum', 'word_count_change1_skew', 'word_count_change1_kurt', 'action_time_gap2_max', 'action_time_gap2_min', 'action_time_gap2_mean', 'action_time_gap2_std', 'action_time_gap2_quantile', 'action_time_gap2_sem', 'action_time_gap2_sum', 'action_time_gap2_skew', 'action_time_gap2_kurt', 'cursor_position_change2_max', 'cursor_position_change2_mean', 'cursor_position_change2_std', 'cursor_position_change2_quantile', 'cursor_position_change2_sem', 'cursor_position_change2_sum', 'cursor_position_change2_skew', 'cursor_position_change2_kurt', 'word_count_change2_max', 'word_count_change2_mean', 'word_count_change2_std', 'word_count_change2_quantile', 'word_count_change2_sem', 'word_count_change2_sum', 'word_count_change2_skew', 'word_count_change2_kurt', 'action_time_gap3_max', 'action_time_gap3_min', 'action_time_gap3_mean', 'action_time_gap3_std', 'action_time_gap3_quantile', 'action_time_gap3_sem', 'action_time_gap3_sum', 'action_time_gap3_skew', 'action_time_gap3_kurt', 'cursor_position_change3_max', 'cursor_position_change3_mean', 'cursor_position_change3_std', 'cursor_position_change3_quantile', 'cursor_position_change3_sem', 'cursor_position_change3_sum', 'cursor_position_change3_skew', 'cursor_position_change3_kurt', 'word_count_change3_max', 'word_count_change3_mean', 'word_count_change3_std', 'word_count_change3_quantile', 'word_count_change3_sem', 'word_count_change3_sum', 'word_count_change3_skew', 'word_count_change3_kurt', 'action_time_gap5_max', 'action_time_gap5_min', 'action_time_gap5_mean', 'action_time_gap5_std', 'action_time_gap5_quantile', 'action_time_gap5_sem', 'action_time_gap5_sum', 'action_time_gap5_skew', 'action_time_gap5_kurt', 'cursor_position_change5_max', 'cursor_position_change5_mean', 'cursor_position_change5_std', 'cursor_position_change5_quantile', 'cursor_position_change5_sem', 'cursor_position_change5_sum', 'cursor_position_change5_skew', 'cursor_position_change5_kurt', 'word_count_change5_max', 'word_count_change5_mean', 'word_count_change5_std', 'word_count_change5_quantile', 'word_count_change5_sem', 'word_count_change5_sum', 'word_count_change5_skew', 'word_count_change5_kurt', 'action_time_gap10_max', 'action_time_gap10_min', 'action_time_gap10_mean', 'action_time_gap10_std', 'action_time_gap10_quantile', 'action_time_gap10_sem', 'action_time_gap10_sum', 'action_time_gap10_skew', 'action_time_gap10_kurt', 'cursor_position_change10_max', 'cursor_position_change10_mean', 'cursor_position_change10_std', 'cursor_position_change10_quantile', 'cursor_position_change10_sem', 'cursor_position_change10_sum', 'cursor_position_change10_skew', 'cursor_position_change10_kurt', 'word_count_change10_max', 'word_count_change10_mean', 'word_count_change10_std', 'word_count_change10_quantile', 'word_count_change10_sem', 'word_count_change10_sum', 'word_count_change10_skew', 'word_count_change10_kurt', 'action_time_gap20_max', 'action_time_gap20_min', 'action_time_gap20_mean', 'action_time_gap20_std', 'action_time_gap20_quantile', 'action_time_gap20_sem', 'action_time_gap20_sum', 'action_time_gap20_skew', 'action_time_gap20_kurt', 'cursor_position_change20_max', 'cursor_position_change20_mean', 'cursor_position_change20_std', 'cursor_position_change20_quantile', 'cursor_position_change20_sem', 'cursor_position_change20_sum', 'cursor_position_change20_skew', 'cursor_position_change20_kurt', 'word_count_change20_max', 'word_count_change20_mean', 'word_count_change20_std', 'word_count_change20_quantile', 'word_count_change20_sem', 'word_count_change20_sum', 'word_count_change20_skew', 'word_count_change20_kurt', 'action_time_gap50_max', 'action_time_gap50_min', 'action_time_gap50_mean', 'action_time_gap50_std', 'action_time_gap50_quantile', 'action_time_gap50_sem', 'action_time_gap50_sum', 'action_time_gap50_skew', 'action_time_gap50_kurt', 'cursor_position_change50_max', 'cursor_position_change50_mean', 'cursor_position_change50_std', 'cursor_position_change50_quantile', 'cursor_position_change50_sem', 'cursor_position_change50_sum', 'cursor_position_change50_skew', 'cursor_position_change50_kurt', 'word_count_change50_max', 'word_count_change50_mean', 'word_count_change50_std', 'word_count_change50_quantile', 'word_count_change50_sem', 'word_count_change50_sum', 'word_count_change50_skew', 'word_count_change50_kurt', 'action_time_gap100_max', 'action_time_gap100_min', 'action_time_gap100_mean', 'action_time_gap100_std', 'action_time_gap100_quantile', 'action_time_gap100_sem', 'action_time_gap100_sum', 'action_time_gap100_skew', 'action_time_gap100_kurt', 'cursor_position_change100_max', 'cursor_position_change100_mean', 'cursor_position_change100_std', 'cursor_position_change100_quantile', 'cursor_position_change100_sem', 'cursor_position_change100_sum', 'cursor_position_change100_skew', 'cursor_position_change100_kurt', 'word_count_change100_max', 'word_count_change100_mean', 'word_count_change100_std', 'word_count_change100_quantile', 'word_count_change100_sem', 'word_count_change100_sum', 'word_count_change100_skew', 'word_count_change100_kurt', 'activity_0_count', 'activity_1_count', 'activity_2_count', 'activity_3_count', 'activity_4_count', 'down_event_0_count', 'down_event_1_count', 'down_event_2_count', 'down_event_3_count', 'down_event_4_count', 'down_event_5_count', 'down_event_6_count', 'down_event_7_count', 'down_event_8_count', 'down_event_9_count', 'down_event_10_count', 'down_event_11_count', 'down_event_12_count', 'down_event_13_count', 'down_event_14_count', 'down_event_15_count', 'up_event_0_count', 'up_event_1_count', 'up_event_2_count', 'up_event_3_count', 'up_event_4_count', 'up_event_5_count', 'up_event_6_count', 'up_event_7_count', 'up_event_8_count', 'up_event_9_count', 'up_event_10_count', 'up_event_11_count', 'up_event_12_count', 'up_event_13_count', 'up_event_14_count', 'up_event_15_count', 'text_change_0_count', 'text_change_1_count', 'text_change_2_count', 'text_change_3_count', 'text_change_4_count', 'text_change_5_count', 'text_change_6_count', 'text_change_7_count', 'text_change_8_count', 'text_change_9_count', 'text_change_10_count', 'text_change_11_count', 'text_change_12_count', 'text_change_13_count', 'text_change_14_count', 'punct_cnt', 'input_word_count', 'input_word_length_mean', 'input_word_length_max', 'input_word_length_std', 'word_time_ratio', 'word_event_ratio', 'event_time_ratio', 'idle_time_ratio', 'sent_count', 'sent_len_mean', 'sent_len_std', 'sent_len_min', 'sent_len_max', 'sent_len_first', 'sent_len_last', 'sent_len_sem', 'sent_len_q1', 'sent_len_median', 'sent_len_q3', 'sent_len_skew', 'sent_len_kurt', 'sent_len_sum', 'sent_word_count_mean', 'sent_word_count_std', 'sent_word_count_min', 'sent_word_count_max', 'sent_word_count_first', 'sent_word_count_last', 'sent_word_count_sem', 'sent_word_count_q1', 'sent_word_count_median', 'sent_word_count_q3', 'sent_word_count_skew', 'sent_word_count_kurt', 'sent_word_count_sum', 'low_sent_count', 'paragraph_count', 'paragraph_len_mean', 'paragraph_len_std', 'paragraph_len_min', 'paragraph_len_max', 'paragraph_len_first', 'paragraph_len_last', 'paragraph_len_sem', 'paragraph_len_q1', 'paragraph_len_median', 'paragraph_len_q3', 'paragraph_len_skew', 'paragraph_len_kurt', 'paragraph_len_sum', 'paragraph_word_count_mean', 'paragraph_word_count_std', 'paragraph_word_count_min', 'paragraph_word_count_max', 'paragraph_word_count_first', 'paragraph_word_count_last', 'paragraph_word_count_sem', 'paragraph_word_count_q1', 'paragraph_word_count_median', 'paragraph_word_count_q3', 'paragraph_word_count_skew', 'paragraph_word_count_kurt', 'paragraph_word_count_sum', 'essay_words_count', 'essay_words_mean', 'essay_words_std', 'essay_words_max', 'essay_words_median', 'essay_words_sum', 'essay_words_last', 'essay_words_q1', 'essay_words_q3', 'essay_words_iqr', 'essay_words_min', 'tok_0', 'tok_1', 'tok_2', 'tok_3', 'tok_4', 'tok_5', 'tok_6', 'tok_7', 'tok_8', 'tok_9', 'tok_10', 'tok_11', 'tok_12', 'tok_13', 'tok_14', 'tok_15', 'tok_16', 'tok_17', 'tok_18', 'tok_19', 'tok_20', 'tok_21', 'tok_22', 'tok_23', 'tok_24', 'tok_25', 'tok_26']\n",
      "Calculating importance for model 1/50\n",
      "Calculating importance for model 2/50\n",
      "Calculating importance for model 3/50\n",
      "Calculating importance for model 4/50\n",
      "Calculating importance for model 5/50\n",
      "Calculating importance for model 6/50\n",
      "Calculating importance for model 7/50\n",
      "Calculating importance for model 8/50\n",
      "Calculating importance for model 9/50\n",
      "Calculating importance for model 10/50\n",
      "Calculating importance for model 11/50\n",
      "Calculating importance for model 12/50\n",
      "Calculating importance for model 13/50\n",
      "Calculating importance for model 14/50\n",
      "Calculating importance for model 15/50\n",
      "Calculating importance for model 16/50\n",
      "Calculating importance for model 17/50\n",
      "Calculating importance for model 18/50\n",
      "Calculating importance for model 19/50\n",
      "Calculating importance for model 20/50\n",
      "Calculating importance for model 21/50\n",
      "Calculating importance for model 22/50\n",
      "Calculating importance for model 23/50\n",
      "Calculating importance for model 24/50\n",
      "Calculating importance for model 25/50\n",
      "Calculating importance for model 26/50\n",
      "Calculating importance for model 27/50\n",
      "Calculating importance for model 28/50\n",
      "Calculating importance for model 29/50\n",
      "Calculating importance for model 30/50\n",
      "Calculating importance for model 31/50\n",
      "Calculating importance for model 32/50\n",
      "Calculating importance for model 33/50\n",
      "Calculating importance for model 34/50\n",
      "Calculating importance for model 35/50\n",
      "Calculating importance for model 36/50\n",
      "Calculating importance for model 37/50\n",
      "Calculating importance for model 38/50\n",
      "Calculating importance for model 39/50\n",
      "Calculating importance for model 40/50\n",
      "Calculating importance for model 41/50\n",
      "Calculating importance for model 42/50\n",
      "Calculating importance for model 43/50\n",
      "Calculating importance for model 44/50\n",
      "Calculating importance for model 45/50\n",
      "Calculating importance for model 46/50\n",
      "Calculating importance for model 47/50\n",
      "Calculating importance for model 48/50\n",
      "Calculating importance for model 49/50\n",
      "Calculating importance for model 50/50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def run_lgb_model_(model, X_train, y_train, X_valid, y_valid, X_test, boosting_type):\n",
    "    if boosting_type != 'dart':\n",
    "        model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_valid, y_valid)], \n",
    "                  callbacks=[lgb.early_stopping(250, first_metric_only=True, verbose=False)])\n",
    "    else:\n",
    "        model.fit(X_train, y_train)  # No early stopping for DART\n",
    "\n",
    "    valid_predictions = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "    test_predictions = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "    return valid_predictions, test_predictions\n",
    "\n",
    "def run_lgb_cv_(train_feats, test_feats, train_cols, target_col, lgb_params, boosting_type, seed, n_repeats, n_splits):\n",
    "    oof_results = pd.DataFrame(columns=['id', 'score', 'prediction'])\n",
    "\n",
    "    X = train_feats[train_cols]\n",
    "    y = train_feats[target_col].values\n",
    "    X_test = test_feats[train_cols]\n",
    "\n",
    "    models = []\n",
    "    for i in range(n_repeats):\n",
    "        skf = KFold(n_splits=n_splits, shuffle=True, random_state=seed + i)\n",
    "\n",
    "        for train_idx, valid_idx in skf.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X.iloc[valid_idx], y[valid_idx]\n",
    "\n",
    "            model_lgb = lgb.LGBMRegressor(**lgb_params, verbose=-1, random_state=seed)\n",
    "            valid_preds_lgb, test_preds_lgb = run_lgb_model_(model = model_lgb,\n",
    "                                               X_train=X_train, y_train=y_train, \n",
    "                                               X_valid=X_valid, y_valid=y_valid, \n",
    "                                               X_test=X_test, boosting_type=boosting_type)\n",
    "        \n",
    "            tmp_df = train_feats.loc[valid_idx][['id','score']]\n",
    "            tmp_df['prediction'] = valid_preds_lgb\n",
    "            oof_results = pd.concat([oof_results, tmp_df])\n",
    "            models.append(model_lgb)\n",
    "\n",
    "    avg_preds = oof_results.groupby(['id', 'score'])['prediction'].mean().reset_index()\n",
    "    rmse = mean_squared_error(avg_preds['score'], avg_preds['prediction'], squared=False)\n",
    "    print(f\"LGBM Average RMSE over {n_repeats * n_splits} folds: {rmse:.6f}\")\n",
    "    \n",
    "    return models, oof_results, rmse\n",
    "\n",
    "def cv_pipeline_(train_feats, test_feats, lgb_params, boosting_type, seed=42, n_repeats=5, n_splits=10):\n",
    "    target_col = 'score'\n",
    "    drop_cols = ['id']\n",
    "    train_cols = [col for col in train_feats.columns if col not in [target_col] + drop_cols]\n",
    "\n",
    "    missing_cols = [col for col in train_cols if col not in test_feats.columns]\n",
    "    missing_cols_df = pd.DataFrame({col: np.nan for col in missing_cols}, index=test_feats.index)\n",
    "    test_feats = pd.concat([test_feats, missing_cols_df], axis=1)\n",
    "\n",
    "    models, oof_preds, rmse = run_lgb_cv_(train_feats=train_feats, test_feats=test_feats, \n",
    "                                         train_cols=train_cols, target_col=target_col, \n",
    "                                         lgb_params=lgb_params, boosting_type=boosting_type,\n",
    "                                         seed=seed, n_repeats=n_repeats, n_splits=n_splits)\n",
    "\n",
    "    # Calculate permutation feature importance\n",
    "    importance_scores = np.zeros((len(models), len(train_cols)))\n",
    "    for i, model in enumerate(models):\n",
    "        result = permutation_importance(model, train_feats[train_cols], train_feats[target_col], n_repeats=30, random_state=seed)\n",
    "        importance_scores[i, :] = result.importances_mean\n",
    "\n",
    "    importance_scores_mean = np.mean(importance_scores, axis=0)\n",
    "    feature_importance = pd.DataFrame({'feature': train_cols, 'importance': importance_scores_mean})\n",
    "    feature_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "    print(\"Number of models:\", len(models))\n",
    "    print(\"Train columns:\", train_cols)\n",
    "\n",
    "    if np.all(importance_scores == 0):\n",
    "        print(\"Warning: All importance scores are zero. Check your data and model.\")\n",
    "\n",
    "    importance_scores_mean = np.mean(importance_scores, axis=0)\n",
    "    feature_importance = pd.DataFrame({'feature': train_cols, 'importance': importance_scores_mean})\n",
    "    feature_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "    return models, oof_preds, rmse, feature_importance\n",
    "\n",
    "_,_,_,feature_importance = cv_pipeline_(train_feats, test_feats, lgbm_params, 'gbdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_important_feats = feature_importance[feature_importance['importance']==0]['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373                              tok_21\n",
       "372                              tok_20\n",
       "366                              tok_14\n",
       "3                       action_time_min\n",
       "367                              tok_15\n",
       "375                              tok_23\n",
       "368                              tok_16\n",
       "374                              tok_22\n",
       "369                              tok_17\n",
       "376                              tok_24\n",
       "377                              tok_25\n",
       "370                              tok_18\n",
       "371                              tok_19\n",
       "258                   up_event_12_count\n",
       "37     cursor_position_change1_quantile\n",
       "95          word_count_change3_quantile\n",
       "261                   up_event_15_count\n",
       "256                   up_event_10_count\n",
       "248                    up_event_2_count\n",
       "245                 down_event_15_count\n",
       "120         word_count_change5_quantile\n",
       "313                      low_sent_count\n",
       "112    cursor_position_change5_quantile\n",
       "92               word_count_change3_max\n",
       "351                     essay_words_min\n",
       "87     cursor_position_change3_quantile\n",
       "70          word_count_change2_quantile\n",
       "67               word_count_change2_max\n",
       "62     cursor_position_change2_quantile\n",
       "345                  essay_words_median\n",
       "45          word_count_change1_quantile\n",
       "42               word_count_change1_max\n",
       "378                              tok_26\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_important_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 400\n",
      "Ridge Average RMSE over 30 folds: 0.621593\n",
      "Alpha 450\n",
      "Ridge Average RMSE over 30 folds: 0.621564\n",
      "Alpha 500\n",
      "Ridge Average RMSE over 30 folds: 0.621566\n",
      "Alpha 550\n",
      "Ridge Average RMSE over 30 folds: 0.621593\n",
      "Alpha 600\n",
      "Ridge Average RMSE over 30 folds: 0.621640\n",
      "Alpha 650\n",
      "Ridge Average RMSE over 30 folds: 0.621703\n",
      "Alpha 700\n",
      "Ridge Average RMSE over 30 folds: 0.621779\n",
      "Alpha 750\n",
      "Ridge Average RMSE over 30 folds: 0.621867\n",
      "Alpha 800\n",
      "Ridge Average RMSE over 30 folds: 0.621963\n",
      "Alpha 850\n",
      "Ridge Average RMSE over 30 folds: 0.622068\n"
     ]
    }
   ],
   "source": [
    "##### RIDGE REGRESSION FOR BLENDING\n",
    "train_feats = pd.read_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats = pd.read_pickle('feat_store_hybrid/test_super.pkl')\n",
    "\n",
    "train_feats = train_feats.drop(columns=non_important_feats)\n",
    "test_feats = test_feats.drop(columns=test_feats.columns.intersection(non_important_feats)) #yj 0.621593\n",
    "train_feats = preprocess_feats(train_feats, PowerTransformer('yeo-johnson')) \n",
    "test_feats = preprocess_feats(test_feats, PowerTransformer('yeo-johnson'))\n",
    "# train_feats = preprocess_feats(train_feats, PowerTransformer('yeo-johnson'))\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "n_repeats = 5\n",
    "n_splits = 6\n",
    "alpha = 450\n",
    "param_grid = list(np.arange(400, 900, 50))\n",
    "\n",
    "for alpha in param_grid:\n",
    "    ridge_params = {'alpha': alpha}  # Create a dictionary with alpha\n",
    "    print(f'Alpha {alpha}')\n",
    "    _, _, ridge_oof_preds, _ = ridge_cv_pipeline(train_feats, test_feats, ridge_params, seed=42, n_repeats=n_repeats, n_splits=n_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
