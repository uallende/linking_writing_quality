{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m6_feats_comb import *\n",
    "from m7_utils import *\n",
    "from m5_models import *\n",
    "from m3_model_params import lgb_params_2 as lgbm_params\n",
    "from m3_model_params import xgb_params_2 as xgb_params\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, StandardScaler\n",
    "from m3_model_params import non_important_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\n",
    "FEAT_STORE_DIR = 'feat_store_combined'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:25<00:00,  2.58s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 14506.76it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13732.50it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 12198.90it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12580.71it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12888.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:419: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:421: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 24.08it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 50533.78it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 40201.00it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 36900.04it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 34285.86it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 24338.32it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:419: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:421: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:04<00:00, 580.04it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:182: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  word_len_feats = diverse_stats(pd.Series(word_lengths), scope)\n",
      "100%|██████████| 2471/2471 [00:04<00:00, 570.17it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3350.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)\n",
    "logs = normalise_up_down_times(logs)\n",
    "\n",
    "train_logs = normalise_up_down_times(train_logs)\n",
    "test_logs = normalise_up_down_times(test_logs)\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "\n",
    "essays = getEssays(logs)\n",
    "sent_feats = compute_sentence_aggregations(essays)\n",
    "par_feats = compute_paragraph_aggregations(essays)\n",
    "word_feats = create_word_length_features(essays, 'essay', 'id', 'essay_words')\n",
    "\n",
    "train_sent = sent_feats[sent_feats['id'].isin(train_ids)]\n",
    "train_par = par_feats[par_feats['id'].isin(train_ids)]\n",
    "train_words = word_feats[word_feats['id'].isin(train_ids)]\n",
    "test_sent = sent_feats[sent_feats['id'].isin(test_ids)]\n",
    "test_par = par_feats[par_feats['id'].isin(test_ids)]\n",
    "test_words = word_feats[word_feats['id'].isin(test_ids)]\n",
    "\n",
    "train_vector = countvectorize_one_one(train_logs)\n",
    "test_vector = countvectorize_one_one(test_logs)\n",
    "\n",
    "train_feats = train_feats.merge(train_sent, how='left', on='id')\n",
    "train_feats = train_feats.merge(train_par, how='left', on='id')\n",
    "train_feats = train_feats.merge(train_words, how='left', on='id')\n",
    "train_feats = pd.concat([train_feats, train_vector], axis=1)\n",
    "\n",
    "test_feats = test_feats.merge(test_sent, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_par, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_words, on='id', how='left')\n",
    "test_feats = pd.concat([test_feats, test_vector], axis=1)\n",
    "\n",
    "train_feats.to_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats.to_pickle('feat_store_hybrid/test_feats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = pd.read_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats = pd.read_pickle('feat_store_hybrid/test_feats.pkl')\n",
    "train_feats.fillna(-10e10, inplace=True)\n",
    "\n",
    "train_feats.isna().sum()[train_feats.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.isna().sum()[train_feats.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NaNs after merging:\n",
      "id                     0\n",
      "down_event_10_count    0\n",
      "up_event_4_count       0\n",
      "up_event_3_count       0\n",
      "up_event_1_count       0\n",
      "up_event_0_count       0\n",
      "down_event_14_count    0\n",
      "down_event_13_count    0\n",
      "down_event_12_count    0\n",
      "down_event_11_count    0\n",
      "dtype: int64\n",
      "id                     0\n",
      "down_event_10_count    0\n",
      "down_event_8_count     0\n",
      "down_event_7_count     0\n",
      "down_event_6_count     0\n",
      "down_event_5_count     0\n",
      "down_event_4_count     0\n",
      "down_event_3_count     0\n",
      "down_event_2_count     0\n",
      "down_event_1_count     0\n",
      "dtype: int64\n",
      "Non-numeric columns: Index(['id'], dtype='object')\n",
      "Check for Infinities on Numeric Columns:\n",
      "event_id_max           0\n",
      "down_event_10_count    0\n",
      "up_event_4_count       0\n",
      "up_event_3_count       0\n",
      "up_event_1_count       0\n",
      "up_event_0_count       0\n",
      "down_event_14_count    0\n",
      "down_event_13_count    0\n",
      "down_event_12_count    0\n",
      "down_event_11_count    0\n",
      "dtype: int64\n",
      "Data types:\n",
      "float64    296\n",
      "int64       51\n",
      "object       1\n",
      "dtype: int64\n",
      "float64    306\n",
      "int64       27\n",
      "object       1\n",
      "dtype: int64\n",
      "Columns used for training:\n",
      "['event_id_max', 'up_time_max', 'action_time_max', 'action_time_mean', 'action_time_std', 'action_time_quantile', 'action_time_sem', 'action_time_sum', 'action_time_skew', 'action_time_kurt', 'activity_nunique', 'down_event_nunique', 'up_event_nunique', 'text_change_nunique', 'cursor_position_nunique', 'cursor_position_max', 'cursor_position_quantile', 'cursor_position_sem', 'cursor_position_mean', 'word_count_nunique', 'word_count_max', 'word_count_quantile', 'word_count_sem', 'word_count_mean', 'action_time_gap1_max', 'action_time_gap1_min', 'action_time_gap1_mean', 'action_time_gap1_std', 'action_time_gap1_quantile', 'action_time_gap1_sem', 'action_time_gap1_sum', 'action_time_gap1_skew', 'action_time_gap1_kurt', 'cursor_position_change1_max', 'cursor_position_change1_mean', 'cursor_position_change1_std', 'cursor_position_change1_sem', 'cursor_position_change1_sum', 'cursor_position_change1_skew', 'cursor_position_change1_kurt', 'word_count_change1_mean', 'word_count_change1_std', 'word_count_change1_sem', 'word_count_change1_sum', 'word_count_change1_skew', 'word_count_change1_kurt', 'action_time_gap2_max', 'action_time_gap2_min', 'action_time_gap2_mean', 'action_time_gap2_std', 'action_time_gap2_quantile', 'action_time_gap2_sem', 'action_time_gap2_sum', 'action_time_gap2_skew', 'action_time_gap2_kurt', 'cursor_position_change2_max', 'cursor_position_change2_mean', 'cursor_position_change2_std', 'cursor_position_change2_sem', 'cursor_position_change2_sum', 'cursor_position_change2_skew', 'cursor_position_change2_kurt', 'word_count_change2_mean', 'word_count_change2_std', 'word_count_change2_sem', 'word_count_change2_sum', 'word_count_change2_skew', 'word_count_change2_kurt', 'action_time_gap3_max', 'action_time_gap3_min', 'action_time_gap3_mean', 'action_time_gap3_std', 'action_time_gap3_quantile', 'action_time_gap3_sem', 'action_time_gap3_sum', 'action_time_gap3_skew', 'action_time_gap3_kurt', 'cursor_position_change3_max', 'cursor_position_change3_mean', 'cursor_position_change3_std', 'cursor_position_change3_sem', 'cursor_position_change3_sum', 'cursor_position_change3_skew', 'cursor_position_change3_kurt', 'word_count_change3_mean', 'word_count_change3_std', 'word_count_change3_sem', 'word_count_change3_sum', 'word_count_change3_skew', 'word_count_change3_kurt', 'action_time_gap5_max', 'action_time_gap5_min', 'action_time_gap5_mean', 'action_time_gap5_std', 'action_time_gap5_quantile', 'action_time_gap5_sem', 'action_time_gap5_sum', 'action_time_gap5_skew', 'action_time_gap5_kurt', 'cursor_position_change5_max', 'cursor_position_change5_mean', 'cursor_position_change5_std', 'cursor_position_change5_sem', 'cursor_position_change5_sum', 'cursor_position_change5_skew', 'cursor_position_change5_kurt', 'word_count_change5_max', 'word_count_change5_mean', 'word_count_change5_std', 'word_count_change5_sem', 'word_count_change5_sum', 'word_count_change5_skew', 'word_count_change5_kurt', 'action_time_gap10_max', 'action_time_gap10_min', 'action_time_gap10_mean', 'action_time_gap10_std', 'action_time_gap10_quantile', 'action_time_gap10_sem', 'action_time_gap10_sum', 'action_time_gap10_skew', 'action_time_gap10_kurt', 'cursor_position_change10_max', 'cursor_position_change10_mean', 'cursor_position_change10_std', 'cursor_position_change10_quantile', 'cursor_position_change10_sem', 'cursor_position_change10_sum', 'cursor_position_change10_skew', 'cursor_position_change10_kurt', 'word_count_change10_max', 'word_count_change10_mean', 'word_count_change10_std', 'word_count_change10_quantile', 'word_count_change10_sem', 'word_count_change10_sum', 'word_count_change10_skew', 'word_count_change10_kurt', 'action_time_gap20_max', 'action_time_gap20_min', 'action_time_gap20_mean', 'action_time_gap20_std', 'action_time_gap20_quantile', 'action_time_gap20_sem', 'action_time_gap20_sum', 'action_time_gap20_skew', 'action_time_gap20_kurt', 'cursor_position_change20_max', 'cursor_position_change20_mean', 'cursor_position_change20_std', 'cursor_position_change20_quantile', 'cursor_position_change20_sem', 'cursor_position_change20_sum', 'cursor_position_change20_skew', 'cursor_position_change20_kurt', 'word_count_change20_max', 'word_count_change20_mean', 'word_count_change20_std', 'word_count_change20_quantile', 'word_count_change20_sem', 'word_count_change20_sum', 'word_count_change20_skew', 'word_count_change20_kurt', 'action_time_gap50_max', 'action_time_gap50_min', 'action_time_gap50_mean', 'action_time_gap50_std', 'action_time_gap50_quantile', 'action_time_gap50_sem', 'action_time_gap50_sum', 'action_time_gap50_skew', 'action_time_gap50_kurt', 'cursor_position_change50_max', 'cursor_position_change50_mean', 'cursor_position_change50_std', 'cursor_position_change50_quantile', 'cursor_position_change50_sem', 'cursor_position_change50_sum', 'cursor_position_change50_skew', 'cursor_position_change50_kurt', 'word_count_change50_max', 'word_count_change50_mean', 'word_count_change50_std', 'word_count_change50_quantile', 'word_count_change50_sem', 'word_count_change50_sum', 'word_count_change50_skew', 'word_count_change50_kurt', 'action_time_gap100_max', 'action_time_gap100_min', 'action_time_gap100_mean', 'action_time_gap100_std', 'action_time_gap100_quantile', 'action_time_gap100_sem', 'action_time_gap100_sum', 'action_time_gap100_skew', 'action_time_gap100_kurt', 'cursor_position_change100_max', 'cursor_position_change100_mean', 'cursor_position_change100_std', 'cursor_position_change100_quantile', 'cursor_position_change100_sem', 'cursor_position_change100_sum', 'cursor_position_change100_skew', 'cursor_position_change100_kurt', 'word_count_change100_max', 'word_count_change100_mean', 'word_count_change100_std', 'word_count_change100_quantile', 'word_count_change100_sem', 'word_count_change100_sum', 'word_count_change100_skew', 'word_count_change100_kurt', 'activity_0_count', 'activity_1_count', 'activity_2_count', 'activity_3_count', 'activity_4_count', 'down_event_0_count', 'down_event_1_count', 'down_event_2_count', 'down_event_3_count', 'down_event_4_count', 'down_event_5_count', 'down_event_6_count', 'down_event_7_count', 'down_event_8_count', 'down_event_9_count', 'down_event_10_count', 'down_event_11_count', 'down_event_12_count', 'down_event_13_count', 'down_event_14_count', 'up_event_0_count', 'up_event_1_count', 'up_event_3_count', 'up_event_4_count', 'up_event_5_count', 'up_event_6_count', 'up_event_7_count', 'up_event_8_count', 'up_event_9_count', 'up_event_11_count', 'up_event_13_count', 'up_event_14_count', 'text_change_0_count', 'text_change_1_count', 'text_change_2_count', 'text_change_3_count', 'text_change_4_count', 'text_change_5_count', 'text_change_6_count', 'text_change_7_count', 'text_change_8_count', 'text_change_9_count', 'text_change_10_count', 'text_change_11_count', 'text_change_12_count', 'text_change_13_count', 'text_change_14_count', 'punct_cnt', 'input_word_count', 'input_word_length_mean', 'input_word_length_max', 'input_word_length_std', 'word_time_ratio', 'word_event_ratio', 'event_time_ratio', 'idle_time_ratio', 'sent_count', 'sent_len_mean', 'sent_len_std', 'sent_len_min', 'sent_len_max', 'sent_len_first', 'sent_len_last', 'sent_len_sem', 'sent_len_q1', 'sent_len_median', 'sent_len_q3', 'sent_len_skew', 'sent_len_kurt', 'sent_len_sum', 'sent_word_count_mean', 'sent_word_count_std', 'sent_word_count_min', 'sent_word_count_max', 'sent_word_count_first', 'sent_word_count_last', 'sent_word_count_sem', 'sent_word_count_q1', 'sent_word_count_median', 'sent_word_count_q3', 'sent_word_count_skew', 'sent_word_count_kurt', 'sent_word_count_sum', 'paragraph_count', 'paragraph_len_mean', 'paragraph_len_std', 'paragraph_len_min', 'paragraph_len_max', 'paragraph_len_first', 'paragraph_len_last', 'paragraph_len_sem', 'paragraph_len_q1', 'paragraph_len_median', 'paragraph_len_q3', 'paragraph_len_skew', 'paragraph_len_kurt', 'paragraph_len_sum', 'paragraph_word_count_mean', 'paragraph_word_count_std', 'paragraph_word_count_min', 'paragraph_word_count_max', 'paragraph_word_count_first', 'paragraph_word_count_last', 'paragraph_word_count_sem', 'paragraph_word_count_q1', 'paragraph_word_count_median', 'paragraph_word_count_q3', 'paragraph_word_count_skew', 'paragraph_word_count_kurt', 'paragraph_word_count_sum', 'essay_words_count', 'essay_words_mean', 'essay_words_std', 'essay_words_max', 'essay_words_sum', 'essay_words_last', 'essay_words_q1', 'essay_words_q3', 'essay_words_iqr', 'tok_0', 'tok_1', 'tok_2', 'tok_3', 'tok_4', 'tok_5', 'tok_6', 'tok_7', 'tok_8', 'tok_9', 'tok_10', 'tok_11', 'tok_12', 'tok_13']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/SPF_experiments_3.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_experiments_3.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m n_splits\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_experiments_3.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m ridge_params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1.0\u001b[39m}  \u001b[39m# Example parameter for Ridge\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_experiments_3.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m _,_,_,_ \u001b[39m=\u001b[39m ridge_cv_pipeline(train_feats, test_feats, ridge_params, seed\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m, n_repeats\u001b[39m=\u001b[39;49mn_repeats, n_splits\u001b[39m=\u001b[39;49mn_splits)\n",
      "File \u001b[0;32m~/Projects/Kaggle/linking-writing/m5_models.py:264\u001b[0m, in \u001b[0;36mridge_cv_pipeline\u001b[0;34m(train_feats, test_feats, ridge_params, seed, n_repeats, n_splits)\u001b[0m\n\u001b[1;32m    261\u001b[0m train_feats\u001b[39m.\u001b[39mreplace([np\u001b[39m.\u001b[39minf, \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf], np\u001b[39m.\u001b[39mnan, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    262\u001b[0m test_feats\u001b[39m.\u001b[39mreplace([np\u001b[39m.\u001b[39minf, \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf], np\u001b[39m.\u001b[39mnan, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 264\u001b[0m test_preds, oof_preds, rmse, model \u001b[39m=\u001b[39m run_ridge_cv(train_feats\u001b[39m=\u001b[39;49mtrain_feats,\n\u001b[1;32m    265\u001b[0m                                                   test_feats\u001b[39m=\u001b[39;49mtest_feats,\n\u001b[1;32m    266\u001b[0m                                                   train_cols\u001b[39m=\u001b[39;49mtrain_cols,\n\u001b[1;32m    267\u001b[0m                                                   target_col\u001b[39m=\u001b[39;49mtarget_col,\n\u001b[1;32m    268\u001b[0m                                                   ridge_params\u001b[39m=\u001b[39;49mridge_params,\n\u001b[1;32m    269\u001b[0m                                                   seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    270\u001b[0m                                                   n_repeats\u001b[39m=\u001b[39;49mn_repeats,\n\u001b[1;32m    271\u001b[0m                                                   n_splits\u001b[39m=\u001b[39;49mn_splits)\n\u001b[1;32m    273\u001b[0m \u001b[39mreturn\u001b[39;00m test_preds, oof_preds, rmse, model\n",
      "File \u001b[0;32m~/Projects/Kaggle/linking-writing/m5_models.py:235\u001b[0m, in \u001b[0;36mrun_ridge_cv\u001b[0;34m(train_feats, test_feats, train_cols, target_col, ridge_params, seed, n_repeats, n_splits)\u001b[0m\n\u001b[1;32m    232\u001b[0m X_valid, y_valid \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mloc[valid_idx], y\u001b[39m.\u001b[39mloc[valid_idx]\n\u001b[1;32m    234\u001b[0m model_ridge \u001b[39m=\u001b[39m Ridge(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mridge_params, random_state\u001b[39m=\u001b[39mseed)\n\u001b[0;32m--> 235\u001b[0m valid_preds_ridge, test_preds_ridge \u001b[39m=\u001b[39m run_ridge_model(model\u001b[39m=\u001b[39;49mmodel_ridge,\n\u001b[1;32m    236\u001b[0m                                                       X_train\u001b[39m=\u001b[39;49mX_train, y_train\u001b[39m=\u001b[39;49my_train,\n\u001b[1;32m    237\u001b[0m                                                       X_valid\u001b[39m=\u001b[39;49mX_valid, X_test\u001b[39m=\u001b[39;49mX_test)\n\u001b[1;32m    239\u001b[0m tmp_df \u001b[39m=\u001b[39m train_feats\u001b[39m.\u001b[39mloc[valid_idx][[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m    240\u001b[0m tmp_df[\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m valid_preds_ridge\n",
      "File \u001b[0;32m~/Projects/Kaggle/linking-writing/m5_models.py:215\u001b[0m, in \u001b[0;36mrun_ridge_model\u001b[0;34m(model, X_train, y_train, X_valid, X_test)\u001b[0m\n\u001b[1;32m    213\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    214\u001b[0m valid_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_valid)\n\u001b[0;32m--> 215\u001b[0m test_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[1;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m valid_predictions, test_predictions\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/linear_model/_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    373\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/linear_model/_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    367\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[1;32m    958\u001b[0m             array,\n\u001b[1;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    123\u001b[0m     X,\n\u001b[1;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "#0.6197 (super), 0.620938 (pause), 0.620495 (StandardScaler), 0.621384 (MinMax), 0.619713(yeo-johnson), 0.619367(super with yeo-johnson)\n",
    "# Super 0.608303, 0.610934 6,5\n",
    "# feats = pd.read_pickle('feat_store_hybrid/super_feats.pkl') \n",
    "# feats = preprocess_feats(feats, PowerTransformer('yeo-johnson'))\n",
    "#train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "# train_feats = feats[feats['id'].isin(train_ids)]\n",
    "# test_feats = feats[feats['id'].isin(test_ids)]\n",
    "\n",
    "train_feats = pd.read_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats = pd.read_pickle('feat_store_hybrid/test_feats.pkl')\n",
    "\n",
    "train_feats = train_feats.drop(columns=non_important_feats)\n",
    "test_feats = test_feats.drop(columns=test_feats.columns.intersection(non_important_feats))\n",
    "\n",
    "# train_feats = preprocess_feats(train_feats, PowerTransformer('yeo-johnson'))\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "train_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "train_feats.fillna(-10e6, inplace=True)\n",
    "test_feats.fillna(-10e6, inplace=True)\n",
    "\n",
    "# After merging and preprocessing\n",
    "print(\"Check for NaNs after merging:\")\n",
    "print(train_feats.isna().sum().sort_values(ascending=False).head(10))\n",
    "print(test_feats.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "non_numeric_cols = train_feats.select_dtypes(exclude=[np.number]).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Now check for infinities on numeric columns\n",
    "print(\"Check for Infinities on Numeric Columns:\")\n",
    "print(np.isinf(train_feats.select_dtypes(include=[np.number])).sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Data types\n",
    "print(\"Data types:\")\n",
    "print(train_feats.dtypes.value_counts())\n",
    "print(test_feats.dtypes.value_counts())\n",
    "\n",
    "target_col = ['score']\n",
    "drop_cols = ['id']\n",
    "train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]\n",
    "# Ensure correct columns are used\n",
    "print(\"Columns used for training:\")\n",
    "print(train_cols)\n",
    "\n",
    "n_repeats=5\n",
    "n_splits=6\n",
    "\n",
    "ridge_params = {'alpha': 1.0}  # Example parameter for Ridge\n",
    "_,_,_,_ = ridge_cv_pipeline(train_feats, test_feats, ridge_params, seed=42, n_repeats=n_repeats, n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 0/5 [00:00<?, ?it/s]/root/miniconda3/envs/lrp/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:59:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Iterations: 100%|██████████| 5/5 [00:54<00:00, 10.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Average RMSE over 30 folds: 0.605156\n",
      "LGBM Average RMSE over 30 folds: 0.610728\n",
      "Blend RMSE 0.605737\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "#0.6197 (super), 0.620938 (pause), 0.620495 (StandardScaler), 0.621384 (MinMax), 0.619713(yeo-johnson), 0.619367(super with yeo-johnson)\n",
    "# Super 0.608303, 0.610934 6,5\n",
    "# feats = pd.read_pickle('feat_store_hybrid/super_feats.pkl') \n",
    "# feats = preprocess_feats(feats, PowerTransformer('yeo-johnson'))\n",
    "#train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "# train_feats = feats[feats['id'].isin(train_ids)]\n",
    "# test_feats = feats[feats['id'].isin(test_ids)]\n",
    "\n",
    "train_feats = pd.read_pickle('feat_store_hybrid/train_super.pkl')\n",
    "test_feats = pd.read_pickle('feat_store_hybrid/test_feats.pkl')\n",
    "\n",
    "train_feats = train_feats.drop(columns=non_important_feats)\n",
    "test_feats = test_feats.drop(columns=test_feats.columns.intersection(non_important_feats))\n",
    "\n",
    "# train_feats = preprocess_feats(train_feats, PowerTransformer('yeo-johnson'))\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "n_repeats=5\n",
    "n_splits=6\n",
    "\n",
    "_, oof_1, rmse, model1 = xgb_cv_pipeline(train_feats=train_feats, \n",
    "                                        test_feats=test_feats, \n",
    "                                        xgb_params=xgb_params, \n",
    "                                        seed=42, \n",
    "                                        n_repeats=n_repeats, \n",
    "                                        n_splits=n_splits)\n",
    "                                        \n",
    "_, oof_2, rmse, model1 = cv_pipeline(train_feats, \n",
    "                                     test_feats, \n",
    "                                     lgbm_params, \n",
    "                                     lgbm_params['boosting_type'],\n",
    "                                     seed = 42,\n",
    "                                     n_repeats= n_repeats,\n",
    "                                     n_splits = n_splits)\n",
    "\n",
    "blend = pd.concat([oof_1, oof_2], axis=0)\n",
    "blend_scores = blend.groupby(['id','score'])['prediction'].mean().reset_index()\n",
    "blend_rmse = mean_squared_error(blend_scores['score'], blend_scores['prediction'], squared=False)\n",
    "print(f'Blend RMSE {blend_rmse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Average RMSE over 50 folds: 0.608614\n",
      "Number of models: 50\n",
      "Train columns: ['event_id_max', 'up_time_max', 'action_time_max', 'action_time_min', 'action_time_mean', 'action_time_std', 'action_time_quantile', 'action_time_sem', 'action_time_sum', 'action_time_skew', 'action_time_kurt', 'activity_nunique', 'down_event_nunique', 'up_event_nunique', 'text_change_nunique', 'cursor_position_nunique', 'cursor_position_max', 'cursor_position_quantile', 'cursor_position_sem', 'cursor_position_mean', 'word_count_nunique', 'word_count_max', 'word_count_quantile', 'word_count_sem', 'word_count_mean', 'action_time_gap1_max', 'action_time_gap1_min', 'action_time_gap1_mean', 'action_time_gap1_std', 'action_time_gap1_quantile', 'action_time_gap1_sem', 'action_time_gap1_sum', 'action_time_gap1_skew', 'action_time_gap1_kurt', 'cursor_position_change1_max', 'cursor_position_change1_mean', 'cursor_position_change1_std', 'cursor_position_change1_quantile', 'cursor_position_change1_sem', 'cursor_position_change1_sum', 'cursor_position_change1_skew', 'cursor_position_change1_kurt', 'word_count_change1_max', 'word_count_change1_mean', 'word_count_change1_std', 'word_count_change1_quantile', 'word_count_change1_sem', 'word_count_change1_sum', 'word_count_change1_skew', 'word_count_change1_kurt', 'action_time_gap2_max', 'action_time_gap2_min', 'action_time_gap2_mean', 'action_time_gap2_std', 'action_time_gap2_quantile', 'action_time_gap2_sem', 'action_time_gap2_sum', 'action_time_gap2_skew', 'action_time_gap2_kurt', 'cursor_position_change2_max', 'cursor_position_change2_mean', 'cursor_position_change2_std', 'cursor_position_change2_quantile', 'cursor_position_change2_sem', 'cursor_position_change2_sum', 'cursor_position_change2_skew', 'cursor_position_change2_kurt', 'word_count_change2_max', 'word_count_change2_mean', 'word_count_change2_std', 'word_count_change2_quantile', 'word_count_change2_sem', 'word_count_change2_sum', 'word_count_change2_skew', 'word_count_change2_kurt', 'action_time_gap3_max', 'action_time_gap3_min', 'action_time_gap3_mean', 'action_time_gap3_std', 'action_time_gap3_quantile', 'action_time_gap3_sem', 'action_time_gap3_sum', 'action_time_gap3_skew', 'action_time_gap3_kurt', 'cursor_position_change3_max', 'cursor_position_change3_mean', 'cursor_position_change3_std', 'cursor_position_change3_quantile', 'cursor_position_change3_sem', 'cursor_position_change3_sum', 'cursor_position_change3_skew', 'cursor_position_change3_kurt', 'word_count_change3_max', 'word_count_change3_mean', 'word_count_change3_std', 'word_count_change3_quantile', 'word_count_change3_sem', 'word_count_change3_sum', 'word_count_change3_skew', 'word_count_change3_kurt', 'action_time_gap5_max', 'action_time_gap5_min', 'action_time_gap5_mean', 'action_time_gap5_std', 'action_time_gap5_quantile', 'action_time_gap5_sem', 'action_time_gap5_sum', 'action_time_gap5_skew', 'action_time_gap5_kurt', 'cursor_position_change5_max', 'cursor_position_change5_mean', 'cursor_position_change5_std', 'cursor_position_change5_quantile', 'cursor_position_change5_sem', 'cursor_position_change5_sum', 'cursor_position_change5_skew', 'cursor_position_change5_kurt', 'word_count_change5_max', 'word_count_change5_mean', 'word_count_change5_std', 'word_count_change5_quantile', 'word_count_change5_sem', 'word_count_change5_sum', 'word_count_change5_skew', 'word_count_change5_kurt', 'action_time_gap10_max', 'action_time_gap10_min', 'action_time_gap10_mean', 'action_time_gap10_std', 'action_time_gap10_quantile', 'action_time_gap10_sem', 'action_time_gap10_sum', 'action_time_gap10_skew', 'action_time_gap10_kurt', 'cursor_position_change10_max', 'cursor_position_change10_mean', 'cursor_position_change10_std', 'cursor_position_change10_quantile', 'cursor_position_change10_sem', 'cursor_position_change10_sum', 'cursor_position_change10_skew', 'cursor_position_change10_kurt', 'word_count_change10_max', 'word_count_change10_mean', 'word_count_change10_std', 'word_count_change10_quantile', 'word_count_change10_sem', 'word_count_change10_sum', 'word_count_change10_skew', 'word_count_change10_kurt', 'action_time_gap20_max', 'action_time_gap20_min', 'action_time_gap20_mean', 'action_time_gap20_std', 'action_time_gap20_quantile', 'action_time_gap20_sem', 'action_time_gap20_sum', 'action_time_gap20_skew', 'action_time_gap20_kurt', 'cursor_position_change20_max', 'cursor_position_change20_mean', 'cursor_position_change20_std', 'cursor_position_change20_quantile', 'cursor_position_change20_sem', 'cursor_position_change20_sum', 'cursor_position_change20_skew', 'cursor_position_change20_kurt', 'word_count_change20_max', 'word_count_change20_mean', 'word_count_change20_std', 'word_count_change20_quantile', 'word_count_change20_sem', 'word_count_change20_sum', 'word_count_change20_skew', 'word_count_change20_kurt', 'action_time_gap50_max', 'action_time_gap50_min', 'action_time_gap50_mean', 'action_time_gap50_std', 'action_time_gap50_quantile', 'action_time_gap50_sem', 'action_time_gap50_sum', 'action_time_gap50_skew', 'action_time_gap50_kurt', 'cursor_position_change50_max', 'cursor_position_change50_mean', 'cursor_position_change50_std', 'cursor_position_change50_quantile', 'cursor_position_change50_sem', 'cursor_position_change50_sum', 'cursor_position_change50_skew', 'cursor_position_change50_kurt', 'word_count_change50_max', 'word_count_change50_mean', 'word_count_change50_std', 'word_count_change50_quantile', 'word_count_change50_sem', 'word_count_change50_sum', 'word_count_change50_skew', 'word_count_change50_kurt', 'action_time_gap100_max', 'action_time_gap100_min', 'action_time_gap100_mean', 'action_time_gap100_std', 'action_time_gap100_quantile', 'action_time_gap100_sem', 'action_time_gap100_sum', 'action_time_gap100_skew', 'action_time_gap100_kurt', 'cursor_position_change100_max', 'cursor_position_change100_mean', 'cursor_position_change100_std', 'cursor_position_change100_quantile', 'cursor_position_change100_sem', 'cursor_position_change100_sum', 'cursor_position_change100_skew', 'cursor_position_change100_kurt', 'word_count_change100_max', 'word_count_change100_mean', 'word_count_change100_std', 'word_count_change100_quantile', 'word_count_change100_sem', 'word_count_change100_sum', 'word_count_change100_skew', 'word_count_change100_kurt', 'activity_0_count', 'activity_1_count', 'activity_2_count', 'activity_3_count', 'activity_4_count', 'down_event_0_count', 'down_event_1_count', 'down_event_2_count', 'down_event_3_count', 'down_event_4_count', 'down_event_5_count', 'down_event_6_count', 'down_event_7_count', 'down_event_8_count', 'down_event_9_count', 'down_event_10_count', 'down_event_11_count', 'down_event_12_count', 'down_event_13_count', 'down_event_14_count', 'down_event_15_count', 'up_event_0_count', 'up_event_1_count', 'up_event_2_count', 'up_event_3_count', 'up_event_4_count', 'up_event_5_count', 'up_event_6_count', 'up_event_7_count', 'up_event_8_count', 'up_event_9_count', 'up_event_10_count', 'up_event_11_count', 'up_event_12_count', 'up_event_13_count', 'up_event_14_count', 'up_event_15_count', 'text_change_0_count', 'text_change_1_count', 'text_change_2_count', 'text_change_3_count', 'text_change_4_count', 'text_change_5_count', 'text_change_6_count', 'text_change_7_count', 'text_change_8_count', 'text_change_9_count', 'text_change_10_count', 'text_change_11_count', 'text_change_12_count', 'text_change_13_count', 'text_change_14_count', 'punct_cnt', 'input_word_count', 'input_word_length_mean', 'input_word_length_max', 'input_word_length_std', 'word_time_ratio', 'word_event_ratio', 'event_time_ratio', 'idle_time_ratio', 'sent_count', 'sent_len_mean', 'sent_len_std', 'sent_len_min', 'sent_len_max', 'sent_len_first', 'sent_len_last', 'sent_len_sem', 'sent_len_q1', 'sent_len_median', 'sent_len_q3', 'sent_len_skew', 'sent_len_kurt', 'sent_len_sum', 'sent_word_count_mean', 'sent_word_count_std', 'sent_word_count_min', 'sent_word_count_max', 'sent_word_count_first', 'sent_word_count_last', 'sent_word_count_sem', 'sent_word_count_q1', 'sent_word_count_median', 'sent_word_count_q3', 'sent_word_count_skew', 'sent_word_count_kurt', 'sent_word_count_sum', 'low_sent_count', 'paragraph_count', 'paragraph_len_mean', 'paragraph_len_std', 'paragraph_len_min', 'paragraph_len_max', 'paragraph_len_first', 'paragraph_len_last', 'paragraph_len_sem', 'paragraph_len_q1', 'paragraph_len_median', 'paragraph_len_q3', 'paragraph_len_skew', 'paragraph_len_kurt', 'paragraph_len_sum', 'paragraph_word_count_mean', 'paragraph_word_count_std', 'paragraph_word_count_min', 'paragraph_word_count_max', 'paragraph_word_count_first', 'paragraph_word_count_last', 'paragraph_word_count_sem', 'paragraph_word_count_q1', 'paragraph_word_count_median', 'paragraph_word_count_q3', 'paragraph_word_count_skew', 'paragraph_word_count_kurt', 'paragraph_word_count_sum', 'essay_words_count', 'essay_words_mean', 'essay_words_std', 'essay_words_max', 'essay_words_median', 'essay_words_sum', 'essay_words_last', 'essay_words_q1', 'essay_words_q3', 'essay_words_iqr', 'essay_words_min', 'tok_0', 'tok_1', 'tok_2', 'tok_3', 'tok_4', 'tok_5', 'tok_6', 'tok_7', 'tok_8', 'tok_9', 'tok_10', 'tok_11', 'tok_12', 'tok_13', 'tok_14', 'tok_15', 'tok_16', 'tok_17', 'tok_18', 'tok_19', 'tok_20', 'tok_21', 'tok_22', 'tok_23', 'tok_24', 'tok_25', 'tok_26']\n",
      "Calculating importance for model 1/50\n",
      "Calculating importance for model 2/50\n",
      "Calculating importance for model 3/50\n",
      "Calculating importance for model 4/50\n",
      "Calculating importance for model 5/50\n",
      "Calculating importance for model 6/50\n",
      "Calculating importance for model 7/50\n",
      "Calculating importance for model 8/50\n",
      "Calculating importance for model 9/50\n",
      "Calculating importance for model 10/50\n",
      "Calculating importance for model 11/50\n",
      "Calculating importance for model 12/50\n",
      "Calculating importance for model 13/50\n",
      "Calculating importance for model 14/50\n",
      "Calculating importance for model 15/50\n",
      "Calculating importance for model 16/50\n",
      "Calculating importance for model 17/50\n",
      "Calculating importance for model 18/50\n",
      "Calculating importance for model 19/50\n",
      "Calculating importance for model 20/50\n",
      "Calculating importance for model 21/50\n",
      "Calculating importance for model 22/50\n",
      "Calculating importance for model 23/50\n",
      "Calculating importance for model 24/50\n",
      "Calculating importance for model 25/50\n",
      "Calculating importance for model 26/50\n",
      "Calculating importance for model 27/50\n",
      "Calculating importance for model 28/50\n",
      "Calculating importance for model 29/50\n",
      "Calculating importance for model 30/50\n",
      "Calculating importance for model 31/50\n",
      "Calculating importance for model 32/50\n",
      "Calculating importance for model 33/50\n",
      "Calculating importance for model 34/50\n",
      "Calculating importance for model 35/50\n",
      "Calculating importance for model 36/50\n",
      "Calculating importance for model 37/50\n",
      "Calculating importance for model 38/50\n",
      "Calculating importance for model 39/50\n",
      "Calculating importance for model 40/50\n",
      "Calculating importance for model 41/50\n",
      "Calculating importance for model 42/50\n",
      "Calculating importance for model 43/50\n",
      "Calculating importance for model 44/50\n",
      "Calculating importance for model 45/50\n",
      "Calculating importance for model 46/50\n",
      "Calculating importance for model 47/50\n",
      "Calculating importance for model 48/50\n",
      "Calculating importance for model 49/50\n",
      "Calculating importance for model 50/50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def run_lgb_model_(model, X_train, y_train, X_valid, y_valid, X_test, boosting_type):\n",
    "    if boosting_type != 'dart':\n",
    "        model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_valid, y_valid)], \n",
    "                  callbacks=[lgb.early_stopping(250, first_metric_only=True, verbose=False)])\n",
    "    else:\n",
    "        model.fit(X_train, y_train)  # No early stopping for DART\n",
    "\n",
    "    valid_predictions = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "    test_predictions = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "    return valid_predictions, test_predictions\n",
    "\n",
    "def run_lgb_cv_(train_feats, test_feats, train_cols, target_col, lgb_params, boosting_type, seed, n_repeats, n_splits):\n",
    "    oof_results = pd.DataFrame(columns=['id', 'score', 'prediction'])\n",
    "\n",
    "    X = train_feats[train_cols]\n",
    "    y = train_feats[target_col].values\n",
    "    X_test = test_feats[train_cols]\n",
    "\n",
    "    models = []\n",
    "    for i in range(n_repeats):\n",
    "        skf = KFold(n_splits=n_splits, shuffle=True, random_state=seed + i)\n",
    "\n",
    "        for train_idx, valid_idx in skf.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X.iloc[valid_idx], y[valid_idx]\n",
    "\n",
    "            model_lgb = lgb.LGBMRegressor(**lgb_params, verbose=-1, random_state=seed)\n",
    "            valid_preds_lgb, test_preds_lgb = run_lgb_model_(model = model_lgb,\n",
    "                                               X_train=X_train, y_train=y_train, \n",
    "                                               X_valid=X_valid, y_valid=y_valid, \n",
    "                                               X_test=X_test, boosting_type=boosting_type)\n",
    "        \n",
    "            tmp_df = train_feats.loc[valid_idx][['id','score']]\n",
    "            tmp_df['prediction'] = valid_preds_lgb\n",
    "            oof_results = pd.concat([oof_results, tmp_df])\n",
    "            models.append(model_lgb)\n",
    "\n",
    "    avg_preds = oof_results.groupby(['id', 'score'])['prediction'].mean().reset_index()\n",
    "    rmse = mean_squared_error(avg_preds['score'], avg_preds['prediction'], squared=False)\n",
    "    print(f\"LGBM Average RMSE over {n_repeats * n_splits} folds: {rmse:.6f}\")\n",
    "    \n",
    "    return models, oof_results, rmse\n",
    "\n",
    "def cv_pipeline_(train_feats, test_feats, lgb_params, boosting_type, seed=42, n_repeats=5, n_splits=10):\n",
    "    target_col = 'score'\n",
    "    drop_cols = ['id']\n",
    "    train_cols = [col for col in train_feats.columns if col not in [target_col] + drop_cols]\n",
    "\n",
    "    missing_cols = [col for col in train_cols if col not in test_feats.columns]\n",
    "    missing_cols_df = pd.DataFrame({col: np.nan for col in missing_cols}, index=test_feats.index)\n",
    "    test_feats = pd.concat([test_feats, missing_cols_df], axis=1)\n",
    "\n",
    "    train_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    models, oof_preds, rmse = run_lgb_cv_(train_feats=train_feats, test_feats=test_feats, \n",
    "                                         train_cols=train_cols, target_col=target_col, \n",
    "                                         lgb_params=lgb_params, boosting_type=boosting_type,\n",
    "                                         seed=seed, n_repeats=n_repeats, n_splits=n_splits)\n",
    "\n",
    "    # Calculate permutation feature importance\n",
    "    importance_scores = np.zeros((len(models), len(train_cols)))\n",
    "    for i, model in enumerate(models):\n",
    "        result = permutation_importance(model, train_feats[train_cols], train_feats[target_col], n_repeats=30, random_state=seed)\n",
    "        importance_scores[i, :] = result.importances_mean\n",
    "\n",
    "    importance_scores_mean = np.mean(importance_scores, axis=0)\n",
    "    feature_importance = pd.DataFrame({'feature': train_cols, 'importance': importance_scores_mean})\n",
    "    feature_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "    print(\"Number of models:\", len(models))\n",
    "    print(\"Train columns:\", train_cols)\n",
    "\n",
    "    # Calculate permutation feature importance\n",
    "    importance_scores = np.zeros((len(models), len(train_cols)))\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"Calculating importance for model {i+1}/{len(models)}\")\n",
    "        result = permutation_importance(model, train_feats[train_cols], train_feats[target_col], n_repeats=10, random_state=seed)\n",
    "        importance_scores[i, :] = result.importances_mean\n",
    "\n",
    "    if np.all(importance_scores == 0):\n",
    "        print(\"Warning: All importance scores are zero. Check your data and model.\")\n",
    "\n",
    "    importance_scores_mean = np.mean(importance_scores, axis=0)\n",
    "    feature_importance = pd.DataFrame({'feature': train_cols, 'importance': importance_scores_mean})\n",
    "    feature_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "    return models, oof_preds, rmse, feature_importance\n",
    "\n",
    "_,_,_,feature_importance = cv_pipeline_(train_feats, test_feats, lgbm_params, 'gbdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_important_feats = feature_importance[feature_importance['importance']==0]['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373                              tok_21\n",
       "372                              tok_20\n",
       "366                              tok_14\n",
       "3                       action_time_min\n",
       "367                              tok_15\n",
       "375                              tok_23\n",
       "368                              tok_16\n",
       "374                              tok_22\n",
       "369                              tok_17\n",
       "376                              tok_24\n",
       "377                              tok_25\n",
       "370                              tok_18\n",
       "371                              tok_19\n",
       "258                   up_event_12_count\n",
       "37     cursor_position_change1_quantile\n",
       "95          word_count_change3_quantile\n",
       "261                   up_event_15_count\n",
       "256                   up_event_10_count\n",
       "248                    up_event_2_count\n",
       "245                 down_event_15_count\n",
       "120         word_count_change5_quantile\n",
       "313                      low_sent_count\n",
       "112    cursor_position_change5_quantile\n",
       "92               word_count_change3_max\n",
       "351                     essay_words_min\n",
       "87     cursor_position_change3_quantile\n",
       "70          word_count_change2_quantile\n",
       "67               word_count_change2_max\n",
       "62     cursor_position_change2_quantile\n",
       "345                  essay_words_median\n",
       "45          word_count_change1_quantile\n",
       "42               word_count_change1_max\n",
       "378                              tok_26\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_important_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = train_feats.drop(columns=non_important_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id_max</th>\n",
       "      <th>up_time_max</th>\n",
       "      <th>action_time_max</th>\n",
       "      <th>action_time_mean</th>\n",
       "      <th>action_time_std</th>\n",
       "      <th>action_time_quantile</th>\n",
       "      <th>action_time_sem</th>\n",
       "      <th>action_time_sum</th>\n",
       "      <th>action_time_skew</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_5</th>\n",
       "      <th>tok_6</th>\n",
       "      <th>tok_7</th>\n",
       "      <th>tok_8</th>\n",
       "      <th>tok_9</th>\n",
       "      <th>tok_10</th>\n",
       "      <th>tok_11</th>\n",
       "      <th>tok_12</th>\n",
       "      <th>tok_13</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>-0.412654</td>\n",
       "      <td>0.297256</td>\n",
       "      <td>0.649055</td>\n",
       "      <td>0.708224</td>\n",
       "      <td>0.832047</td>\n",
       "      <td>0.602728</td>\n",
       "      <td>1.013468</td>\n",
       "      <td>-0.076341</td>\n",
       "      <td>0.510733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511288</td>\n",
       "      <td>-0.177651</td>\n",
       "      <td>0.660481</td>\n",
       "      <td>0.136887</td>\n",
       "      <td>-0.418590</td>\n",
       "      <td>-1.324192</td>\n",
       "      <td>-1.207260</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>-0.503278</td>\n",
       "      <td>0.139794</td>\n",
       "      <td>0.417773</td>\n",
       "      <td>0.550464</td>\n",
       "      <td>-0.113848</td>\n",
       "      <td>0.718889</td>\n",
       "      <td>0.094719</td>\n",
       "      <td>-0.232795</td>\n",
       "      <td>0.381025</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.269252</td>\n",
       "      <td>-1.538362</td>\n",
       "      <td>-2.665536</td>\n",
       "      <td>-0.739307</td>\n",
       "      <td>-0.664690</td>\n",
       "      <td>-1.324192</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>1.498801</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>0.653824</td>\n",
       "      <td>0.175623</td>\n",
       "      <td>0.891881</td>\n",
       "      <td>0.131551</td>\n",
       "      <td>0.656133</td>\n",
       "      <td>-0.099564</td>\n",
       "      <td>0.378357</td>\n",
       "      <td>0.682566</td>\n",
       "      <td>0.779134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762865</td>\n",
       "      <td>1.424855</td>\n",
       "      <td>1.668483</td>\n",
       "      <td>-0.252833</td>\n",
       "      <td>1.443518</td>\n",
       "      <td>-0.184352</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>-1.501620</td>\n",
       "      <td>-1.492521</td>\n",
       "      <td>-0.423774</td>\n",
       "      <td>0.923781</td>\n",
       "      <td>1.143871</td>\n",
       "      <td>0.525154</td>\n",
       "      <td>1.627881</td>\n",
       "      <td>-0.950668</td>\n",
       "      <td>-0.780595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167792</td>\n",
       "      <td>-2.473972</td>\n",
       "      <td>-2.207593</td>\n",
       "      <td>-1.041033</td>\n",
       "      <td>-1.365132</td>\n",
       "      <td>-0.184352</td>\n",
       "      <td>1.630053</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>-0.435189</td>\n",
       "      <td>-0.570642</td>\n",
       "      <td>-0.596296</td>\n",
       "      <td>1.003290</td>\n",
       "      <td>0.128298</td>\n",
       "      <td>1.257973</td>\n",
       "      <td>0.301895</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>-0.927886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913036</td>\n",
       "      <td>-0.919497</td>\n",
       "      <td>-1.561731</td>\n",
       "      <td>-0.252833</td>\n",
       "      <td>0.416839</td>\n",
       "      <td>0.426843</td>\n",
       "      <td>-1.207260</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>0.957832</td>\n",
       "      <td>0.183501</td>\n",
       "      <td>0.972627</td>\n",
       "      <td>0.278844</td>\n",
       "      <td>0.178234</td>\n",
       "      <td>0.641475</td>\n",
       "      <td>-0.336100</td>\n",
       "      <td>1.082875</td>\n",
       "      <td>1.308845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061690</td>\n",
       "      <td>-0.177651</td>\n",
       "      <td>-0.247576</td>\n",
       "      <td>-1.041033</td>\n",
       "      <td>-0.418590</td>\n",
       "      <td>1.584498</td>\n",
       "      <td>-1.207260</td>\n",
       "      <td>1.498801</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>-0.372480</td>\n",
       "      <td>0.216784</td>\n",
       "      <td>-0.021411</td>\n",
       "      <td>-0.713232</td>\n",
       "      <td>-1.214167</td>\n",
       "      <td>-0.652955</td>\n",
       "      <td>-0.976094</td>\n",
       "      <td>-0.724162</td>\n",
       "      <td>0.320733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.833085</td>\n",
       "      <td>-0.710961</td>\n",
       "      <td>-1.041033</td>\n",
       "      <td>-1.978150</td>\n",
       "      <td>0.426843</td>\n",
       "      <td>-0.038395</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>-0.013636</td>\n",
       "      <td>0.849759</td>\n",
       "      <td>-0.879874</td>\n",
       "      <td>-1.020584</td>\n",
       "      <td>0.174176</td>\n",
       "      <td>-1.052774</td>\n",
       "      <td>0.148528</td>\n",
       "      <td>-0.575474</td>\n",
       "      <td>-0.500749</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402204</td>\n",
       "      <td>-1.105123</td>\n",
       "      <td>-0.391202</td>\n",
       "      <td>-1.407041</td>\n",
       "      <td>-0.209522</td>\n",
       "      <td>-1.324192</td>\n",
       "      <td>-1.207260</td>\n",
       "      <td>1.498801</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>0.112226</td>\n",
       "      <td>-0.965479</td>\n",
       "      <td>0.183245</td>\n",
       "      <td>-0.401310</td>\n",
       "      <td>-0.151052</td>\n",
       "      <td>-0.454518</td>\n",
       "      <td>-0.254160</td>\n",
       "      <td>-0.131240</td>\n",
       "      <td>0.248387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>-0.177651</td>\n",
       "      <td>0.014538</td>\n",
       "      <td>0.750972</td>\n",
       "      <td>1.050112</td>\n",
       "      <td>1.385238</td>\n",
       "      <td>1.500539</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>2.624682</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>0.356486</td>\n",
       "      <td>1.222734</td>\n",
       "      <td>0.498642</td>\n",
       "      <td>-0.669358</td>\n",
       "      <td>0.181916</td>\n",
       "      <td>-0.494132</td>\n",
       "      <td>-0.024967</td>\n",
       "      <td>-0.048656</td>\n",
       "      <td>0.596487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.703483</td>\n",
       "      <td>-0.444888</td>\n",
       "      <td>-0.545041</td>\n",
       "      <td>0.136887</td>\n",
       "      <td>1.050112</td>\n",
       "      <td>0.426843</td>\n",
       "      <td>0.530949</td>\n",
       "      <td>-0.637806</td>\n",
       "      <td>-0.380843</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  event_id_max  up_time_max  action_time_max  action_time_mean  \\\n",
       "0     001519c8     -0.412654     0.297256         0.649055          0.708224   \n",
       "1     0022f953     -0.503278     0.139794         0.417773          0.550464   \n",
       "2     0042269b      0.653824     0.175623         0.891881          0.131551   \n",
       "3     0059420b     -1.501620    -1.492521        -0.423774          0.923781   \n",
       "4     0075873a     -0.435189    -0.570642        -0.596296          1.003290   \n",
       "...        ...           ...          ...              ...               ...   \n",
       "2466  ffb8c745      0.957832     0.183501         0.972627          0.278844   \n",
       "2467  ffbef7e5     -0.372480     0.216784        -0.021411         -0.713232   \n",
       "2468  ffccd6fd     -0.013636     0.849759        -0.879874         -1.020584   \n",
       "2469  ffec5b38      0.112226    -0.965479         0.183245         -0.401310   \n",
       "2470  fff05981      0.356486     1.222734         0.498642         -0.669358   \n",
       "\n",
       "      action_time_std  action_time_quantile  action_time_sem  action_time_sum  \\\n",
       "0            0.832047              0.602728         1.013468        -0.076341   \n",
       "1           -0.113848              0.718889         0.094719        -0.232795   \n",
       "2            0.656133             -0.099564         0.378357         0.682566   \n",
       "3            1.143871              0.525154         1.627881        -0.950668   \n",
       "4            0.128298              1.257973         0.301895         0.036145   \n",
       "...               ...                   ...              ...              ...   \n",
       "2466         0.178234              0.641475        -0.336100         1.082875   \n",
       "2467        -1.214167             -0.652955        -0.976094        -0.724162   \n",
       "2468         0.174176             -1.052774         0.148528        -0.575474   \n",
       "2469        -0.151052             -0.454518        -0.254160        -0.131240   \n",
       "2470         0.181916             -0.494132        -0.024967        -0.048656   \n",
       "\n",
       "      action_time_skew  ...     tok_5     tok_6     tok_7     tok_8     tok_9  \\\n",
       "0             0.510733  ... -0.511288 -0.177651  0.660481  0.136887 -0.418590   \n",
       "1             0.381025  ... -1.269252 -1.538362 -2.665536 -0.739307 -0.664690   \n",
       "2             0.779134  ...  0.762865  1.424855  1.668483 -0.252833  1.443518   \n",
       "3            -0.780595  ... -0.167792 -2.473972 -2.207593 -1.041033 -1.365132   \n",
       "4            -0.927886  ... -0.913036 -0.919497 -1.561731 -0.252833  0.416839   \n",
       "...                ...  ...       ...       ...       ...       ...       ...   \n",
       "2466          1.308845  ...  0.061690 -0.177651 -0.247576 -1.041033 -0.418590   \n",
       "2467          0.320733  ...  0.338687  0.833085 -0.710961 -1.041033 -1.978150   \n",
       "2468         -0.500749  ... -1.402204 -1.105123 -0.391202 -1.407041 -0.209522   \n",
       "2469          0.248387  ...  0.926199 -0.177651  0.014538  0.750972  1.050112   \n",
       "2470          0.596487  ... -0.703483 -0.444888 -0.545041  0.136887  1.050112   \n",
       "\n",
       "        tok_10    tok_11    tok_12    tok_13  score  \n",
       "0    -1.324192 -1.207260 -0.637806 -0.380843    3.5  \n",
       "1    -1.324192  0.891026  1.498801 -0.380843    3.5  \n",
       "2    -0.184352  0.891026 -0.637806 -0.380843    6.0  \n",
       "3    -0.184352  1.630053 -0.637806 -0.380843    2.0  \n",
       "4     0.426843 -1.207260 -0.637806 -0.380843    4.0  \n",
       "...        ...       ...       ...       ...    ...  \n",
       "2466  1.584498 -1.207260  1.498801 -0.380843    3.5  \n",
       "2467  0.426843 -0.038395 -0.637806 -0.380843    4.0  \n",
       "2468 -1.324192 -1.207260  1.498801 -0.380843    1.5  \n",
       "2469  1.385238  1.500539 -0.637806  2.624682    5.0  \n",
       "2470  0.426843  0.530949 -0.637806 -0.380843    4.0  \n",
       "\n",
       "[2471 rows x 348 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [feature, importance]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance[~feature_importance['importance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:27<00:00,  2.65s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 13157.46it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 12394.92it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2474/2474 [00:00<00:00, 12763.46it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 13661.11it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 13841.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:419: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:421: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n",
      "100%|██████████| 2474/2474 [00:04<00:00, 599.09it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:182: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  word_len_feats = diverse_stats(pd.Series(word_lengths), scope)\n",
      "100%|██████████| 2474/2474 [00:03<00:00, 618.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)\n",
    "logs = normalise_up_down_times(logs)\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "feats = preprocessor.make_feats(logs)\n",
    "nan_cols = feats.columns[feats[feats['id'].isin(train_ids)].isna().any()].tolist()\n",
    "feats = feats.drop(columns=nan_cols)\n",
    "\n",
    "essays = getEssays(logs)\n",
    "sent_feats = compute_sentence_aggregations(essays)\n",
    "par_feats = compute_paragraph_aggregations(essays)\n",
    "word_feats = create_word_length_features(essays, 'essay', 'id', 'essay_words')\n",
    "vector_feats = countvectorize_one_one(logs)\n",
    "\n",
    "feats = feats.merge(sent_feats, how='left', on='id')\n",
    "feats = feats.merge(par_feats, how='left', on='id')\n",
    "feats = feats.merge(word_feats, how='left', on='id')\n",
    "feats = pd.concat([feats, vector_feats], axis=1)\n",
    "\n",
    "feats.to_pickle('feat_store_hybrid/super_feats.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
