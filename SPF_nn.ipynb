{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import tensor\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_feats(feats, scaler=StandardScaler()):\n",
    "    feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    feats.fillna(-1e10, inplace=True)\n",
    "    feats_columns = feats.columns\n",
    "    feats.loc[:, feats_columns != 'id'] = scaler.fit_transform(feats.loc[:, feats_columns != 'id'])\n",
    "    return feats\n",
    "\n",
    "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "    x_train = data_x.iloc[train_idx]\n",
    "    y_train = data_y[train_idx]\n",
    "    x_valid = data_x.iloc[valid_idx]\n",
    "    y_valid = data_y[valid_idx]\n",
    "    return x_train, y_train, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, feat_dim, \n",
    "    layer_size=32, \n",
    "    dropout_rate=0.3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input = nn.Linear(feat_dim, layer_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(layer_size, layer_size)\n",
    "        self.out = nn.Linear(layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_feats = pd.read_pickle('feature_selection/train_feats.pkl')\n",
    "test_feats = pd.read_pickle('feature_selection/test_feats.pkl')\n",
    "\n",
    "train_feats.iloc[:,:-1] = preprocess_feats(train_feats.iloc[:,:-1])\n",
    "\n",
    "# x = tensor(train_feats.drop(['id','score'], axis=1).values.astype(np.float32))\n",
    "# y = tensor(train_feats['score'].values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def calculate_rmse(y, yhat):\n",
    "    return mean_squared_error(y, yhat, squared=False)\n",
    "\n",
    "def nn_pipeline(train, test, model_class, param, epochs, n_splits=10, iterations=5):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    test_preds = []\n",
    "    valid_preds = pd.DataFrame()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    targets = train_feats['score'].values.copy()\n",
    "    x = tensor(train_feats.drop(['id','score'], axis=1).values, dtype=torch.float).to(device)\n",
    "    y = tensor(targets, dtype=torch.float).to(device)\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        for i, (train_index, valid_index) in enumerate(skf.split(x, targets.astype(str))):\n",
    "            # Splitting data\n",
    "            x_train, y_train = x[train_index], y[train_index]\n",
    "            x_valid, y_valid = x[valid_index], y[valid_index]\n",
    "            \n",
    "            # Model setup\n",
    "            model = model_class(**param).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                outputs = outputs.squeeze()  \n",
    "                loss = criterion(outputs, y_train)\n",
    "                rmse_loss = torch.sqrt(loss)\n",
    "                rmse_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log training metrics\n",
    "                wandb.log({'train_loss': loss.item(), 'epoch': epoch})\n",
    "\n",
    "            # Validation predictions\n",
    "            model.eval()\n",
    "            valid_predictions = []\n",
    "            with torch.no_grad():\n",
    "                outputs = model(x_valid).squeeze()\n",
    "                valid_predictions.extend(outputs.tolist())\n",
    "\n",
    "            # Log validation metrics\n",
    "            tmp_rmse = np.sqrt(mean_squared_error(y_valid.cpu().numpy(), valid_predictions))\n",
    "            wandb.log({'valid_rmse': tmp_rmse, 'iteration': i + 1})\n",
    "\n",
    "            tmp_df = pd.DataFrame({'id': valid_index, 'score': y_valid.cpu().numpy(), 'preds': valid_predictions})\n",
    "            tmp_df['iteration'] = i + 1\n",
    "            valid_preds = pd.concat([valid_preds, tmp_df])\n",
    "\n",
    "    final_rmse = np.sqrt(mean_squared_error(valid_preds['score'], valid_preds['preds']))\n",
    "    cv_rmse = valid_preds.groupby('iteration').apply(lambda g: np.sqrt(mean_squared_error(g['score'], g['preds'])))\n",
    "    wandb.finish()\n",
    "    return valid_preds, final_rmse, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(layer_size, dropout_rate, epochs):\n",
    "\n",
    "    model = MLPModel(feat_dim=train_feats.shape[1]-2, layer_size=layer_size, dropout_rate=dropout_rate).to(device)\n",
    "    param = {'feat_dim': train_feats.shape[1]-2, 'layer_size': layer_size, 'dropout_rate': dropout_rate}\n",
    "\n",
    "    valid_preds, final_rmse, trained_model = nn_pipeline(\n",
    "        train=train_feats,  \n",
    "        test=test_feats,\n",
    "        model_class=MLPModel,\n",
    "        param=param,\n",
    "        epochs=epochs,\n",
    "        n_splits=10,  \n",
    "        iterations=5\n",
    "    )\n",
    "\n",
    "    print(f\"Layer Size: {layer_size}, Dropout: {dropout_rate}, Epochs: {epochs}, Final RMSE: {final_rmse}\")\n",
    "    return valid_preds, final_rmse, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"linking-quality-writing\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/Projects/Kaggle/linking-writing/wandb/run-20231223_004539-ywevniyx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/allende-rev/linking-quality-writing/runs/ywevniyx' target=\"_blank\">wandering-plant-11</a></strong> to <a href='https://wandb.ai/allende-rev/linking-quality-writing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/allende-rev/linking-quality-writing' target=\"_blank\">https://wandb.ai/allende-rev/linking-quality-writing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/allende-rev/linking-quality-writing/runs/ywevniyx' target=\"_blank\">https://wandb.ai/allende-rev/linking-quality-writing/runs/ywevniyx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ywevniyx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-plant-11</strong> at: <a href='https://wandb.ai/allende-rev/linking-quality-writing/runs/ywevniyx' target=\"_blank\">https://wandb.ai/allende-rev/linking-quality-writing/runs/ywevniyx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231223_004539-ywevniyx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ywevniyx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/Projects/Kaggle/linking-writing/wandb/run-20231223_004542-n7xcyubb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/allende-rev/linking-quality-writing/runs/n7xcyubb' target=\"_blank\">hardy-pyramid-12</a></strong> to <a href='https://wandb.ai/allende-rev/linking-quality-writing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/allende-rev/linking-quality-writing' target=\"_blank\">https://wandb.ai/allende-rev/linking-quality-writing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/allende-rev/linking-quality-writing/runs/n7xcyubb' target=\"_blank\">https://wandb.ai/allende-rev/linking-quality-writing/runs/n7xcyubb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▅▃▅▇▂▄▆█▂▅▇▁▃▅▇▂▄▆▁▃▅▇▂▄▆█▃▅▇▂▄▆█▂▅▇▁▃▅▇</td></tr><tr><td>iteration</td><td>▁▂▃▃▅▆▆▇▁▂▃▃▅▆▆▇▁▂▃▃▅▆▆▇▁▂▃▃▅▆▆▇▁▂▃▃▅▆▆█</td></tr><tr><td>train_loss</td><td>▂▅▃▂▆▃▂▁▄▃▁█▅▂▁▆▃▂▇▃▂▁▇▃▃▂▅▂▁▆▄▂▁▆▄▁█▄▂▁</td></tr><tr><td>valid_rmse</td><td>▄▃▃▄▃▂▄▄▂▆▃█▃▂▄▃▆▄▅▃▂▅▅▄█▆▅▂▄▃▁▃▅▄▄▇▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iteration</td><td>10</td></tr><tr><td>train_loss</td><td>2.28376</td></tr><tr><td>valid_rmse</td><td>1.38651</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-pyramid-12</strong> at: <a href='https://wandb.ai/allende-rev/linking-quality-writing/runs/n7xcyubb' target=\"_blank\">https://wandb.ai/allende-rev/linking-quality-writing/runs/n7xcyubb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231223_004542-n7xcyubb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Size: 24, Dropout: 0.1, Epochs: 100, Final RMSE: 1.461810641201038\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/SPF_nn.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_nn.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Run the experiment and log the results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_nn.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m valid_preds, final_rmse, trained_model \u001b[39m=\u001b[39m run_experiment(layer_size, dropout_rate, epochs)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_nn.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m wandb\u001b[39m.\u001b[39;49mlog({\u001b[39m'\u001b[39;49m\u001b[39mfinal_rmse\u001b[39;49m\u001b[39m'\u001b[39;49m: final_rmse})\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_nn.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Finish the wandb session\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_nn.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m run\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreinit_wrapper\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m wandb\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou must call wandb.init() before \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"linking-quality-writing\"\n",
    "run = wandb.init(project='linking-quality-writing')\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "layer_sizes = [24, 32, 48, 64, 128]\n",
    "dropout_rates = list(np.arange(0.1, 0.3, 0.05))\n",
    "epoch_settings = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "# Running the experiments\n",
    "for layer_size in layer_sizes:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for epochs in epoch_settings:\n",
    "            # Start a new wandb session for each set of hyperparameters\n",
    "            run = wandb.init(project='linking-quality-writing', reinit=True)\n",
    "            \n",
    "            # Update wandb configuration\n",
    "            wandb.config.update({\n",
    "                'epochs': epochs,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'layer_size': layer_size\n",
    "            })\n",
    "\n",
    "            # Run the experiment and log the results\n",
    "            valid_preds, final_rmse, trained_model = run_experiment(layer_size, dropout_rate, epochs)\n",
    "            wandb.log({'final_rmse': final_rmse})\n",
    "            \n",
    "            # Finish the wandb session\n",
    "            run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.6505302674845131\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model = MLPModel(train_feats.shape[1]).to(device)\n",
    "\n",
    "param = {'feat_dim': train_feats.drop(['id','score'], axis=1).shape[1]}\n",
    "# Using the nn_pipeline function\n",
    "valid_preds, final_rmse, trained_model = nn_pipeline(\n",
    "    train=train_feats,  \n",
    "    test=test_feats,\n",
    "    model_class=MLPModel,\n",
    "    param=param,\n",
    "    n_splits=10,  \n",
    "    iterations=5  \n",
    ")\n",
    "\n",
    "print(f\"Final RMSE: {final_rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
