{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import tensor\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_feats(feats, scaler=StandardScaler()):\n",
    "    feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    feats.fillna(-1e10, inplace=True)\n",
    "    feats_columns = feats.columns\n",
    "    feats.loc[:, feats_columns != 'id'] = scaler.fit_transform(feats.loc[:, feats_columns != 'id'])\n",
    "    return feats\n",
    "\n",
    "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "    x_train = data_x.iloc[train_idx]\n",
    "    y_train = data_y[train_idx]\n",
    "    x_valid = data_x.iloc[valid_idx]\n",
    "    y_valid = data_y[valid_idx]\n",
    "    return x_train, y_train, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, feat_dim, \n",
    "    layer_size=32, \n",
    "    dropout_rate=0.3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input = nn.Linear(feat_dim, layer_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(layer_size, layer_size)\n",
    "        self.out = nn.Linear(layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_feats = pd.read_pickle('feature_selection/train_feats.pkl')\n",
    "test_feats = pd.read_pickle('feature_selection/test_feats.pkl')\n",
    "\n",
    "train_feats.iloc[:,:-1] = preprocess_feats(train_feats.iloc[:,:-1])\n",
    "\n",
    "# x = tensor(train_feats.drop(['id','score'], axis=1).values.astype(np.float32))\n",
    "# y = tensor(train_feats['score'].values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def calculate_rmse(y, yhat):\n",
    "    return mean_squared_error(y, yhat, squared=False)\n",
    "\n",
    "def nn_pipeline(train, test, model_class, param, n_splits=10, iterations=5, batch_size=64):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    test_preds = []\n",
    "    valid_preds = pd.DataFrame()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    targets = train_feats['score'].values.copy()\n",
    "    x = tensor(train_feats.drop(['id','score'], axis=1).values, dtype=torch.float).to(device)\n",
    "    y = tensor(targets, dtype=torch.float).to(device)\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        for i, (train_index, valid_index) in enumerate(skf.split(x, targets.astype(str))):\n",
    "            # Splitting data\n",
    "            x_train, y_train = x[train_index], y[train_index]\n",
    "            x_valid, y_valid = x[valid_index], y[valid_index]\n",
    "            \n",
    "            # Model setup\n",
    "            model = model_class(**param).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train)\n",
    "                outputs = outputs.squeeze()  \n",
    "                loss = criterion(outputs, y_train)\n",
    "                rmse_loss = torch.sqrt(loss)\n",
    "                rmse_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log training metrics\n",
    "                wandb.log({'train_loss': loss.item(), 'epoch': epoch})\n",
    "\n",
    "            # Validation predictions\n",
    "            model.eval()\n",
    "            valid_predictions = []\n",
    "            with torch.no_grad():\n",
    "                outputs = model(x_valid).squeeze()\n",
    "                valid_predictions.extend(outputs.tolist())\n",
    "\n",
    "            # Log validation metrics\n",
    "            tmp_rmse = np.sqrt(mean_squared_error(y_valid.cpu().numpy(), valid_predictions))\n",
    "            wandb.log({'valid_rmse': tmp_rmse, 'iteration': i + 1})\n",
    "\n",
    "            tmp_df = pd.DataFrame({'id': valid_index, 'score': y_valid.cpu().numpy(), 'preds': valid_predictions})\n",
    "            tmp_df['iteration'] = i + 1\n",
    "            valid_preds = pd.concat([valid_preds, tmp_df])\n",
    "\n",
    "    final_rmse = np.sqrt(mean_squared_error(valid_preds['score'], valid_preds['preds']))\n",
    "    cv_rmse = valid_preds.groupby('iteration').apply(lambda g: np.sqrt(mean_squared_error(g['score'], g['preds'])))\n",
    "    wandb.finish()\n",
    "    return valid_preds, final_rmse, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(layer_size, dropout_rate, epochs):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project='linking-quality-writing', entity='allende-rev')\n",
    "    wandb.config.update({\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'n_splits': n_splits,\n",
    "        'iterations': iterations,\n",
    "    })\n",
    "\n",
    "    model = MLPModel(feat_dim=train_feats.shape[1], layer_size=layer_size, dropout_rate=dropout_rate).to(device)\n",
    "    param = {'feat_dim': train_feats.shape[1], 'layer_size': layer_size, 'dropout_rate': dropout_rate}\n",
    "\n",
    "    valid_preds, final_rmse, trained_model = nn_pipeline(\n",
    "        train=train_feats,  \n",
    "        test=test_feats,\n",
    "        model_class=MLPModel,\n",
    "        param=param,\n",
    "        n_splits=10,  \n",
    "        iterations=5,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    wandb.log({'final_rmse': final_rmse})\n",
    "    print(f\"Layer Size: {layer_size}, Dropout: {dropout_rate}, Epochs: {epochs}, Final RMSE: {final_rmse}\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Define the hyperparameters you want to experiment with\n",
    "layer_sizes = [32, 64, 128]\n",
    "dropout_rates = [0.1, 0.3, 0.5]\n",
    "epoch_settings = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "# Running the experiments\n",
    "for layer_size in layer_sizes:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for epochs in epoch_settings:\n",
    "            run_experiment(layer_size, dropout_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.6505302674845131\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model = MLPModel(train_feats.shape[1]).to(device)\n",
    "\n",
    "param = {'feat_dim': train_feats.drop(['id','score'], axis=1).shape[1]}\n",
    "# Using the nn_pipeline function\n",
    "valid_preds, final_rmse, trained_model = nn_pipeline(\n",
    "    train=train_feats,  \n",
    "    test=test_feats,\n",
    "    model_class=MLPModel,\n",
    "    param=param,\n",
    "    n_splits=10,  \n",
    "    iterations=5  \n",
    ")\n",
    "\n",
    "print(f\"Final RMSE: {final_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mallende-rev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/Projects/Kaggle/linking-writing/wandb/run-20231223_001341-kbpd14c4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/allende-rev/linking-writing/runs/kbpd14c4' target=\"_blank\">vital-sky-1</a></strong> to <a href='https://wandb.ai/allende-rev/linking-writing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/allende-rev/linking-writing' target=\"_blank\">https://wandb.ai/allende-rev/linking-writing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/allende-rev/linking-writing/runs/kbpd14c4' target=\"_blank\">https://wandb.ai/allende-rev/linking-writing/runs/kbpd14c4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/allende-rev/linking-writing/runs/kbpd14c4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fec2813aec0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
