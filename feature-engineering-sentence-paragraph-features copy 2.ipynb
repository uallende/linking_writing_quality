{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24727126",
   "metadata": {
    "papermill": {
     "duration": 0.009441,
     "end_time": "2023-11-20T00:49:21.872389",
     "exception": false,
     "start_time": "2023-11-20T00:49:21.862948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction of sentence and paragraph features\n",
    "\n",
    "- This notebook extends previous work shared in the context of this competition: \n",
    "    - [\"{ENTER}ing the TimeSeries {SPACE} Sec 3 + New Aggs\"](https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs) (shared by [Abdullah Meda](https://www.kaggle.com/abdullahmeda))\n",
    "    - (Not used in newest version of the notebook) [\"Essay Contructor\"](https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor) (shared by [Kawaii Coder UwU](https://www.kaggle.com/kawaiicoderuwu))\n",
    "    - [\"Fast essay constructor\"](https://www.kaggle.com/code/yuriao/fast-essay-constructor) (shared by [yuri-ao](https://www.kaggle.com/yuriao))\n",
    "- If you find this notebook helpful and decide to upvote it, please also upvote the notebooks mentioned above and the notebooks Abdullah referrences in his notebook:\n",
    "    - https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline\n",
    "    - https://www.kaggle.com/code/abhranta/lgbm-finetuning-with-optuna\n",
    "    - https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline\n",
    "    - https://www.kaggle.com/code/gopidurgaprasad/youtube-video-writing-quality-lgbm\n",
    "    - https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "- **Contribution:** \n",
    "    - I used the \"getEssays\" function implemented in the \"Essay Constructor\" notebook to construct the essays, split them into sentences and paragraphs, counted characters and words in the sentences and paragraphs, and computed aggregations (e.g., number of sentences, number of paragraphs, mean sentence lengths). \n",
    "    - The feature importance plot reveals that some of these features are indeed \"important.\" With the additional features, the notebook's scores improve slightly (e.g., 0.595 to 0.588 Public LB Score).\n",
    "- **Changelog (only listing versions with \"relevant\" changes):**\n",
    "    - Version 16: Used \"fast essay constructor\" code to construct the essays\n",
    "    - Version 09: Consideration of words per sentence and paragraph (in addition to characters per sentence and paragraph) (Public LB Score: 0.588)\n",
    "    - Version 08: Additional aggregations (e.g., quantiles, skew) (Public LB Score: 0.592)\n",
    "    - Version 05: Additional sentence and paragraph features (Public LB Score: 0.593)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538da5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:21.892584Z",
     "iopub.status.busy": "2023-11-20T00:49:21.891796Z",
     "iopub.status.idle": "2023-11-20T00:49:26.951869Z",
     "shell.execute_reply": "2023-11-20T00:49:26.950244Z"
    },
    "papermill": {
     "duration": 5.074858,
     "end_time": "2023-11-20T00:49:26.956234",
     "exception": false,
     "start_time": "2023-11-20T00:49:21.881376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf840d5",
   "metadata": {
    "papermill": {
     "duration": 0.008139,
     "end_time": "2023-11-20T00:49:26.973094",
     "exception": false,
     "start_time": "2023-11-20T00:49:26.964955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f8a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:26.992040Z",
     "iopub.status.busy": "2023-11-20T00:49:26.991563Z",
     "iopub.status.idle": "2023-11-20T00:49:45.940174Z",
     "shell.execute_reply": "2023-11-20T00:49:45.938404Z"
    },
    "papermill": {
     "duration": 18.962319,
     "end_time": "2023-11-20T00:49:45.943681",
     "exception": false,
     "start_time": "2023-11-20T00:49:26.981362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c80b6",
   "metadata": {
    "papermill": {
     "duration": 0.008265,
     "end_time": "2023-11-20T00:49:46.121968",
     "exception": false,
     "start_time": "2023-11-20T00:49:46.113703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ba0a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:46.141111Z",
     "iopub.status.busy": "2023-11-20T00:49:46.140605Z",
     "iopub.status.idle": "2023-11-20T00:49:46.161502Z",
     "shell.execute_reply": "2023-11-20T00:49:46.160166Z"
    },
    "papermill": {
     "duration": 0.034128,
     "end_time": "2023-11-20T00:49:46.164316",
     "exception": false,
     "start_time": "2023-11-20T00:49:46.130188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to construct essays copied from here (small adjustments): https://www.kaggle.com/code/yuriao/fast-essay-constructor\n",
    "\n",
    "def processingInputs(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        # Input[0] = activity\n",
    "        # Input[1] = cursor_position\n",
    "        # Input[2] = text_change\n",
    "        # Input[3] = id\n",
    "        # If activity = Replace\n",
    "        if Input[0] == 'Replace':\n",
    "            # splits text_change at ' => '\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "\n",
    "        # If activity = Paste    \n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "\n",
    "        # If activity = Remove/Cut\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "\n",
    "        # If activity = Move...\n",
    "        if \"M\" in Input[0]:\n",
    "            # Gets rid of the \"Move from to\" text\n",
    "            croppedTxt = Input[0][10:]              \n",
    "            # Splits cropped text by ' To '\n",
    "            splitTxt = croppedTxt.split(' To ')              \n",
    "            # Splits split text again by ', ' for each item\n",
    "            valueArr = [item.split(', ') for item in splitTxt]              \n",
    "            # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            # Skip if someone manages to activiate this by moving to same place\n",
    "            if moveData[0] != moveData[2]:\n",
    "                # Check if they move text forward in essay (they are different)\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n",
    "                    essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n",
    "                    essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue                \n",
    "\n",
    "        # If activity = input\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def getEssays(df):\n",
    "    # Copy required columns\n",
    "    textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n",
    "    # Get rid of text inputs that make no change\n",
    "    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n",
    "    # construct essay, fast \n",
    "    tqdm.pandas()\n",
    "    essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(\n",
    "        lambda x: processingInputs(x))      \n",
    "    # to dataframe\n",
    "    essayFrame=essay.to_frame().reset_index()\n",
    "    essayFrame.columns=['id','essay']\n",
    "    # Returns the essay series\n",
    "    return essayFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af2ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:46.183385Z",
     "iopub.status.busy": "2023-11-20T00:49:46.182860Z",
     "iopub.status.idle": "2023-11-20T00:49:46.189365Z",
     "shell.execute_reply": "2023-11-20T00:49:46.188022Z"
    },
    "papermill": {
     "duration": 0.018996,
     "end_time": "2023-11-20T00:49:46.191723",
     "exception": false,
     "start_time": "2023-11-20T00:49:46.172727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9c372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:46.210832Z",
     "iopub.status.busy": "2023-11-20T00:49:46.210270Z",
     "iopub.status.idle": "2023-11-20T00:49:46.229442Z",
     "shell.execute_reply": "2023-11-20T00:49:46.228344Z"
    },
    "papermill": {
     "duration": 0.0326,
     "end_time": "2023-11-20T00:49:46.232692",
     "exception": false,
     "start_time": "2023-11-20T00:49:46.200092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "\n",
    "def split_essays_into_sentences(df):\n",
    "    essay_df = df\n",
    "    #essay_df['id'] = essay_df.index\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(df):\n",
    "    sent_agg_df = pd.concat(\n",
    "        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    )\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def split_essays_into_paragraphs(df):\n",
    "    essay_df = df\n",
    "    #essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(df):\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfd872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:46.254679Z",
     "iopub.status.busy": "2023-11-20T00:49:46.253494Z",
     "iopub.status.idle": "2023-11-20T00:49:54.834234Z",
     "shell.execute_reply": "2023-11-20T00:49:54.832866Z"
    },
    "papermill": {
     "duration": 8.593849,
     "end_time": "2023-11-20T00:49:54.836955",
     "exception": false,
     "start_time": "2023-11-20T00:49:46.243106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentence features for train dataset\n",
    "train_essays = getEssays(train_logs)\n",
    "train_sent_df = split_essays_into_sentences(train_essays)\n",
    "train_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n",
    "plt.figure(figsize=(15, 1.5))\n",
    "plt.boxplot(x=train_sent_df.sent_len, vert=False, labels=['Sentence length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d861819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:49:54.857252Z",
     "iopub.status.busy": "2023-11-20T00:49:54.856742Z",
     "iopub.status.idle": "2023-11-20T00:50:02.887260Z",
     "shell.execute_reply": "2023-11-20T00:50:02.886276Z"
    },
    "papermill": {
     "duration": 8.044176,
     "end_time": "2023-11-20T00:50:02.890086",
     "exception": false,
     "start_time": "2023-11-20T00:49:54.845910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paragraph features for train dataset\n",
    "train_paragraph_df = split_essays_into_paragraphs(train_essays)\n",
    "train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n",
    "plt.figure(figsize=(15, 1.5))\n",
    "plt.boxplot(x=train_paragraph_df.paragraph_len, vert=False, labels=['Paragraph length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb21dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:50:02.910270Z",
     "iopub.status.busy": "2023-11-20T00:50:02.909763Z",
     "iopub.status.idle": "2023-11-20T00:50:02.999110Z",
     "shell.execute_reply": "2023-11-20T00:50:02.997583Z"
    },
    "papermill": {
     "duration": 0.102931,
     "end_time": "2023-11-20T00:50:03.002036",
     "exception": false,
     "start_time": "2023-11-20T00:50:02.899105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features for test dataset\n",
    "test_essays = getEssays(test_logs)\n",
    "test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n",
    "test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05b109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:50:03.023874Z",
     "iopub.status.busy": "2023-11-20T00:50:03.023332Z",
     "iopub.status.idle": "2023-11-20T00:50:03.086170Z",
     "shell.execute_reply": "2023-11-20T00:50:03.084687Z"
    },
    "papermill": {
     "duration": 0.078993,
     "end_time": "2023-11-20T00:50:03.090631",
     "exception": false,
     "start_time": "2023-11-20T00:50:03.011638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following code comes almost Abdullah's notebook: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "# Abdullah's code is based on work shared in previous notebooks (e.g., https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        \n",
    "        self.idf = defaultdict(float)\n",
    "    \n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "\n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def make_feats(self, df):\n",
    "        \n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('up_time', ['max']),\n",
    "            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n",
    "            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering ratios data\")\n",
    "        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
    "        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467dded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:50:03.112947Z",
     "iopub.status.busy": "2023-11-20T00:50:03.112464Z",
     "iopub.status.idle": "2023-11-20T00:56:27.754303Z",
     "shell.execute_reply": "2023-11-20T00:56:27.752851Z"
    },
    "papermill": {
     "duration": 384.657396,
     "end_time": "2023-11-20T00:56:27.758357",
     "exception": false,
     "start_time": "2023-11-20T00:50:03.100961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n",
    "train_feats = train_feats.drop(columns=nan_cols)\n",
    "test_feats = test_feats.drop(columns=nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32d31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:56:27.973157Z",
     "iopub.status.busy": "2023-11-20T00:56:27.972709Z",
     "iopub.status.idle": "2023-11-20T00:56:36.238988Z",
     "shell.execute_reply": "2023-11-20T00:56:36.237370Z"
    },
    "papermill": {
     "duration": 8.377328,
     "end_time": "2023-11-20T00:56:36.242491",
     "exception": false,
     "start_time": "2023-11-20T00:56:27.865163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for additional aggregations comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "\n",
    "train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n",
    "train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n",
    "train_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n",
    "test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n",
    "test_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a32ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:56:36.493982Z",
     "iopub.status.busy": "2023-11-20T00:56:36.493419Z",
     "iopub.status.idle": "2023-11-20T00:56:48.604162Z",
     "shell.execute_reply": "2023-11-20T00:56:48.602830Z"
    },
    "papermill": {
     "duration": 12.241231,
     "end_time": "2023-11-20T00:56:48.607848",
     "exception": false,
     "start_time": "2023-11-20T00:56:36.366617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for creating these features comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "# Idea is based on features introduced in Section 3 of this research paper: https://files.eric.ed.gov/fulltext/ED592674.pdf\n",
    "\n",
    "data = []\n",
    "\n",
    "for logs in [train_logs, test_logs]:\n",
    "    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "    group = logs.groupby('id')['time_diff']\n",
    "    largest_lantency = group.max()\n",
    "    smallest_lantency = group.min()\n",
    "    median_lantency = group.median()\n",
    "    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "\n",
    "    data.append(pd.DataFrame({\n",
    "        'id': logs['id'].unique(),\n",
    "        'largest_lantency': largest_lantency,\n",
    "        'smallest_lantency': smallest_lantency,\n",
    "        'median_lantency': median_lantency,\n",
    "        'initial_pause': initial_pause,\n",
    "        'pauses_half_sec': pauses_half_sec,\n",
    "        'pauses_1_sec': pauses_1_sec,\n",
    "        'pauses_1_half_sec': pauses_1_half_sec,\n",
    "        'pauses_2_sec': pauses_2_sec,\n",
    "        'pauses_3_sec': pauses_3_sec,\n",
    "    }).reset_index(drop=True))\n",
    "\n",
    "train_eD592674, test_eD592674 = data\n",
    "\n",
    "train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95565208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:56:48.816134Z",
     "iopub.status.busy": "2023-11-20T00:56:48.815662Z",
     "iopub.status.idle": "2023-11-20T00:56:48.858334Z",
     "shell.execute_reply": "2023-11-20T00:56:48.857043Z"
    },
    "papermill": {
     "duration": 0.149631,
     "end_time": "2023-11-20T00:56:48.861342",
     "exception": false,
     "start_time": "2023-11-20T00:56:48.711711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding the additional features to the original feature set\n",
    "\n",
    "train_feats = train_feats.merge(train_sent_agg_df, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_paragraph_agg_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_sent_agg_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_paragraph_agg_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24887dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:56:49.066574Z",
     "iopub.status.busy": "2023-11-20T00:56:49.066054Z",
     "iopub.status.idle": "2023-11-20T00:56:49.072711Z",
     "shell.execute_reply": "2023-11-20T00:56:49.071442Z"
    },
    "papermill": {
     "duration": 0.112452,
     "end_time": "2023-11-20T00:56:49.074993",
     "exception": false,
     "start_time": "2023-11-20T00:56:48.962541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = ['score']\n",
    "drop_cols = ['id']\n",
    "train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73ae1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T00:56:49.282325Z",
     "iopub.status.busy": "2023-11-20T00:56:49.281825Z",
     "iopub.status.idle": "2023-11-20T01:04:06.960736Z",
     "shell.execute_reply": "2023-11-20T01:04:06.959184Z"
    },
    "papermill": {
     "duration": 437.787746,
     "end_time": "2023-11-20T01:04:06.963796",
     "exception": false,
     "start_time": "2023-11-20T00:56:49.176050",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "\n",
    "models_dict = {}\n",
    "scores = []\n",
    "\n",
    "test_predict_list = []\n",
    "best_params = {'reg_alpha': 0.007678095440286993, \n",
    "               'reg_lambda': 0.34230534302168353, \n",
    "               'colsample_bytree': 0.627061253588415, \n",
    "               'subsample': 0.854942238828458, \n",
    "               'learning_rate': 0.038697981947473245, \n",
    "               'num_leaves': 22, \n",
    "               'max_depth': 37, \n",
    "               'min_child_samples': 18}\n",
    "\n",
    "for i in range(5): \n",
    "    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n",
    "    oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "    X_test = test_feats[train_cols]\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "        \n",
    "        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            'random_state': 42,\n",
    "            \"n_estimators\" : 12001,\n",
    "            \"verbosity\": -1,\n",
    "            **best_params\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n",
    "        verbose_callback = lgb.log_evaluation(100)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n",
    "                  callbacks=[early_stopping_callback, verbose_callback],\n",
    "        )\n",
    "        valid_predict = model.predict(X_valid)\n",
    "        oof_valid_preds[valid_idx] = valid_predict\n",
    "        test_predict = model.predict(X_test)\n",
    "        test_predict_list.append(test_predict)\n",
    "        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "        models_dict[f'{fold}_{i}'] = model\n",
    "\n",
    "    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n",
    "    scores.append(oof_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d24e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T01:04:07.199646Z",
     "iopub.status.busy": "2023-11-20T01:04:07.198268Z",
     "iopub.status.idle": "2023-11-20T01:04:16.830265Z",
     "shell.execute_reply": "2023-11-20T01:04:16.828797Z"
    },
    "papermill": {
     "duration": 9.771255,
     "end_time": "2023-11-20T01:04:16.852040",
     "exception": false,
     "start_time": "2023-11-20T01:04:07.080785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_values = np.asarray([model.feature_importances_ for model in models_dict.values()]).mean(axis=0)\n",
    "feature_importance_df = pd.DataFrame({'name': train_cols, 'importance': feature_importances_values})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "feature_importance_df['group'] = feature_importance_df['name'].apply(\n",
    "    lambda x: 'sentence features' if 'sent' in x else 'paragraph features' if 'paragraph' in x else 'other features')\n",
    "\n",
    "plt.figure(figsize=(10, 120))\n",
    "ax = sns.barplot(data=feature_importance_df, x='importance', y='name', hue='group', dodge=False)\n",
    "ax.set_title(f\"Mean feature importances\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb3733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T01:04:17.136548Z",
     "iopub.status.busy": "2023-11-20T01:04:17.135865Z",
     "iopub.status.idle": "2023-11-20T01:04:17.155930Z",
     "shell.execute_reply": "2023-11-20T01:04:17.154539Z"
    },
    "papermill": {
     "duration": 0.166789,
     "end_time": "2023-11-20T01:04:17.158647",
     "exception": false,
     "start_time": "2023-11-20T01:04:16.991858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_feats['score'] = np.mean(test_predict_list, axis=0)\n",
    "test_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)\n",
    "test_feats[['id', 'score']].head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    },
    {
     "datasetId": 3949123,
     "sourceId": 6973319,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 902.637778,
   "end_time": "2023-11-20T01:04:19.907889",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-20T00:49:17.270111",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
