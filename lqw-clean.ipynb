{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-18T16:20:56.671429Z","iopub.status.busy":"2023-12-18T16:20:56.671017Z","iopub.status.idle":"2023-12-18T16:20:58.568632Z","shell.execute_reply":"2023-12-18T16:20:58.567768Z","shell.execute_reply.started":"2023-12-18T16:20:56.671393Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import polars as pl\n","from m3_model_params import lgb_params_1\n","from m4_feats_polars import *\n","from m5_sb_models import *\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T16:20:58.587076Z","iopub.status.busy":"2023-12-18T16:20:58.586426Z","iopub.status.idle":"2023-12-18T16:20:58.601881Z","shell.execute_reply":"2023-12-18T16:20:58.600973Z","shell.execute_reply.started":"2023-12-18T16:20:58.587045Z"},"trusted":true},"outputs":[],"source":["# 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\n","data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n","train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n","test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n","train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')\n","\n","# train_logs, test_logs = amend_event_id_order(train_logs, test_logs)     worsens"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# between paragraph pauses ?\n","# backspace pauses\n","# edit pauses"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def add_word_pauses(train_logs, test_logs):\n","    print(\"< added words pauses basic\")    \n","    feats = []\n","\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","\n","    for data in [tr_logs, ts_logs]:\n","        logs = data.clone()\n","        logs = logs.select(pl.col(['id','event_id','word_count','down_time','up_time','action_time']))\n","        logs = logs.with_columns(pl.col('word_count')\n","                    .diff()\n","                    .over('id')\n","                    .fill_null(1)\n","                    .alias('word_diff'))\n","\n","        logs = logs.with_columns(pl.col('down_time')\n","                    .diff()\n","                    .over('id','word_count')\n","                    .fill_null(0)\n","                    .alias('down_time_diff')) \n","\n","        word_pause = logs.filter(pl.col('word_diff')>0)\n","        word_pause = word_pause.group_by(['id']).agg(\n","                add_words_pause_count = pl.col('down_time_diff').count(),\n","                add_words_pause_mean = pl.col('down_time_diff').mean(),\n","                add_words_pause_sum = pl.col('down_time_diff').sum(),\n","                add_words_pause_std = pl.col('down_time_diff').std(),\n","                add_words_pause_median = pl.col('down_time_diff').median(),\n","                add_words_pause_max = pl.col('down_time_diff').max(),\n","                add_words_pause_q1 = pl.col('down_time_diff').quantile(0.25),\n","                add_words_pause_q3 = pl.col('down_time_diff').quantile(0.75),\n","                add_words_pause_kurt = pl.col('down_time_diff').kurtosis(),\n","                add_words_pause_skew = pl.col('down_time_diff').skew(),\n","        )\n","        feats.append(word_pause)\n","    return feats[0], feats[1]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def word_wait_shift(train_logs, test_logs,shift):\n","    print('< word_wait_shift >')\n","    feats = []\n","    for data in [train_logs,test_logs]:\n","        logs = data.clone()\n","        logs = logs.group_by('id','word_count').agg(\n","            word_start_time = pl.col('down_time').min()).sort('id','word_count')\n","\n","        shift = 10\n","        logs = logs.with_columns(pl.col('word_start_time').shift(shift).over('id').alias(f'shifted'))\n","        logs = logs.with_columns((pl.col('word_start_time') - pl.col('shifted')).alias(f'word_time_diff_{shift}'))\n","\n","        words_shifted = logs.group_by('id').agg(\n","                        pl.col(f'word_time_diff_{shift}').count().name.suffix('count'),\n","                        pl.col(f'word_time_diff_{shift}').mean().name.suffix('mean'),\n","                        pl.col(f'word_time_diff_{shift}').sum().name.suffix('sum'),\n","                        pl.col(f'word_time_diff_{shift}').std().name.suffix('std'),\n","                        pl.col(f'word_time_diff_{shift}').median().name.suffix('median'),\n","                        pl.col(f'word_time_diff_{shift}').max().name.suffix('max'),\n","                        pl.col(f'word_time_diff_{shift}').quantile(0.25).name.suffix('quantile_25'),\n","                        pl.col(f'word_time_diff_{shift}').quantile(0.75).name.suffix('quantile_75'),\n","                        pl.col(f'word_time_diff_{shift}').kurtosis().name.suffix('kurt'),\n","                        pl.col(f'word_time_diff_{shift}').skew().name.suffix('skew'),\n","        )\n","        feats.append(words_shifted)\n","    return feats[0], feats[1]"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["< word_wait_shift >\n"]}],"source":["for word in [2,5,10,25,50]:\n","    tr,ts=word_wait_shift(train_logs, test_logs)\n","    train_feats = tr.join(train_scores, on='id', how='left')\n","    test_feats = ts.clone().collect().to_pandas()\n","    train_feats = train_feats.collect().to_pandas()\n","    test_preds, oof_preds, rmse, model = lgb_pipeline(train_feats, test_feats, lgb_params_1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bad: sent_by_par, par_words, word_pauses_basic,\n","\n","# neutral: cursor_pos_acceleration, countvectorize_two_one (slightly better than cursor_pos_acc + 4),  sent_timings\n","\n","# good: full sentences, full paragraphs, down_events, one_grams, create_pauses -  r-burst, nunique, words feats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Best Feature Set: ['train_essay_sentences.pkl', 'train_create_pauses.pkl', \n","# 'train_vector_one_gram.pkl', 'train_essay_paragraphs.pkl', \n","# 'train_categorical_nunique.pkl', 'train_word_pauses.pkl', \n","# 'train_events_counts_rate_of_change.pkl', 'train_word_counts_rate_of_change.pkl', \n","# 'train_r_burst_feats.pkl', 'train_vector_two_gram.pkl', 'train_events_counts_acceleration.pkl']\n","\n","# Best_Feature_Set = ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","# 'train_create_pauses.pkl', 'train_sentences_per_paragraph.pkl', \n","# 'train_add_word_pauses_basic.pkl', 'train_cursor_pos_acceleration.pkl', \n","# 'train_remove_word_pauses_adv.pkl', 'train_paragraph_length.pkl', \n","# 'train_categorical_nunique.pkl', 'train_paragraph_words.pkl', \n","# 'train_events_counts_time_based.pkl', 'train_vector_two_gram.pkl', \n","# 'train_sentences_words.pkl', 'train_add_word_pauses_adv.pkl']\n","\n","# Best_Feature_Set = ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', \n","# 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', \n","# 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', \n","# 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', \n","# 'train_remove_word_pauses_adv.pkl', 'train_add_word_pauses_adv.pkl', \n","# 'train_word_count_acceleration_adv.pkl']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# best_feature_set_1 - PARTIAL\n","train_essays          = get_essay_df(train_logs.collect().to_pandas())\n","test_essays           = get_essay_df(test_logs.collect().to_pandas())\n","\n","tr_down_events_counts, ts_down_events_counts = down_events_counts(train_logs, test_logs)\n","tr_vect_one, ts_vect_one = countvectorize_one_one(train_essays, test_essays)\n","tr_pauses, ts_pauses = create_pauses(train_logs, test_logs)\n","tr_cursor_pos_acc, ts_cursor_pos_acc = cursor_pos_acceleration(train_logs, test_logs)\n","tr_word_pause, ts_word_pause = word_pauses(train_logs, test_logs)\n","tr_word_count_acc, ts_word_count_acc = word_count_acceleration(train_logs, test_logs)\n","#tr_p_burst, ts_p_burst = p_burst_feats(train_logs, test_logs, 2)\n","tr_r_burst, ts_r_burst = r_burst_feats(train_logs, test_logs)\n","#tr_event_acc, ts_event_acc = events_counts_acceleration(train_logs, test_logs)\n","# tr_nunique, ts_nunique = categorical_nunique(train_logs, test_logs)\n","tr_vect_two, ts_vect_two = countvectorize_two_one(train_essays, test_essays)\n","# tr_time_by_act, ts_time_by_act = action_time_by_activity(train_logs, test_logs)\n","# tr_cursor_pos_roc, ts_cursor_pos_roc = cursor_pos_rate_of_change(train_logs, test_logs)\n","# \n","# tr_act_count, ts_act_count = count_of_activities(train_logs, test_logs)\n","# tr_get_keys, ts_get_keys = get_keys_pressed_per_second(train_logs.collect().to_pandas(), \n","#                                                        test_logs.collect().to_pandas())\n","# \n","# tr_input_change, ts_input_change = input_text_change_feats(train_logs, test_logs)\n","# tr_wc_roc, ts_wc_roc =  word_counts_rate_of_change(train_logs, test_logs)\n","\n","train_feats = tr_down_events_counts.join(tr_vect_one, on='id', how='left')\n","train_feats = train_feats.join(tr_pauses, on='id', how='left')\n","train_feats = train_feats.join(tr_cursor_pos_acc, on='id', how='left')\n","train_feats = train_feats.join(tr_word_pause, on='id', how='left')\n","train_feats = train_feats.join(tr_word_count_acc, on='id', how='left')\n","#train_feats = train_feats.join(tr_p_burst, on='id', how='left')\n","train_feats = train_feats.join(tr_r_burst, on='id', how='left')\n","train_feats = train_feats.join(tr_vect_two, on='id', how='left')\n","# train_feats = train_feats.join(tr_event_acc, on='id', how='left')\n","# train_feats = train_feats.join(tr_nunique, on='id', how='left')\n","# train_feats = train_feats.join(tr_wc_roc, on='id', how='left')\n","# train_feats = train_feats.join(tr_act_count, on='id', how='left')\n","# train_feats = train_feats.join(tr_cursor_pos_roc, on='id', how='left')\n","\n","# train_feats = train_feats.join(tr_get_keys, on='id', how='left')\n","# train_feats = train_feats.join(tr_input_change, on='id', how='left')\n","# train_feats = train_feats.join(tr_time_by_act, on='id', how='left')\n","\n","test_feats = ts_down_events_counts.join(ts_vect_one, on='id', how='left')\n","test_feats = test_feats.join(ts_pauses, on='id', how='left')\n","test_feats = test_feats.join(ts_cursor_pos_acc, on='id', how='left')\n","test_feats = test_feats.join(ts_word_pause, on='id', how='left')\n","test_feats = test_feats.join(ts_word_count_acc, on='id', how='left')\n","# test_feats = test_feats.join(ts_p_burst, on='id', how='left')\n","test_feats = test_feats.join(ts_r_burst, on='id', how='left')\n","test_feats = test_feats.join(ts_vect_two, on='id', how='left')\n","# test_feats = test_feats.join(tr_event_acc, on='id', how='left')\n","# test_feats = test_feats.join(ts_nunique, on='id', how='left')\n","# test_feats = test_feats.join(ts_wc_roc, on='id', how='left')\n","# test_feats = test_feats.join(ts_act_count, on='id', how='left')\n","# test_feats = test_feats.join(ts_cursor_pos_roc, on='id', how='left')\n","\n","\n","# test_feats = test_feats.join(ts_get_keys, on='id', how='left')\n","# test_feats = test_feats.join(ts_input_change, on='id', how='left')\n","# test_feats = test_feats.join(ts_time_by_act, on='id', how='left')\n","\n","\n","train_logs = train_logs.collect().to_pandas()\n","test_logs = test_logs.collect().to_pandas()\n","train_scores = train_scores.collect().to_pandas()\n","train_feats = train_feats.sort('id')\n","train_feats = train_feats.collect().to_pandas()\n","test_feats = test_feats.collect().to_pandas()\n","\n","train_feats           = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n","\n","train_feats           = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n","\n","train_feats = train_feats.merge(train_scores, on='id', how='left')\n","print(f'train feats shape {train_feats.shape}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T16:21:44.320780Z","iopub.status.busy":"2023-12-18T16:21:44.320496Z"},"trusted":true},"outputs":[],"source":["from m5_sb_models import lgb_pipeline\n","lgb_params_1 = {\n","    'boosting_type': 'gbdt', \n","    'metric': 'rmse',\n","    'reg_alpha': 0.0031, \n","    'reg_lambda': 0.001, \n","    'colsample_bytree': 0.8,  \n","    'subsample_freq': 1,  \n","    'subsample': 0.75,  \n","    'learning_rate': 0.017, \n","    'num_leaves': 19, \n","    'min_child_samples': 46,\n","    'n_estimators': 350,\n","    'verbosity': -1\n","    }\n","\n","param = {'n_estimators': 1024,\n","        'learning_rate': 0.005,\n","        'metric': 'rmse',\n","        'force_col_wise': True,\n","        'verbosity': 0,}\n","\n","# train_feats = train_feats[['id', 'score'] + feat_select]\n","# test_feats = test_feats[['id'] + feat_select]\n","\n","print(f'train feats shape {train_feats.shape}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shuffle_preds = []\n","\n","for i in range(15):\n","    train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n","    test_preds, oof_preds, rmse, model = lgb_pipeline(train_feats, test_feats, lgb_params_1)\n","    shuffle_preds.append(rmse)\n","    #test_preds, oof_preds, rmse, model = lgb_pipeline(train_feats, test_feats, param)\n","\n","np.mean(shuffle_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["oof_res = oof_preds.groupby(['id', 'score'])['preds'].mean().reset_index()\n","# oof_res['rmse'] = oof_res.apply(lambda x: np.sqrt((x['score']-x['preds'])**2))\n","oof_res['RMSE'] = np.sqrt((oof_res['score']-oof_res['preds'])**2)\n","oof_res.groupby(['score'])['RMSE'].mean().reset_index().sort_values('RMSE', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["oof_res = oof_preds.groupby(['id', 'score'])['preds'].mean().reset_index()\n","oof_res['RMSE'] = np.sqrt((oof_res['score']-oof_res['preds'])**2)\n","oof_res.groupby(['score'])['RMSE'].mean().reset_index().sort_values('RMSE', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["oof_res = oof_preds.groupby(['id', 'score'])['preds'].mean().reset_index()\n","# oof_res['rmse'] = oof_res.apply(lambda x: np.sqrt((x['score']-x['preds'])**2))\n","oof_res['RMSE'] = np.sqrt((oof_res['score']-oof_res['preds'])**2)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
