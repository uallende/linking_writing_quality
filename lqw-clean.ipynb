{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-18T16:20:56.671429Z","iopub.status.busy":"2023-12-18T16:20:56.671017Z","iopub.status.idle":"2023-12-18T16:20:58.568632Z","shell.execute_reply":"2023-12-18T16:20:58.567768Z","shell.execute_reply.started":"2023-12-18T16:20:56.671393Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import polars as pl\n","from m3_model_params import lgb_params_1\n","from m4_feats_polars import *\n","from m5_sb_models import *\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n","train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n","test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n","train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# bad: sent_by_par, par_words, word_pauses_basic,\n","\n","# neutral: cursor_pos_acceleration, countvectorize_two_one (slightly better than cursor_pos_acc + 4),  sent_timings, train_remove_word_pauses\n","\n","# good: full sentences, full paragraphs, down_events, one_grams, create_pauses -  r-burst, nunique, words feats\n","\n","# ratios\n","\n","# 1. Pause time vs words Time\n","# 2. Number of pauses vs number of words\n","# 3. # events input / # number of events remove/cut\n","# 4. # events input / # events non-production\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Best Feature Set: ['train_essay_sentences.pkl', 'train_create_pauses.pkl', \n","# 'train_vector_one_gram.pkl', 'train_essay_paragraphs.pkl', \n","# 'train_categorical_nunique.pkl', 'train_word_pauses.pkl', \n","# 'train_events_counts_rate_of_change.pkl', 'train_word_counts_rate_of_change.pkl', \n","# 'train_r_burst_feats.pkl', 'train_vector_two_gram.pkl', 'train_events_counts_acceleration.pkl']\n","\n","# Best_Feature_Set = ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","# 'train_create_pauses.pkl', 'train_sentences_per_paragraph.pkl', \n","# 'train_add_word_pauses_basic.pkl', 'train_cursor_pos_acceleration.pkl', \n","# 'train_remove_word_pauses_adv.pkl', 'train_paragraph_length.pkl', \n","# 'train_categorical_nunique.pkl', 'train_paragraph_words.pkl', \n","# 'train_events_counts_time_based.pkl', 'train_vector_two_gram.pkl', \n","# 'train_sentences_words.pkl', 'train_add_word_pauses_adv.pkl']\n","\n","# Best_Feature_Set = ['train_down_events_counts_one.pkl', 'train_essay_par_length.pkl', \n","# 'train_create_pauses.pkl', 'train_countvectorize_one_two.pkl', \n","# 'train_down_events_counts_two.pkl', 'train_cursor_pos_acceleration_basic.pkl', \n","# 'train_essay_sents_per_par_basic.pkl', 'train_countvectorize_two_one.pkl', \n","# 'train_remove_word_pauses_adv.pkl', 'train_add_word_pauses_adv.pkl', \n","# 'train_word_count_acceleration_adv.pkl']\n","\n","\n","# Best Feature Set: ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","# 'train_create_pauses.pkl', 'train_essay_paragraphs.pkl', \n","# 'train_cursor_pos_acceleration.pkl', 'train_sentences_per_paragraph.pkl', \n","# 'train_remove_word_pauses.pkl', 'train_vector_two_gram.pkl', \n","# 'train_p_burst_feats.pkl', 'train_r_burst_feats.pkl']\n","\n","\n","# Best Feature Set: ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","#                    'train_create_pauses.pkl', 'train_sentences_per_paragraph.pkl', \n","#                    'train_essay_paragraphs.pkl', 'train_cursor_pos_acceleration.pkl', \n","#                    'train_p_burst_feats.pkl', 'train_remove_word_pauses.pkl', \n","#                    'train_vector_two_gram.pkl', 'train_remove_words_time_spent.pkl', \n","#                    'train_product_to_keys.pkl']\n","\n","\n","# Best Feature Set: ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","# 'train_create_pauses.pkl', 'train_essay_paragraphs.pkl', \n","# 'train_word_count_acceleration.pkl', 'train_remove_words_time_spent.pkl']\n","\n","# Best Feature Set SVR: ['train_essay_sentences.pkl', 'train_create_pauses.pkl', \n","# 'train_essay_paragraphs.pkl', 'train_word_wait_10.pkl', \n","# 'train_categorical_nunique.pkl', 'train_remove_words_time_spent.pkl', \n","# 'train_essay_words.pkl', 'train_word_wait_1.pkl', \n","# 'train_add_word_pauses.pkl', 'train_word_wait_25.pkl']\n","\n","## UNFINISHED XGBOOST\n","# ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","# 'train_create_pauses.pkl', 'train_essay_paragraphs.pkl', \n","# 'train_essay_sentences.pkl', 'train_word_counts_rate_of_change.pkl', \n","# 'train_vector_two_gram.pkl', 'train_text_changes_counts.pkl', \n","# 'train_remove_word_pauses.pkl']\n","\n","# Best Feature Set: ['train_down_events_counts.pkl', 'train_vector_one_gram.pkl', \n","#                    'train_create_pauses.pkl', 'train_essay_paragraphs.pkl', \n","#                    'train_activity_time_on_down_time.pkl', 'train_remove_word_pauses.pkl', \n","#                    'train_cursor_pos_acceleration.pkl']\n","\n","\n","# cat <150\n","\n","# lgbm 450\n","\n","# xgb 225"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["< Events counts features >\n","< Count vectorize one-grams >\n","< Idle time features >\n","< word count acceleration >\n","< remove_words_time_spent >\n","< Count vectorize bi-grams >\n","< cursor position acceleration >\n","< R-burst features >\n","< Categorical # unique values features >\n","< removed words pauses basic\n","< word_wait_shift >\n","< event_id rate of change >\n","< Essays paragraphs feats >\n","< Essays paragraphs feats >\n","< Essays sentences feats >\n","< Essays sentences feats >\n","< Essays word feats >\n","< Essays word feats >\n","train feats shape (2471, 185)\n"]}],"source":["# PANDAS FEATS\n","train_essays          = get_essay_df(train_logs.collect().to_pandas())\n","test_essays           = get_essay_df(test_logs.collect().to_pandas())\n","\n","tr_down_events_counts, ts_down_events_counts = down_events_counts(train_logs, test_logs)\n","tr_vect_one, ts_vect_one = countvectorize_one_one(train_essays, test_essays)\n","tr_pauses, ts_pauses = create_pauses(train_logs, test_logs)\n","tr_word_c_acc, ts_word_c_acc = word_count_acceleration(train_logs, test_logs)\n","tr_rem_words_time_spent, ts_rem_words_time_spent = remove_words_time_spent(train_logs, test_logs)\n","tr_vect_two, ts_vect_two = countvectorize_two_one(train_essays, test_essays)\n","tr_cursor_pos_acc, ts_cursor_pos_acc = cursor_pos_acceleration(train_logs, test_logs)\n","tr_r_burst, ts_r_burst = r_burst_feats(train_logs, test_logs)\n","tr_nuni, ts_nuni = categorical_nunique(train_logs, test_logs)\n","tr_remove_pause, ts_remove_pause = remove_word_pauses(train_logs, test_logs)\n","tr_word_wait, ts_word_wait = word_wait_shift(train_logs, test_logs, 1)\n","tr_e_counts_roc, ts_e_counts_roc = events_counts_rate_of_change(train_logs, test_logs, time_agg=3)\n","\n","# tr_wc_roc, ts_wc_roc = word_counts_rate_of_change(train_logs, test_logs)\n","\n","train_feats = tr_down_events_counts.join(tr_vect_one, on='id', how='left')\n","train_feats = train_feats.join(tr_pauses, on='id', how='left')\n","train_feats = train_feats.join(tr_word_c_acc, on='id', how='left')\n","train_feats = train_feats.join(tr_rem_words_time_spent, on='id', how='left')\n","train_feats = train_feats.join(tr_vect_two, on='id', how='left')\n","train_feats = train_feats.join(tr_r_burst, on='id', how='left')\n","train_feats = train_feats.join(tr_cursor_pos_acc, on='id', how='left')\n","train_feats = train_feats.join(tr_nuni, on='id', how='left')\n","train_feats = train_feats.join(tr_remove_pause, on='id', how='left')\n","train_feats = train_feats.join(tr_word_wait, on='id', how='left')\n","train_feats = train_feats.join(tr_e_counts_roc, on='id', how='left')\n","# train_feats = train_feats.join(tr_wc_roc, on='id', how='left')\n","\n","\n","test_feats = ts_down_events_counts.join(ts_vect_one, on='id', how='left')\n","test_feats = test_feats.join(ts_pauses, on='id', how='left')\n","test_feats = test_feats.join(ts_word_c_acc, on='id', how='left')\n","test_feats = test_feats.join(ts_rem_words_time_spent, on='id', how='left')\n","test_feats = test_feats.join(ts_vect_two, on='id', how='left')\n","test_feats = test_feats.join(ts_r_burst, on='id', how='left')\n","test_feats = test_feats.join(ts_cursor_pos_acc, on='id', how='left')\n","test_feats = test_feats.join(ts_nuni, on='id', how='left')\n","test_feats = test_feats.join(ts_remove_pause, on='id', how='left')\n","test_feats = test_feats.join(ts_word_wait, on='id', how='left')\n","test_feats = test_feats.join(ts_e_counts_roc, on='id', how='left')\n","# test_feats = test_feats.join(ts_wc_roc, on='id', how='left')\n","\n","train_logs = train_logs.collect().to_pandas()\n","test_logs = test_logs.collect().to_pandas()\n","train_scores = train_scores.collect().to_pandas()\n","train_feats = train_feats.collect().to_pandas()\n","test_feats = test_feats.collect().to_pandas()\n","\n","train_feats           = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n","train_feats           = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n","train_feats           = train_feats.merge(word_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n","\n","train_feats           = train_feats.merge(train_scores, on=['id'], how='left')\n","print(f'train feats shape {train_feats.shape}')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Estimators 250: 0.6041376201654391\n"]}],"source":["from m5_sb_models import lgb_pipeline, xgb_pipeline\n","shuffle_preds = []\n","\n","for est in [250,300,350,400,450,500]:\n","\n","    xgb_params = {\n","        'alpha': 1,\n","        'colsample_bytree': 0.8,\n","        'gamma': 1.5,\n","        'learning_rate': 0.05,\n","        'max_depth': 4,\n","        'min_child_weight': 10,\n","        'subsample': 0.8,\n","        'n_estimators': est \n","        }\n","    \n","    for i in range(8):\n","        train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n","        test_preds, oof_preds, rmse, model = xgb_pipeline(train_feats, test_feats, xgb_params)\n","        shuffle_preds.append(rmse)\n","    print(f'Estimators {est}: {np.mean(shuffle_preds)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from m5_sb_models import catboost_pipeline\n","\n","shuffle_preds = []\n","for est in [50,75,100,125,150,175,200,225,250,300,350,400,450,500]:\n","\n","    catboost_params = {\n","        'iterations': est, \n","        'learning_rate': 0.1, \n","        'depth': 6, \n","        'loss_function': 'RMSE', \n","        'od_wait': 20, \n","        'od_type': 'Iter', \n","        'verbose': False, \n","        'metric_period': 50, \n","        'eval_metric': 'RMSE', \n","        'bagging_temperature': 0.2\n","    }\n","        \n","    for i in range(15):\n","        train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n","        test_preds, oof_preds, rmse, model = catboost_pipeline(train_feats, test_feats, catboost_params)\n","        shuffle_preds.append(rmse)\n","    print(f'Estimators {est}: {np.mean(shuffle_preds)}')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T16:21:44.320780Z","iopub.status.busy":"2023-12-18T16:21:44.320496Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Estimators 250: 0.6059676985712591\n","Estimators 300: 0.6055514276074799\n","Estimators 350: 0.6053147217238997\n","Estimators 400: 0.6051365824502887\n","Estimators 450: 0.6051214823348453\n"]}],"source":["from m5_sb_models import lgb_pipeline, xgb_pipeline\n","shuffle_preds = []\n","\n","for est in [250,300,350,400,450]:\n","\n","    lgb_params_1 = {\n","        'boosting_type': 'gbdt', \n","        'metric': 'rmse',\n","        'reg_alpha': 0.0031, \n","        'reg_lambda': 0.001, \n","        'colsample_bytree': 0.8,  \n","        'subsample_freq': 1,  \n","        'subsample': 0.75,  \n","        'learning_rate': 0.017, \n","        'num_leaves': 19, \n","        'min_child_samples': 46,\n","        'n_estimators': est,\n","        'verbosity': -1\n","        }\n","    \n","    for i in range(15):\n","        train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n","        test_preds, oof_preds, rmse, model = lgb_pipeline(train_feats, test_feats, lgb_params_1)\n","        shuffle_preds.append(rmse)\n","    print(f'Estimators {est}: {np.mean(shuffle_preds)}')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/root/Projects/Kaggle/linking-writing/lqw-clean.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw-clean.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m15\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw-clean.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     train_feats \u001b[39m=\u001b[39m train_feats\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw-clean.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     test_preds, oof_preds, rmse, model \u001b[39m=\u001b[39m lgb_pipeline(train_feats, test_feats, lgb_params_1)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw-clean.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     shuffle_preds\u001b[39m.\u001b[39mappend(rmse)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw-clean.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m np\u001b[39m.\u001b[39mmean(shuffle_preds)\n","File \u001b[0;32m~/Projects/Kaggle/linking-writing/m5_sb_models.py:54\u001b[0m, in \u001b[0;36mlgb_pipeline\u001b[0;34m(train, test, param, n_splits, iterations)\u001b[0m\n\u001b[1;32m     52\u001b[0m train_x, train_y, valid_x, valid_y \u001b[39m=\u001b[39m train_valid_split(x, y, train_index, valid_index)\n\u001b[1;32m     53\u001b[0m model \u001b[39m=\u001b[39m LGBMRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m \u001b[39m+\u001b[39m \u001b[39miter\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_x, train_y)\n\u001b[1;32m     56\u001b[0m \u001b[39m# model.fit(\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m#     train_x, train_y, \u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m#     eval_set=[(valid_x, valid_y)],\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m#     callbacks=[lgb.early_stopping(50, first_metric_only=True, verbose=False)])\u001b[39;00m\n\u001b[1;32m     61\u001b[0m valid_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(valid_x)\n","File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/sklearn.py:818\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y,\n\u001b[1;32m    812\u001b[0m         sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    813\u001b[0m         eval_set\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_names\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    814\u001b[0m         eval_init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    815\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, feature_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, categorical_feature\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    816\u001b[0m         callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    817\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight, init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[1;32m    819\u001b[0m                 eval_set\u001b[39m=\u001b[39;49meval_set, eval_names\u001b[39m=\u001b[39;49meval_names, eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[1;32m    820\u001b[0m                 eval_init_score\u001b[39m=\u001b[39;49meval_init_score, eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[1;32m    821\u001b[0m                 early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, verbose\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    822\u001b[0m                 categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature, callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n\u001b[1;32m    823\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/sklearn.py:683\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(init_model, LGBMModel):\n\u001b[1;32m    681\u001b[0m     init_model \u001b[39m=\u001b[39m init_model\u001b[39m.\u001b[39mbooster_\n\u001b[0;32m--> 683\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(params, train_set,\n\u001b[1;32m    684\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators, valid_sets\u001b[39m=\u001b[39;49mvalid_sets, valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m    685\u001b[0m                       early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m    686\u001b[0m                       evals_result\u001b[39m=\u001b[39;49mevals_result, fobj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fobj, feval\u001b[39m=\u001b[39;49meval_metrics_callable,\n\u001b[1;32m    687\u001b[0m                       verbose_eval\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    688\u001b[0m                       callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n\u001b[1;32m    690\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n\u001b[1;32m    691\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n","File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/engine.py:249\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    242\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    243\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    244\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    245\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    246\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    247\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 249\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    251\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    252\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/basic.py:2643\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2641\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   2642\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2643\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   2644\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   2645\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   2646\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   2647\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["shuffle_preds = []\n","\n","for i in range(15):\n","    train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n","    test_preds, oof_preds, rmse, model = lgb_pipeline(train_feats, test_feats, lgb_params_1)\n","    shuffle_preds.append(rmse)\n","\n","np.mean(shuffle_preds)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
