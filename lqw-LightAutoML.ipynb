{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "import joblib\n",
    "from m4_feats_polars import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from m5_sb_models import *\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def map_class(x, task, reader):\n",
    "    if task.name == 'multiclass':\n",
    "        return reader[x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "mapped = np.vectorize(map_class)\n",
    "\n",
    "def score(task, y_true, y_pred):\n",
    "    if task.name == 'binary':\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    elif task.name == 'multiclass':\n",
    "        return log_loss(y_true, y_pred)\n",
    "    elif task.name == 'reg' or task.name == 'multi:reg':\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    else:\n",
    "        raise 'Task is not correct.'\n",
    "        \n",
    "def take_pred_from_task(pred, task):\n",
    "    if task.name == 'binary' or task.name == 'reg':\n",
    "        return pred[:, 0]\n",
    "    elif task.name == 'multiclass' or task.name == 'multi:reg':\n",
    "        return pred\n",
    "    else:\n",
    "        raise 'Task is not correct.'\n",
    "        \n",
    "def use_plr(USE_PLR):\n",
    "    if USE_PLR:\n",
    "        return \"plr\"\n",
    "    else:\n",
    "        return \"cont\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Events counts features >\n",
      "< Count vectorize one-grams >\n",
      "< Idle time features >\n",
      "< cursor position acceleration >\n",
      "< R-burst features >\n",
      "< Categorical # unique values features >\n",
      "< event_id rate of change >\n",
      "< Word counts rate of change features >\n",
      "< removed words pauses basic\n",
      "< Count vectorize bi-grams >\n",
      "< word_wait_shift >\n",
      "< remove_words_time_spent >\n",
      "< Essays paragraphs feats >\n",
      "< Essays paragraphs feats >\n",
      "< Essays sentences feats >\n",
      "< Essays sentences feats >\n",
      "< Essays word feats >\n",
      "< Essays word feats >\n",
      "train feats shape (2471, 186)\n"
     ]
    }
   ],
   "source": [
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n",
    "test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n",
    "train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')\n",
    "\n",
    "# PANDAS FEATS\n",
    "train_essays          = get_essay_df(train_logs.collect().to_pandas())\n",
    "test_essays           = get_essay_df(test_logs.collect().to_pandas())\n",
    "\n",
    "tr_down_events_counts, ts_down_events_counts = down_events_counts(train_logs, test_logs)\n",
    "tr_vect_one, ts_vect_one = countvectorize_one_one(train_essays, test_essays)\n",
    "tr_pauses, ts_pauses = create_pauses(train_logs, test_logs)\n",
    "tr_cursor_pos_acc, ts_cursor_pos_acc = cursor_pos_acceleration(train_logs, test_logs)\n",
    "tr_r_burst, ts_r_burst = r_burst_feats(train_logs, test_logs)\n",
    "tr_nuni, ts_nuni = categorical_nunique(train_logs, test_logs)\n",
    "tr_e_counts_roc, ts_e_counts_roc = events_counts_rate_of_change(train_logs, test_logs, time_agg=3)\n",
    "tr_wc_roc, ts_wc_roc = word_counts_rate_of_change(train_logs, test_logs)\n",
    "tr_remove_pause, ts_remove_pause = remove_word_pauses(train_logs, test_logs)\n",
    "tr_vect_two, ts_vect_two = countvectorize_two_one(train_essays, test_essays)\n",
    "tr_word_wait, ts_word_wait = word_wait_shift(train_logs, test_logs, 1)\n",
    "tr_rem_words_time_spent, ts_rem_words_time_spent = remove_words_time_spent(train_logs, test_logs)\n",
    "\n",
    "\n",
    "train_feats = tr_down_events_counts.join(tr_vect_one, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_pauses, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_cursor_pos_acc, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_r_burst, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_nuni, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_e_counts_roc, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_wc_roc, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_remove_pause, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_vect_two, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_word_wait, on='id', how='left')\n",
    "train_feats = train_feats.join(tr_rem_words_time_spent, on='id', how='left')\n",
    "\n",
    "\n",
    "test_feats = ts_down_events_counts.join(ts_vect_one, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_pauses, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_cursor_pos_acc, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_r_burst, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_nuni, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_e_counts_roc, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_wc_roc, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_remove_pause, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_vect_two, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_word_wait, on='id', how='left')\n",
    "test_feats = test_feats.join(ts_rem_words_time_spent, on='id', how='left')\n",
    "\n",
    "train_logs = train_logs.collect().to_pandas()\n",
    "test_logs = test_logs.collect().to_pandas()\n",
    "train_scores = train_scores.collect().to_pandas()\n",
    "train_feats = train_feats.collect().to_pandas()\n",
    "test_feats = test_feats.collect().to_pandas()\n",
    "\n",
    "train_feats           = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "train_feats           = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "train_feats           = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "\n",
    "tr_ids = train_feats.id\n",
    "ts_ids = test_feats.id\n",
    "\n",
    "feats = pd.concat([train_feats,test_feats], axis=0)\n",
    "feats = preprocess_feats(feats)\n",
    "train_feats = feats[feats['id'].isin(tr_ids)]\n",
    "test_feats = feats[feats['id'].isin(ts_ids)]\n",
    "\n",
    "train_feats           = train_feats.merge(train_scores, on=['id'], how='left')\n",
    "print(f'train feats shape {train_feats.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N_THREADS = 16\n",
    "N_FOLDS = 10\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.2\n",
    "TIMEOUT = 10000\n",
    "ADVANCED_ROLES = False\n",
    "USE_QNT = True\n",
    "TASK = 'reg'\n",
    "ALGOS_FOR_BLEND =  [\"dense\", \"tabnet\", \"fttransformer\", \"autoint\"]\n",
    "USE_PLR = True\n",
    "TRAIN_BS = 128\n",
    "USE_FS = False\n",
    "FEATURE_RATIO = 0.8\n",
    "\n",
    "# example for binary classification\n",
    "TARGET_NAME = 'score'\n",
    "    \n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)\n",
    "\n",
    "task = Task(TASK)\n",
    "\n",
    "# example for binary classification\n",
    "roles = {\n",
    "    'target': TARGET_NAME,\n",
    "    #'drop': ['id']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:58:31] Stdout logging level is INFO3.\n",
      "[11:58:31] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[11:58:31] Task: reg\n",
      "\n",
      "[11:58:31] Start automl preset with listed constraints:\n",
      "[11:58:31] - time: 10000.00 seconds\n",
      "[11:58:31] - CPU: 16 cores\n",
      "[11:58:31] - memory: 16 GB\n",
      "\n",
      "[11:58:31] \u001b[1mTrain data shape: (2471, 186)\u001b[0m\n",
      "\n",
      "[11:58:31] Layer \u001b[1m1\u001b[0m train process start. Time left 9999.93 secs\n",
      "[11:58:32] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m ...\n",
      "[11:58:32] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:35] Epoch: 0, train loss: 0.7018664479255676, val loss: 0.40319305658340454, val metric: -0.40131697058677673\n",
      "[11:58:35] Epoch: 1, train loss: 0.41542747616767883, val loss: 0.3703724145889282, val metric: -0.36911892890930176\n",
      "[11:58:35] Epoch: 2, train loss: 0.3619755506515503, val loss: 0.3633615970611572, val metric: -0.36206766963005066\n",
      "[11:58:35] Epoch: 3, train loss: 0.3239513337612152, val loss: 0.37720149755477905, val metric: -0.37525150179862976\n",
      "[11:58:35] Epoch: 4, train loss: 0.29727864265441895, val loss: 0.3795014023780823, val metric: -0.37755903601646423\n",
      "[11:58:35] Epoch: 5, train loss: 0.26001885533332825, val loss: 0.390047550201416, val metric: -0.38848671317100525\n",
      "[11:58:35] Epoch: 6, train loss: 0.2286422997713089, val loss: 0.4090268909931183, val metric: -0.40736547112464905\n",
      "[11:58:35] Epoch: 7, train loss: 0.19915273785591125, val loss: 0.39563605189323425, val metric: -0.3936799466609955\n",
      "[11:58:36] Epoch: 8, train loss: 0.18242157995700836, val loss: 0.4764118194580078, val metric: -0.4746497571468353\n",
      "[11:58:36] Epoch: 9, train loss: 0.14212433993816376, val loss: 0.45319297909736633, val metric: -0.45153167843818665\n",
      "[11:58:36] Epoch: 10, train loss: 0.12071748822927475, val loss: 0.42793768644332886, val metric: -0.4260551929473877\n",
      "[11:58:36] Epoch: 11, train loss: 0.11431373655796051, val loss: 0.433028906583786, val metric: -0.431255578994751\n",
      "[11:58:36] Epoch: 12, train loss: 0.1008472666144371, val loss: 0.45602893829345703, val metric: -0.45447590947151184\n",
      "[11:58:36] Epoch: 13, train loss: 0.0967085137963295, val loss: 0.44796252250671387, val metric: -0.4465084671974182\n",
      "[11:58:36] Early stopping: val loss: 0.3642353415489197, val metric: -0.362795889377594\n",
      "[11:58:36] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:36] Epoch: 0, train loss: 0.7149007320404053, val loss: 0.47625932097435, val metric: -0.47633418440818787\n",
      "[11:58:36] Epoch: 1, train loss: 0.4104715883731842, val loss: 0.4242597222328186, val metric: -0.42324742674827576\n",
      "[11:58:36] Epoch: 2, train loss: 0.352924108505249, val loss: 0.40783554315567017, val metric: -0.4066392481327057\n",
      "[11:58:36] Epoch: 3, train loss: 0.3195284903049469, val loss: 0.41496843099594116, val metric: -0.4139721095561981\n",
      "[11:58:36] Epoch: 4, train loss: 0.2889983057975769, val loss: 0.43449866771698, val metric: -0.43324631452560425\n",
      "[11:58:37] Epoch: 5, train loss: 0.26263701915740967, val loss: 0.45318102836608887, val metric: -0.452141135931015\n",
      "[11:58:37] Epoch: 6, train loss: 0.22883817553520203, val loss: 0.44038429856300354, val metric: -0.43873441219329834\n",
      "[11:58:37] Epoch: 7, train loss: 0.2123824506998062, val loss: 0.4660942852497101, val metric: -0.46439605951309204\n",
      "[11:58:37] Epoch: 8, train loss: 0.18384692072868347, val loss: 0.5135977864265442, val metric: -0.512696385383606\n",
      "[11:58:37] Epoch: 9, train loss: 0.1527176797389984, val loss: 0.46363258361816406, val metric: -0.46220165491104126\n",
      "[11:58:37] Epoch: 10, train loss: 0.13253256678581238, val loss: 0.47961920499801636, val metric: -0.47831234335899353\n",
      "[11:58:37] Epoch: 11, train loss: 0.12082028388977051, val loss: 0.48652297258377075, val metric: -0.48542293906211853\n",
      "[11:58:37] Epoch: 12, train loss: 0.10757102072238922, val loss: 0.4780213236808777, val metric: -0.47686243057250977\n",
      "[11:58:37] Epoch: 13, train loss: 0.10087119787931442, val loss: 0.4987085163593292, val metric: -0.49748367071151733\n",
      "[11:58:37] Early stopping: val loss: 0.4085773825645447, val metric: -0.40752869844436646\n",
      "[11:58:37] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:37] Epoch: 0, train loss: 0.7205576300621033, val loss: 0.4002123475074768, val metric: -0.40005457401275635\n",
      "[11:58:37] Epoch: 1, train loss: 0.41642171144485474, val loss: 0.33665499091148376, val metric: -0.3366987705230713\n",
      "[11:58:37] Epoch: 2, train loss: 0.3546566069126129, val loss: 0.33267199993133545, val metric: -0.33266618847846985\n",
      "[11:58:38] Epoch: 3, train loss: 0.33336055278778076, val loss: 0.35178354382514954, val metric: -0.35216209292411804\n",
      "[11:58:38] Epoch: 4, train loss: 0.2899077236652374, val loss: 0.3593360483646393, val metric: -0.35957372188568115\n",
      "[11:58:38] Epoch: 5, train loss: 0.27228766679763794, val loss: 0.39298132061958313, val metric: -0.3931843340396881\n",
      "[11:58:38] Epoch: 6, train loss: 0.235338032245636, val loss: 0.40268105268478394, val metric: -0.40286317467689514\n",
      "[11:58:38] Epoch: 7, train loss: 0.20280884206295013, val loss: 0.38917022943496704, val metric: -0.3889804482460022\n",
      "[11:58:38] Epoch: 8, train loss: 0.17099779844284058, val loss: 0.40390413999557495, val metric: -0.4032672345638275\n",
      "[11:58:38] Epoch: 9, train loss: 0.13681404292583466, val loss: 0.41211941838264465, val metric: -0.41186949610710144\n",
      "[11:58:38] Epoch: 10, train loss: 0.130464568734169, val loss: 0.405808687210083, val metric: -0.4052509367465973\n",
      "[11:58:38] Epoch: 11, train loss: 0.1217927560210228, val loss: 0.41084980964660645, val metric: -0.410388708114624\n",
      "[11:58:38] Epoch: 12, train loss: 0.10612055659294128, val loss: 0.4102431535720825, val metric: -0.4097824990749359\n",
      "[11:58:38] Epoch: 13, train loss: 0.09883245825767517, val loss: 0.4289863109588623, val metric: -0.428744912147522\n",
      "[11:58:38] Early stopping: val loss: 0.3310272693634033, val metric: -0.33119872212409973\n",
      "[11:58:38] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:39] Epoch: 0, train loss: 0.7192148566246033, val loss: 0.44838154315948486, val metric: -0.4492345154285431\n",
      "[11:58:39] Epoch: 1, train loss: 0.4134320914745331, val loss: 0.43627798557281494, val metric: -0.43709057569503784\n",
      "[11:58:39] Epoch: 2, train loss: 0.3576481342315674, val loss: 0.42714035511016846, val metric: -0.42775729298591614\n",
      "[11:58:39] Epoch: 3, train loss: 0.33880314230918884, val loss: 0.41564249992370605, val metric: -0.41624656319618225\n",
      "[11:58:39] Epoch: 4, train loss: 0.3054705560207367, val loss: 0.39877110719680786, val metric: -0.39931803941726685\n",
      "[11:58:39] Epoch: 5, train loss: 0.2647818326950073, val loss: 0.42960309982299805, val metric: -0.43006154894828796\n",
      "[11:58:39] Epoch: 6, train loss: 0.22982484102249146, val loss: 0.4641522169113159, val metric: -0.4647859036922455\n",
      "[11:58:39] Epoch: 7, train loss: 0.20267392694950104, val loss: 0.4467788338661194, val metric: -0.44695112109184265\n",
      "[11:58:39] Epoch: 8, train loss: 0.1805342435836792, val loss: 0.4244166612625122, val metric: -0.4255615770816803\n",
      "[11:58:39] Epoch: 9, train loss: 0.14806421101093292, val loss: 0.5259524583816528, val metric: -0.5267171859741211\n",
      "[11:58:39] Epoch: 10, train loss: 0.14538781344890594, val loss: 0.44618988037109375, val metric: -0.44701069593429565\n",
      "[11:58:39] Epoch: 11, train loss: 0.1202092319726944, val loss: 0.45521730184555054, val metric: -0.45555001497268677\n",
      "[11:58:39] Epoch: 12, train loss: 0.10200485587120056, val loss: 0.46022894978523254, val metric: -0.4606943726539612\n",
      "[11:58:39] Epoch: 13, train loss: 0.08645178377628326, val loss: 0.4660094380378723, val metric: -0.4663534462451935\n",
      "[11:58:39] Epoch: 14, train loss: 0.08427727222442627, val loss: 0.4571477174758911, val metric: -0.4576018154621124\n",
      "[11:58:40] Epoch: 15, train loss: 0.07796759158372879, val loss: 0.43691128492355347, val metric: -0.43726396560668945\n",
      "[11:58:40] Epoch: 16, train loss: 0.08862954378128052, val loss: 0.44341760873794556, val metric: -0.4438382685184479\n",
      "[11:58:40] Epoch: 17, train loss: 0.07304346561431885, val loss: 0.44722455739974976, val metric: -0.4480105936527252\n",
      "[11:58:40] Epoch: 18, train loss: 0.06403984874486923, val loss: 0.4574916362762451, val metric: -0.45825251936912537\n",
      "[11:58:40] Early stopping: val loss: 0.4108629822731018, val metric: -0.4115796387195587\n",
      "[11:58:40] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:40] Epoch: 0, train loss: 0.735150158405304, val loss: 0.38045474886894226, val metric: -0.3820701837539673\n",
      "[11:58:40] Epoch: 1, train loss: 0.4247729182243347, val loss: 0.3563145399093628, val metric: -0.35721516609191895\n",
      "[11:58:40] Epoch: 2, train loss: 0.3687311112880707, val loss: 0.3381096124649048, val metric: -0.33960503339767456\n",
      "[11:58:40] Epoch: 3, train loss: 0.3286827504634857, val loss: 0.34139615297317505, val metric: -0.34240907430648804\n",
      "[11:58:40] Epoch: 4, train loss: 0.30073872208595276, val loss: 0.3442477583885193, val metric: -0.34550613164901733\n",
      "[11:58:40] Epoch: 5, train loss: 0.2625074088573456, val loss: 0.35211408138275146, val metric: -0.3531826436519623\n",
      "[11:58:40] Epoch: 6, train loss: 0.2403343766927719, val loss: 0.36268553137779236, val metric: -0.3643048107624054\n",
      "[11:58:41] Epoch: 7, train loss: 0.2056216448545456, val loss: 0.40322721004486084, val metric: -0.4041271209716797\n",
      "[11:58:41] Epoch: 8, train loss: 0.16931550204753876, val loss: 0.39674580097198486, val metric: -0.39751678705215454\n",
      "[11:58:41] Epoch: 9, train loss: 0.1407322883605957, val loss: 0.38911986351013184, val metric: -0.3898293673992157\n",
      "[11:58:41] Epoch: 10, train loss: 0.13642090559005737, val loss: 0.40301454067230225, val metric: -0.4040195047855377\n",
      "[11:58:41] Epoch: 11, train loss: 0.1187174990773201, val loss: 0.38312405347824097, val metric: -0.3843861222267151\n",
      "[11:58:41] Epoch: 12, train loss: 0.11016802489757538, val loss: 0.40052706003189087, val metric: -0.40168139338493347\n",
      "[11:58:41] Epoch: 13, train loss: 0.09130225330591202, val loss: 0.4037424921989441, val metric: -0.40488505363464355\n",
      "[11:58:41] Epoch: 14, train loss: 0.10058093070983887, val loss: 0.38984864950180054, val metric: -0.3911215662956238\n",
      "[11:58:41] Early stopping: val loss: 0.33326107263565063, val metric: -0.33450978994369507\n",
      "[11:58:41] ===== Start working with \u001b[1mfold 5\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:41] Epoch: 0, train loss: 0.7160654067993164, val loss: 0.4983425736427307, val metric: -0.4993997812271118\n",
      "[11:58:41] Epoch: 1, train loss: 0.4030342102050781, val loss: 0.4378008246421814, val metric: -0.43831223249435425\n",
      "[11:58:41] Epoch: 2, train loss: 0.3578595221042633, val loss: 0.44554316997528076, val metric: -0.4459041953086853\n",
      "[11:58:41] Epoch: 3, train loss: 0.31831157207489014, val loss: 0.4480857849121094, val metric: -0.44902274012565613\n",
      "[11:58:42] Epoch: 4, train loss: 0.28748443722724915, val loss: 0.4640563130378723, val metric: -0.464650958776474\n",
      "[11:58:42] Epoch: 5, train loss: 0.2506381571292877, val loss: 0.477272629737854, val metric: -0.4782182276248932\n",
      "[11:58:42] Epoch: 6, train loss: 0.22578555345535278, val loss: 0.49443650245666504, val metric: -0.4956754744052887\n",
      "[11:58:42] Epoch: 7, train loss: 0.20783063769340515, val loss: 0.5101264715194702, val metric: -0.5114413499832153\n",
      "[11:58:42] Epoch: 8, train loss: 0.1641143411397934, val loss: 0.4959430992603302, val metric: -0.49771758913993835\n",
      "[11:58:42] Epoch: 9, train loss: 0.14631657302379608, val loss: 0.5084488391876221, val metric: -0.511104941368103\n",
      "[11:58:42] Epoch: 10, train loss: 0.1405285745859146, val loss: 0.5092127919197083, val metric: -0.5112943053245544\n",
      "[11:58:42] Epoch: 11, train loss: 0.11510523408651352, val loss: 0.5116757154464722, val metric: -0.5137692093849182\n",
      "[11:58:42] Epoch: 12, train loss: 0.10613612085580826, val loss: 0.5030529499053955, val metric: -0.5055392980575562\n",
      "[11:58:42] Epoch: 13, train loss: 0.10018474608659744, val loss: 0.5082284212112427, val metric: -0.5101972222328186\n",
      "[11:58:42] Early stopping: val loss: 0.4363744556903839, val metric: -0.43693098425865173\n",
      "[11:58:42] ===== Start working with \u001b[1mfold 6\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:42] Epoch: 0, train loss: 0.7214709520339966, val loss: 0.47805678844451904, val metric: -0.47823870182037354\n",
      "[11:58:43] Epoch: 1, train loss: 0.39348360896110535, val loss: 0.38945165276527405, val metric: -0.39017143845558167\n",
      "[11:58:43] Epoch: 2, train loss: 0.34224647283554077, val loss: 0.38131627440452576, val metric: -0.38222557306289673\n",
      "[11:58:43] Epoch: 3, train loss: 0.3253689408302307, val loss: 0.38832801580429077, val metric: -0.38927412033081055\n",
      "[11:58:43] Epoch: 4, train loss: 0.28876978158950806, val loss: 0.3930037021636963, val metric: -0.3938278555870056\n",
      "[11:58:43] Epoch: 5, train loss: 0.26131951808929443, val loss: 0.39971035718917847, val metric: -0.40013065934181213\n",
      "[11:58:43] Epoch: 6, train loss: 0.22567857801914215, val loss: 0.4285193383693695, val metric: -0.42885223031044006\n",
      "[11:58:43] Epoch: 7, train loss: 0.19595825672149658, val loss: 0.43824437260627747, val metric: -0.43835484981536865\n",
      "[11:58:43] Epoch: 8, train loss: 0.17717954516410828, val loss: 0.45439645648002625, val metric: -0.4542381763458252\n",
      "[11:58:43] Epoch: 9, train loss: 0.1429244577884674, val loss: 0.448196142911911, val metric: -0.44800207018852234\n",
      "[11:58:43] Epoch: 10, train loss: 0.11917184293270111, val loss: 0.4370930790901184, val metric: -0.43676355481147766\n",
      "[11:58:43] Epoch: 11, train loss: 0.11452138423919678, val loss: 0.4602351188659668, val metric: -0.4599892497062683\n",
      "[11:58:43] Epoch: 12, train loss: 0.09641966223716736, val loss: 0.4797608256340027, val metric: -0.4797484278678894\n",
      "[11:58:43] Epoch: 13, train loss: 0.09604985266923904, val loss: 0.43671780824661255, val metric: -0.43621599674224854\n",
      "[11:58:43] Early stopping: val loss: 0.37964585423469543, val metric: -0.3805134892463684\n",
      "[11:58:44] ===== Start working with \u001b[1mfold 7\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:44] Epoch: 0, train loss: 0.7007368206977844, val loss: 0.5070780515670776, val metric: -0.507264256477356\n",
      "[11:58:44] Epoch: 1, train loss: 0.39987659454345703, val loss: 0.46621090173721313, val metric: -0.4674088954925537\n",
      "[11:58:44] Epoch: 2, train loss: 0.335229754447937, val loss: 0.4797278046607971, val metric: -0.4818328619003296\n",
      "[11:58:44] Epoch: 3, train loss: 0.3120259642601013, val loss: 0.4809495210647583, val metric: -0.4831101894378662\n",
      "[11:58:44] Epoch: 4, train loss: 0.280351847410202, val loss: 0.4837510883808136, val metric: -0.485826313495636\n",
      "[11:58:44] Epoch: 5, train loss: 0.25424158573150635, val loss: 0.5031725168228149, val metric: -0.5051831007003784\n",
      "[11:58:44] Epoch: 6, train loss: 0.2166222780942917, val loss: 0.5226202011108398, val metric: -0.5242371559143066\n",
      "[11:58:44] Epoch: 7, train loss: 0.18853113055229187, val loss: 0.5369634032249451, val metric: -0.5386625528335571\n",
      "[11:58:44] Epoch: 8, train loss: 0.15277829766273499, val loss: 0.5344926118850708, val metric: -0.5365143418312073\n",
      "[11:58:44] Epoch: 9, train loss: 0.13929587602615356, val loss: 0.5634307265281677, val metric: -0.5655806064605713\n",
      "[11:58:44] Epoch: 10, train loss: 0.13294638693332672, val loss: 0.557410478591919, val metric: -0.5590324997901917\n",
      "[11:58:44] Epoch: 11, train loss: 0.12001122534275055, val loss: 0.5626171827316284, val metric: -0.5643028020858765\n",
      "[11:58:44] Epoch: 12, train loss: 0.10815225541591644, val loss: 0.567391037940979, val metric: -0.569390058517456\n",
      "[11:58:44] Epoch: 13, train loss: 0.10770455002784729, val loss: 0.6061564087867737, val metric: -0.608170747756958\n",
      "[11:58:45] Early stopping: val loss: 0.4671901762485504, val metric: -0.4689403474330902\n",
      "[11:58:45] ===== Start working with \u001b[1mfold 8\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:45] Epoch: 0, train loss: 0.695082426071167, val loss: 0.4466637372970581, val metric: -0.44499433040618896\n",
      "[11:58:45] Epoch: 1, train loss: 0.4053133428096771, val loss: 0.3626839518547058, val metric: -0.36075273156166077\n",
      "[11:58:45] Epoch: 2, train loss: 0.3604065179824829, val loss: 0.37731003761291504, val metric: -0.37513601779937744\n",
      "[11:58:45] Epoch: 3, train loss: 0.3181873559951782, val loss: 0.36175209283828735, val metric: -0.35948750376701355\n",
      "[11:58:45] Epoch: 4, train loss: 0.300162672996521, val loss: 0.3608357310295105, val metric: -0.3585737943649292\n",
      "[11:58:45] Epoch: 5, train loss: 0.2618561387062073, val loss: 0.3628036677837372, val metric: -0.36104702949523926\n",
      "[11:58:45] Epoch: 6, train loss: 0.2256118655204773, val loss: 0.3728518486022949, val metric: -0.3710688054561615\n",
      "[11:58:45] Epoch: 7, train loss: 0.21114809811115265, val loss: 0.3807463049888611, val metric: -0.37965887784957886\n",
      "[11:58:45] Epoch: 8, train loss: 0.17904779314994812, val loss: 0.3886808753013611, val metric: -0.38688430190086365\n",
      "[11:58:45] Epoch: 9, train loss: 0.15158943831920624, val loss: 0.3891204297542572, val metric: -0.38802462816238403\n",
      "[11:58:45] Epoch: 10, train loss: 0.11921657621860504, val loss: 0.3852214813232422, val metric: -0.3843485116958618\n",
      "[11:58:46] Epoch: 11, train loss: 0.10582000762224197, val loss: 0.3981757164001465, val metric: -0.39675667881965637\n",
      "[11:58:46] Epoch: 12, train loss: 0.08993146568536758, val loss: 0.39600870013237, val metric: -0.39482972025871277\n",
      "[11:58:46] Epoch: 13, train loss: 0.08865072578191757, val loss: 0.3982071876525879, val metric: -0.396578848361969\n",
      "[11:58:46] Epoch: 14, train loss: 0.07999225705862045, val loss: 0.40163278579711914, val metric: -0.3999446630477905\n",
      "[11:58:46] Early stopping: val loss: 0.3510832190513611, val metric: -0.3488973081111908\n",
      "[11:58:46] ===== Start working with \u001b[1mfold 9\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m =====\n",
      "[11:58:46] Epoch: 0, train loss: 0.711948037147522, val loss: 0.47621089220046997, val metric: -0.47908568382263184\n",
      "[11:58:46] Epoch: 1, train loss: 0.415121465921402, val loss: 0.4452669024467468, val metric: -0.44732171297073364\n",
      "[11:58:46] Epoch: 2, train loss: 0.36835286021232605, val loss: 0.4156620502471924, val metric: -0.4191727936267853\n",
      "[11:58:46] Epoch: 3, train loss: 0.33079105615615845, val loss: 0.43464726209640503, val metric: -0.4386310875415802\n",
      "[11:58:46] Epoch: 4, train loss: 0.2934732735157013, val loss: 0.44628822803497314, val metric: -0.4505663514137268\n",
      "[11:58:46] Epoch: 5, train loss: 0.2789960205554962, val loss: 0.45524823665618896, val metric: -0.4597201347351074\n",
      "[11:58:46] Epoch: 6, train loss: 0.22638708353042603, val loss: 0.4843652844429016, val metric: -0.48936259746551514\n",
      "[11:58:47] Epoch: 7, train loss: 0.19881749153137207, val loss: 0.4768424332141876, val metric: -0.48127564787864685\n",
      "[11:58:47] Epoch: 8, train loss: 0.19088390469551086, val loss: 0.469026118516922, val metric: -0.4739643931388855\n",
      "[11:58:47] Epoch: 9, train loss: 0.154213547706604, val loss: 0.48469221591949463, val metric: -0.48964664340019226\n",
      "[11:58:47] Epoch: 10, train loss: 0.1326858252286911, val loss: 0.5033261179924011, val metric: -0.5086489915847778\n",
      "[11:58:47] Epoch: 11, train loss: 0.11999808251857758, val loss: 0.5174201726913452, val metric: -0.5224148631095886\n",
      "[11:58:47] Epoch: 12, train loss: 0.10343696177005768, val loss: 0.4981663227081299, val metric: -0.5036091208457947\n",
      "[11:58:47] Epoch: 13, train loss: 0.09258048236370087, val loss: 0.4965648353099823, val metric: -0.5020028352737427\n",
      "[11:58:47] Early stopping: val loss: 0.41550055146217346, val metric: -0.4186541736125946\n",
      "[11:58:47] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m finished. score = \u001b[1m-0.3901438295744318\u001b[0m\n",
      "[11:58:47] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_denselight_0\u001b[0m fitting and predicting completed\n",
      "[11:58:47] Time left 9983.43 secs\n",
      "\n",
      "[11:58:47] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:58:47] \u001b[1mAutoml preset training completed in 16.57 seconds\u001b[0m\n",
      "\n",
      "[11:58:47] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (10 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_denselight_0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo = 'denselight'\n",
    "automl = TabularAutoML(\n",
    "    task = task, \n",
    "    timeout = TIMEOUT,\n",
    "    cpu_limit = N_THREADS,\n",
    "    general_params = {\"use_algos\": [[algo]]}, # ['nn', 'mlp', 'dense', 'denselight', 'resnet', 'snn', 'node', 'autoint', 'fttransformer'] or custom torch model\n",
    "    nn_params = {\n",
    "        \"n_epochs\": 100, \n",
    "        \"bs\": TRAIN_BS, \n",
    "        \"num_workers\": 0, \n",
    "        \"path_to_save\": None, \n",
    "        \"freeze_defaults\": True,\n",
    "       # \"cont_embedder\": use_plr(USE_PLR),\n",
    "    },\n",
    "    nn_pipeline_params = {\"use_qnt\": USE_QNT, \"use_te\": False},\n",
    "    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE, 'advanced_roles': ADVANCED_ROLES},\n",
    ")\n",
    "\n",
    "oof_pred = automl.fit_predict(train_feats, roles = roles, verbose = 3)\n",
    "test_pred = automl.predict(test_feats)\n",
    "oof = score(task, mapped(train_feats[TARGET_NAME].values, task, automl.reader.class_mapping), take_pred_from_task(oof_pred.data, task))\n",
    "denselight_list = [(oof, oof_pred.data[:, 0], test_pred.data[:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
