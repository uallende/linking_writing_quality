{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m6_feats_comb import *\n",
    "from m7_utils import *\n",
    "from m5_models import *\n",
    "from m3_model_params import lgb_params_2 as lgbm_params\n",
    "from m3_model_params import xgb_params_2 as xgb_params\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, StandardScaler\n",
    "from m3_model_params import non_important_feats\n",
    "from m6_feats_comb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\n",
    "FEAT_STORE_DIR = 'feat_store_combined'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:25<00:00,  2.59s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12987.15it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 11570.78it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 12261.91it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 14004.57it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 11614.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1604892/2586306173.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/tmp/ipykernel_1604892/2586306173.py:222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_1604892/2586306173.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/tmp/ipykernel_1604892/2586306173.py:224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 26.27it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 45262.27it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 24338.32it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 36792.14it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 29676.68it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 24105.20it/s]\n",
      "/tmp/ipykernel_1604892/2586306173.py:221: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/tmp/ipykernel_1604892/2586306173.py:222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_1604892/2586306173.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/tmp/ipykernel_1604892/2586306173.py:224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    }
   ],
   "source": [
    "# The following code comes almost Abdullah's notebook: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "# Abdullah's code is based on work shared in previous notebooks (e.g., https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        \n",
    "        self.idf = defaultdict(float)\n",
    "    \n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "\n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def make_feats(self, df):\n",
    "        \n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('up_time', ['max']),\n",
    "            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n",
    "            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering ratios data\")\n",
    "        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
    "        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n",
    "\n",
    "        return feats\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n",
    "train_feats = train_feats.drop(columns=nan_cols)\n",
    "test_feats = test_feats.drop(columns=nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:26<00:00,  2.61s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13850.99it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12038.60it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 11893.51it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12419.10it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 11896.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:417: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:419: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 24.57it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 31536.12it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 37786.52it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 33554.43it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 35951.18it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22878.02it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:417: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:418: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:419: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m6_feats_comb.py:420: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    }
   ],
   "source": [
    "from m6_feats_comb import Preprocessor\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats_ = preprocessor.make_feats(train_logs)\n",
    "test_feats_ = preprocessor.make_feats(test_logs)\n",
    "nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n",
    "train_feats_ = train_feats.drop(columns=nan_cols)\n",
    "test_feats_ = test_feats.drop(columns=nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.equals(train_feats_), test_feats.equals(test_feats_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:04<00:00, 546.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2463.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sentence features for train dataset\n",
    "AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "\n",
    "def split_essays_into_sentences(df):\n",
    "    essay_df = df\n",
    "    #essay_df['id'] = essay_df.index\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(df):\n",
    "    sent_agg_df = pd.concat(\n",
    "        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    )\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def split_essays_into_paragraphs(df):\n",
    "    essay_df = df\n",
    "    #essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(df):\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "train_essays = getEssays(train_logs)\n",
    "train_sent_df = split_essays_into_sentences(train_essays)\n",
    "train_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n",
    "\n",
    "# Features for test dataset\n",
    "test_essays = getEssays(test_logs)\n",
    "test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from m6_feats_comb import compute_sentence_aggregations\n",
    "train_sent_ = compute_sentence_aggregations(train_essays)\n",
    "test_sent_ = compute_sentence_aggregations(test_essays)\n",
    "\n",
    "train_sent_agg_df.equals(train_sent_), test_sent_agg_df.equals(test_sent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_essays_into_paragraphs(df):\n",
    "    essay_df = df\n",
    "    #essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(df):\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "\n",
    "train_paragraph_df = split_essays_into_paragraphs(train_essays)\n",
    "train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n",
    "test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from m6_feats_comb import compute_paragraph_aggregations\n",
    "train_par = compute_paragraph_aggregations(train_essays)\n",
    "test_par = compute_paragraph_aggregations(test_essays)\n",
    "\n",
    "train_paragraph_agg_df.equals(train_par), test_paragraph_agg_df.equals(test_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for creating these features comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "# Idea is based on features introduced in Section 3 of this research paper: https://files.eric.ed.gov/fulltext/ED592674.pdf\n",
    "\n",
    "data = []\n",
    "\n",
    "for logs in [train_logs, test_logs]:\n",
    "    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "    group = logs.groupby('id')['time_diff']\n",
    "    largest_lantency = group.max()\n",
    "    smallest_lantency = group.min()\n",
    "    median_lantency = group.median()\n",
    "    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "\n",
    "    data.append(pd.DataFrame({\n",
    "        'id': logs['id'].unique(),\n",
    "        'largest_lantency': largest_lantency,\n",
    "        'smallest_lantency': smallest_lantency,\n",
    "        'median_lantency': median_lantency,\n",
    "        'initial_pause': initial_pause,\n",
    "        'pauses_half_sec': pauses_half_sec,\n",
    "        'pauses_1_sec': pauses_1_sec,\n",
    "        'pauses_1_half_sec': pauses_1_half_sec,\n",
    "        'pauses_2_sec': pauses_2_sec,\n",
    "        'pauses_3_sec': pauses_3_sec,\n",
    "    }).reset_index(drop=True))\n",
    "\n",
    "train_eD592674, test_eD592674 = data\n",
    "\n",
    "train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "tr_feats_ = pauses_feats(train_logs)\n",
    "ts_feats_ = pauses_feats(test_logs)\n",
    "train_feats_ = train_feats_.merge(tr_feats_, on='id', how='left')\n",
    "test_feats_ = test_feats_.merge(ts_feats_, on='id', how='left')\n",
    "train_feats_ = train_feats_.merge(train_scores, on='id', how='left')\n",
    "\n",
    "train_feats.equals(train_feats_), test_feats.equals(test_feats_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for additional aggregations comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "\n",
    "train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n",
    "train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n",
    "train_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n",
    "test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n",
    "test_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')\n",
    "\n",
    "tr_feats = agg_fe_df(train_logs)\n",
    "ts_feats = agg_fe_df(test_logs)\n",
    "\n",
    "train_feats_ = train_feats_.merge(tr_feats, on='id', how='left')\n",
    "test_feats_ = test_feats_.merge(ts_feats, on='id', how='left')\n",
    "\n",
    "train_feats.equals(train_feats_), test_feats.equals(test_feats_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the additional features to the original feature set\n",
    "train_feats = train_feats.merge(train_sent_agg_df, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_paragraph_agg_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_sent_agg_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_paragraph_agg_df, on='id', how='left')\n",
    "\n",
    "train_feats_ = train_feats_.merge(train_sent_, on='id', how='left')\n",
    "test_feats_ = test_feats_.merge(test_sent_, on='id', how='left')\n",
    "\n",
    "train_feats_ = train_feats_.merge(train_par, on='id', how='left')\n",
    "test_feats_ = test_feats_.merge(test_par, on='id', how='left')\n",
    "\n",
    "\n",
    "train_feats.equals(train_feats_), test_feats.equals(test_feats_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 0.590619\n",
      "[200]\tvalid_0's rmse: 0.5901\n",
      "[300]\tvalid_0's rmse: 0.589689\n",
      "[400]\tvalid_0's rmse: 0.592255\n",
      "[100]\tvalid_0's rmse: 0.514657\n",
      "[200]\tvalid_0's rmse: 0.51532\n",
      "[300]\tvalid_0's rmse: 0.5149\n",
      "[100]\tvalid_0's rmse: 0.691645\n",
      "[200]\tvalid_0's rmse: 0.69192\n",
      "[100]\tvalid_0's rmse: 0.641919\n",
      "[200]\tvalid_0's rmse: 0.638459\n",
      "[300]\tvalid_0's rmse: 0.638128\n",
      "[100]\tvalid_0's rmse: 0.606788\n",
      "[200]\tvalid_0's rmse: 0.604338\n",
      "[300]\tvalid_0's rmse: 0.604057\n",
      "[100]\tvalid_0's rmse: 0.616534\n",
      "[200]\tvalid_0's rmse: 0.617657\n",
      "[300]\tvalid_0's rmse: 0.615856\n",
      "[100]\tvalid_0's rmse: 0.669932\n",
      "[200]\tvalid_0's rmse: 0.668427\n",
      "[300]\tvalid_0's rmse: 0.668694\n",
      "[100]\tvalid_0's rmse: 0.647304\n",
      "[200]\tvalid_0's rmse: 0.652896\n",
      "[100]\tvalid_0's rmse: 0.638899\n",
      "[200]\tvalid_0's rmse: 0.636809\n",
      "[300]\tvalid_0's rmse: 0.640047\n",
      "[100]\tvalid_0's rmse: 0.568653\n",
      "[200]\tvalid_0's rmse: 0.572998\n",
      "[100]\tvalid_0's rmse: 0.608902\n",
      "[200]\tvalid_0's rmse: 0.61036\n",
      "[300]\tvalid_0's rmse: 0.613244\n",
      "[100]\tvalid_0's rmse: 0.603354\n",
      "[200]\tvalid_0's rmse: 0.61099\n",
      "[100]\tvalid_0's rmse: 0.590413\n",
      "[200]\tvalid_0's rmse: 0.586989\n",
      "[300]\tvalid_0's rmse: 0.59016\n",
      "[100]\tvalid_0's rmse: 0.609835\n",
      "[200]\tvalid_0's rmse: 0.616494\n",
      "[100]\tvalid_0's rmse: 0.622037\n",
      "[200]\tvalid_0's rmse: 0.629439\n",
      "[100]\tvalid_0's rmse: 0.640069\n",
      "[200]\tvalid_0's rmse: 0.642844\n",
      "[100]\tvalid_0's rmse: 0.669271\n",
      "[200]\tvalid_0's rmse: 0.661079\n",
      "[300]\tvalid_0's rmse: 0.660125\n",
      "[400]\tvalid_0's rmse: 0.66134\n",
      "[500]\tvalid_0's rmse: 0.659766\n",
      "[100]\tvalid_0's rmse: 0.597505\n",
      "[200]\tvalid_0's rmse: 0.588681\n",
      "[300]\tvalid_0's rmse: 0.591688\n",
      "[400]\tvalid_0's rmse: 0.593897\n",
      "[100]\tvalid_0's rmse: 0.664843\n",
      "[200]\tvalid_0's rmse: 0.667897\n",
      "[300]\tvalid_0's rmse: 0.668617\n",
      "[100]\tvalid_0's rmse: 0.642403\n",
      "[200]\tvalid_0's rmse: 0.63531\n",
      "[300]\tvalid_0's rmse: 0.635737\n",
      "[400]\tvalid_0's rmse: 0.635527\n",
      "[500]\tvalid_0's rmse: 0.637578\n",
      "[100]\tvalid_0's rmse: 0.689714\n",
      "[200]\tvalid_0's rmse: 0.688768\n",
      "[300]\tvalid_0's rmse: 0.690952\n",
      "[400]\tvalid_0's rmse: 0.691204\n",
      "[100]\tvalid_0's rmse: 0.598277\n",
      "[200]\tvalid_0's rmse: 0.600714\n",
      "[100]\tvalid_0's rmse: 0.596835\n",
      "[200]\tvalid_0's rmse: 0.596508\n",
      "[300]\tvalid_0's rmse: 0.600679\n",
      "[100]\tvalid_0's rmse: 0.609852\n",
      "[200]\tvalid_0's rmse: 0.604623\n",
      "[300]\tvalid_0's rmse: 0.601577\n",
      "[400]\tvalid_0's rmse: 0.602936\n",
      "[100]\tvalid_0's rmse: 0.592582\n",
      "[200]\tvalid_0's rmse: 0.591664\n",
      "[300]\tvalid_0's rmse: 0.595605\n",
      "[100]\tvalid_0's rmse: 0.632643\n",
      "[200]\tvalid_0's rmse: 0.631808\n",
      "[300]\tvalid_0's rmse: 0.633637\n",
      "[100]\tvalid_0's rmse: 0.695029\n",
      "[200]\tvalid_0's rmse: 0.69179\n",
      "[300]\tvalid_0's rmse: 0.694068\n",
      "[100]\tvalid_0's rmse: 0.569487\n",
      "[200]\tvalid_0's rmse: 0.584259\n",
      "[100]\tvalid_0's rmse: 0.651327\n",
      "[200]\tvalid_0's rmse: 0.653267\n",
      "[100]\tvalid_0's rmse: 0.60607\n",
      "[200]\tvalid_0's rmse: 0.605\n",
      "[300]\tvalid_0's rmse: 0.608948\n",
      "[100]\tvalid_0's rmse: 0.624736\n",
      "[200]\tvalid_0's rmse: 0.62898\n",
      "[300]\tvalid_0's rmse: 0.628374\n",
      "[100]\tvalid_0's rmse: 0.612889\n",
      "[200]\tvalid_0's rmse: 0.605569\n",
      "[300]\tvalid_0's rmse: 0.601319\n",
      "[400]\tvalid_0's rmse: 0.60037\n",
      "[500]\tvalid_0's rmse: 0.599864\n",
      "[600]\tvalid_0's rmse: 0.599302\n",
      "[700]\tvalid_0's rmse: 0.598874\n",
      "[800]\tvalid_0's rmse: 0.599364\n",
      "[900]\tvalid_0's rmse: 0.599855\n",
      "[100]\tvalid_0's rmse: 0.644386\n",
      "[200]\tvalid_0's rmse: 0.642377\n",
      "[300]\tvalid_0's rmse: 0.645266\n",
      "[100]\tvalid_0's rmse: 0.626261\n",
      "[200]\tvalid_0's rmse: 0.630213\n",
      "[300]\tvalid_0's rmse: 0.633935\n",
      "[100]\tvalid_0's rmse: 0.650441\n",
      "[200]\tvalid_0's rmse: 0.648966\n",
      "[300]\tvalid_0's rmse: 0.649516\n",
      "[100]\tvalid_0's rmse: 0.578961\n",
      "[200]\tvalid_0's rmse: 0.580338\n",
      "[100]\tvalid_0's rmse: 0.638193\n",
      "[200]\tvalid_0's rmse: 0.630724\n",
      "[300]\tvalid_0's rmse: 0.633144\n",
      "[400]\tvalid_0's rmse: 0.636398\n",
      "[100]\tvalid_0's rmse: 0.629463\n",
      "[200]\tvalid_0's rmse: 0.634589\n",
      "[100]\tvalid_0's rmse: 0.679821\n",
      "[200]\tvalid_0's rmse: 0.675923\n",
      "[300]\tvalid_0's rmse: 0.679435\n",
      "[100]\tvalid_0's rmse: 0.558522\n",
      "[200]\tvalid_0's rmse: 0.561374\n",
      "[100]\tvalid_0's rmse: 0.612102\n",
      "[200]\tvalid_0's rmse: 0.62201\n",
      "[100]\tvalid_0's rmse: 0.583265\n",
      "[200]\tvalid_0's rmse: 0.582505\n",
      "[300]\tvalid_0's rmse: 0.582334\n",
      "[400]\tvalid_0's rmse: 0.582719\n",
      "[100]\tvalid_0's rmse: 0.646001\n",
      "[200]\tvalid_0's rmse: 0.648822\n",
      "[300]\tvalid_0's rmse: 0.648273\n",
      "[100]\tvalid_0's rmse: 0.629088\n",
      "[200]\tvalid_0's rmse: 0.629846\n",
      "[300]\tvalid_0's rmse: 0.630675\n",
      "[100]\tvalid_0's rmse: 0.588861\n",
      "[200]\tvalid_0's rmse: 0.58696\n",
      "[300]\tvalid_0's rmse: 0.583251\n",
      "[400]\tvalid_0's rmse: 0.583554\n",
      "[500]\tvalid_0's rmse: 0.583596\n",
      "[100]\tvalid_0's rmse: 0.677723\n",
      "[200]\tvalid_0's rmse: 0.67631\n",
      "[300]\tvalid_0's rmse: 0.677735\n",
      "[400]\tvalid_0's rmse: 0.67866\n",
      "[100]\tvalid_0's rmse: 0.618387\n",
      "[200]\tvalid_0's rmse: 0.624962\n",
      "[100]\tvalid_0's rmse: 0.628225\n",
      "[200]\tvalid_0's rmse: 0.625998\n",
      "[300]\tvalid_0's rmse: 0.628134\n",
      "[100]\tvalid_0's rmse: 0.639818\n",
      "[200]\tvalid_0's rmse: 0.640983\n",
      "[300]\tvalid_0's rmse: 0.640976\n",
      "[400]\tvalid_0's rmse: 0.641022\n",
      "[100]\tvalid_0's rmse: 0.630241\n",
      "[200]\tvalid_0's rmse: 0.622926\n",
      "[300]\tvalid_0's rmse: 0.62067\n",
      "[400]\tvalid_0's rmse: 0.620023\n",
      "[500]\tvalid_0's rmse: 0.6192\n",
      "[600]\tvalid_0's rmse: 0.620209\n"
     ]
    }
   ],
   "source": [
    "# Code comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n",
    "from sklearn import model_selection\n",
    "\n",
    "target_col = ['score']\n",
    "drop_cols = ['id']\n",
    "train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]\n",
    "\n",
    "models_dict = {}\n",
    "scores = []\n",
    "\n",
    "test_predict_list = []\n",
    "best_params = {'reg_alpha': 0.007678095440286993, \n",
    "               'reg_lambda': 0.34230534302168353, \n",
    "               'colsample_bytree': 0.627061253588415, \n",
    "               'subsample': 0.854942238828458, \n",
    "               'learning_rate': 0.038697981947473245, \n",
    "               'num_leaves': 22, \n",
    "               'max_depth': 37, \n",
    "               'min_child_samples': 18}\n",
    "\n",
    "for i in range(5): \n",
    "    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n",
    "    oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "    X_test = test_feats[train_cols]\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "        \n",
    "        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            'random_state': 42,\n",
    "            \"n_estimators\" : 12001,\n",
    "            \"verbosity\": -1,\n",
    "            **best_params\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n",
    "        verbose_callback = lgb.log_evaluation(100)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n",
    "                  callbacks=[early_stopping_callback, verbose_callback],\n",
    "        )\n",
    "        valid_predict = model.predict(X_valid)\n",
    "        oof_valid_preds[valid_idx] = valid_predict\n",
    "        test_predict = model.predict(X_test)\n",
    "        test_predict_list.append(test_predict)\n",
    "        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "        models_dict[f'{fold}_{i}'] = model\n",
    "\n",
    "    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n",
    "    scores.append(oof_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Average RMSE over 50 folds: 0.614785\n",
      "Mean RMSE of all iterations: 0.620711\n"
     ]
    }
   ],
   "source": [
    "def calculate_rmse(df):\n",
    "    return mean_squared_error(df['score'], df['prediction'], squared=False)\n",
    "\n",
    "def run_lgb_cv(train_feats, test_feats, train_cols, target_col, lgb_params, boosting_type, seed, n_repeats, n_splits):\n",
    "\n",
    "    oof_results = pd.DataFrame(columns = ['id', 'iteration', 'score', 'prediction'])\n",
    "\n",
    "    X = train_feats[train_cols]\n",
    "    y = train_feats[target_col]\n",
    "    X_test = test_feats[train_cols]\n",
    "    test_preds = []\n",
    "\n",
    "    for i in range(n_repeats):\n",
    "        skf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=seed + i)\n",
    "\n",
    "        for train_idx, valid_idx in skf.split(train_feats, y):\n",
    "            X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "            X_valid, y_valid = X.loc[valid_idx], y.loc[valid_idx]\n",
    "\n",
    "            model = lgb.LGBMRegressor(**lgb_params, verbose=-1, random_state=seed)\n",
    "            if boosting_type != 'dart':\n",
    "                model.fit(X_train, y_train, \n",
    "                        eval_set=[(X_valid, y_valid)], \n",
    "                        callbacks=[lgb.early_stopping(200, first_metric_only=True, verbose=False)])\n",
    "            else:\n",
    "                model.fit(X_train, y_train)  # No early stopping for DART\n",
    "\n",
    "            valid_predictions = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "            test_predictions = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            test_preds.append(test_predictions)\n",
    "        \n",
    "            tmp_df = train_feats.loc[valid_idx][['id','score']]\n",
    "            tmp_df['prediction'] = valid_predictions\n",
    "            tmp_df['iteration'] = i + 1\n",
    "            oof_results = pd.concat([oof_results, tmp_df])\n",
    "\n",
    "    avg_preds = oof_results.groupby(['id','score'])['prediction'].mean().reset_index()\n",
    "    rmse = mean_squared_error(avg_preds['score'], avg_preds['prediction'], squared=False)\n",
    "    print(f\"LGBM Average RMSE over {n_repeats * n_splits} folds: {rmse:.6f}\")\n",
    "    return test_preds, oof_results, rmse, model_lgb  \n",
    "\n",
    "def cv_pipeline(train_feats, test_feats, lgb_params, boosting_type, seed=42, n_repeats=5, n_splits=10):\n",
    "\n",
    "    target_col = ['score']\n",
    "    drop_cols = ['id']\n",
    "    train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]\n",
    "\n",
    "    missing_cols = [col for col in train_cols if col not in test_feats.columns]\n",
    "    missing_cols_df = pd.DataFrame({col: np.nan for col in missing_cols}, index=test_feats.index)\n",
    "    test_feats = pd.concat([test_feats, missing_cols_df], axis=1)\n",
    "\n",
    "\n",
    "    test_preds, oof_preds, rmse, model = run_lgb_cv(train_feats=train_feats, test_feats=test_feats, \n",
    "                                             train_cols=train_cols, target_col=target_col, \n",
    "                                             lgb_params=lgb_params, boosting_type=boosting_type,\n",
    "                                             seed=seed, n_repeats=n_repeats, n_splits=n_splits)\n",
    "    \n",
    "    rmse_per_iteration = oof_preds.groupby('iteration').apply(calculate_rmse)\n",
    "    print(f'Mean RMSE of all iterations: {np.mean(rmse_per_iteration):.6f}')\n",
    "\n",
    "    return test_preds, oof_preds, rmse, model\n",
    "\n",
    "params = {\n",
    "        'reg_alpha': 0.007678095440286993, \n",
    "        'reg_lambda': 0.34230534302168353, \n",
    "        'colsample_bytree': 0.627061253588415, \n",
    "        'subsample': 0.854942238828458, \n",
    "        'learning_rate': 0.038697981947473245, \n",
    "        'num_leaves': 22, \n",
    "        'max_depth': 37, \n",
    "        'min_child_samples': 18,\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"n_estimators\" : 12001,\n",
    "            }\n",
    "\n",
    "test_preds, oof_preds, rmse, model = cv_pipeline(train_feats, test_feats, params, 'gbdt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
