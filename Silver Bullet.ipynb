{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56512d1a",
   "metadata": {
    "papermill": {
     "duration": 0.004254,
     "end_time": "2023-12-06T06:43:52.235251",
     "exception": false,
     "start_time": "2023-12-06T06:43:52.230997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3303c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T06:43:52.246483Z",
     "iopub.status.busy": "2023-12-06T06:43:52.246011Z",
     "iopub.status.idle": "2023-12-06T06:43:56.282602Z",
     "shell.execute_reply": "2023-12-06T06:43:56.281389Z"
    },
    "papermill": {
     "duration": 4.045816,
     "end_time": "2023-12-06T06:43:56.285512",
     "exception": false,
     "start_time": "2023-12-06T06:43:52.239696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "# train_feats   = dev_feats(train_logs)\n",
    "# train_feats   = train_feats.collect().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f19b0",
   "metadata": {
    "papermill": {
     "duration": 0.004184,
     "end_time": "2023-12-06T06:43:56.294210",
     "exception": false,
     "start_time": "2023-12-06T06:43:56.290026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Polars FE & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc58732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T06:43:56.305392Z",
     "iopub.status.busy": "2023-12-06T06:43:56.304976Z",
     "iopub.status.idle": "2023-12-06T06:43:56.352851Z",
     "shell.execute_reply": "2023-12-06T06:43:56.351555Z"
    },
    "papermill": {
     "duration": 0.05663,
     "end_time": "2023-12-06T06:43:56.355334",
     "exception": false,
     "start_time": "2023-12-06T06:43:56.298704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    print(\"< Numerical columns features >\")\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "    \n",
    "    print(\"< Idle time features >\")\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "    x_train = data_x.iloc[train_idx]\n",
    "    y_train = data_y[train_idx]\n",
    "    x_valid = data_x.iloc[valid_idx]\n",
    "    y_valid = data_y[valid_idx]\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
    "    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n",
    "\n",
    "    valid_preds = pd.DataFrame()\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # valid\n",
    "        valid_preds = pd.concat([valid_x[['score','id']]])\n",
    "        valid_preds['preds'] = model.predict(valid_x)\n",
    "    \n",
    "        test_y[:, i] = model.predict(test_x)\n",
    "\n",
    "    return valid_preds, test_y\n",
    "\n",
    "def calculate_rmse(y, yhat):\n",
    "    return mean_squared_error(y, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f30406",
   "metadata": {
    "papermill": {
     "duration": 0.004928,
     "end_time": "2023-12-06T06:43:56.365164",
     "exception": false,
     "start_time": "2023-12-06T06:43:56.360236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pandas FE & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4388e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T06:43:56.377351Z",
     "iopub.status.busy": "2023-12-06T06:43:56.376884Z",
     "iopub.status.idle": "2023-12-06T06:43:56.414869Z",
     "shell.execute_reply": "2023-12-06T06:43:56.413655Z"
    },
    "papermill": {
     "duration": 0.047589,
     "end_time": "2023-12-06T06:43:56.417466",
     "exception": false,
     "start_time": "2023-12-06T06:43:56.369877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813b7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    }
   ],
   "source": [
    "from m7_utils import preprocess_feats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = get_essay_df(test_logs)\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee24a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATS_DIR = 'silver_bullet'\n",
    "# \n",
    "# train_feats.to_pickle(f'{FEATS_DIR}/train_feats.pkl')\n",
    "# test_feats.to_pickle(f'{FEATS_DIR}/test_feats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b800b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS_DIR = 'silver_bullet'\n",
    " \n",
    "train_feats= pd.read_pickle(f'{FEATS_DIR}/train_feats.pkl')\n",
    "test_feats = pd.read_pickle(f'{FEATS_DIR}/test_feats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2cb414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def word_change_stats(train_logs, test_logs):\n",
    "\n",
    "    data = []\n",
    "    for logs in [train_logs, test_logs]:\n",
    "\n",
    "        logs = logs.with_columns(\n",
    "            pl.col('word_count').diff().over('id').fill_nan(0).alias('wc_diff')\n",
    "        )\n",
    "\n",
    "        pref = 'wc_diff'\n",
    "\n",
    "        x = logs.group_by('id').agg([\n",
    "            pl.col('wc_diff').mean().alias(f'{pref}_mean'),\n",
    "            pl.col('wc_diff').std().alias(f'{pref}_std'),\n",
    "            pl.col('wc_diff').kurtosis().alias(f'{pref}_kurt'),\n",
    "            pl.col('wc_diff').skew().alias(f'{pref}_skew')\n",
    "        ])\n",
    "        data.append(x)\n",
    "\n",
    "    return data[0].collect().to_pandas(), data[1].collect().to_pandas()\n",
    "\n",
    "train_logs   = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "\n",
    "tr_feats, ts_feats = word_change_stats(train_logs, test_logs)\n",
    "\n",
    "\n",
    "train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "test_feats = test_feats.merge(ts_feats, on='id', how='left') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9310f3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" from sklearn.feature_extraction.text import CountVectorizer\\nfrom m4_feats_functions import getEssays\\n\\ndef countvectorize_one_one(train_logs, test_logs):\\n\\n    data = []\\n\\n    for logs in [train_logs, test_logs]:\\n\\n        ids = logs.id.unique()\\n        essays = getEssays(logs)\\n        c_vect = CountVectorizer(ngram_range=(1, 1))\\n        toks = c_vect.fit_transform(essays['essay']).todense()\\n        toks = toks[:,:16]\\n        toks_df = pd.DataFrame(columns = [f'tok_{i}' for i in range(toks.shape[1])], data=toks)\\n        toks_df['id'] = ids\\n        toks_df.reset_index(drop=True, inplace=True)\\n        data.append(toks_df)\\n\\n    return data[0], data[1]\\n\\ntest_logs   = pl.scan_csv(data_path + 'test_logs.csv').collect().to_pandas()\\ntr_feats, ts_feats = countvectorize_one_one(train_logs, test_logs)\\n\\ntrain_feats = train_feats.merge(tr_feats, on='id', how='left')\\ntest_feats = test_feats.merge(ts_feats, on='id', how='left')\\nmissing_cols = set(tr_feats.columns) - set(ts_feats.columns)\\n\\nfor col in missing_cols:\\n    test_feats[col] = np.nan \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.feature_extraction.text import CountVectorizer\n",
    "from m4_feats_functions import getEssays\n",
    "\n",
    "def countvectorize_one_one(train_logs, test_logs):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for logs in [train_logs, test_logs]:\n",
    "\n",
    "        ids = logs.id.unique()\n",
    "        essays = getEssays(logs)\n",
    "        c_vect = CountVectorizer(ngram_range=(1, 1))\n",
    "        toks = c_vect.fit_transform(essays['essay']).todense()\n",
    "        toks = toks[:,:16]\n",
    "        toks_df = pd.DataFrame(columns = [f'tok_{i}' for i in range(toks.shape[1])], data=toks)\n",
    "        toks_df['id'] = ids\n",
    "        toks_df.reset_index(drop=True, inplace=True)\n",
    "        data.append(toks_df)\n",
    "\n",
    "    return data[0], data[1]\n",
    "\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv').collect().to_pandas()\n",
    "tr_feats, ts_feats = countvectorize_one_one(train_logs, test_logs)\n",
    "\n",
    "train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "missing_cols = set(tr_feats.columns) - set(ts_feats.columns)\n",
    "\n",
    "for col in missing_cols:\n",
    "    test_feats[col] = np.nan \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5badc745",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LazyFrame' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/Silver Bullet.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_action_time_gap_feats, test_action_time_gap_feats\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m test_logs   \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mscan_csv(data_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest_logs.csv\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcollect()\u001b[39m.\u001b[39mto_pandas()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m tr_feats, ts_feats \u001b[39m=\u001b[39m process_feats_action_time_gap(train_logs, test_logs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m train_feats \u001b[39m=\u001b[39m train_feats\u001b[39m.\u001b[39mmerge(tr_feats, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m test_feats \u001b[39m=\u001b[39m test_feats\u001b[39m.\u001b[39mmerge(ts_feats, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/Silver Bullet.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     grouped \u001b[39m=\u001b[39m action_time_gap_df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39maction_time_gap\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grouped\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: diverse_stats(x, \u001b[39m'\u001b[39m\u001b[39maction_time_gap\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m train_action_time_gap_feats \u001b[39m=\u001b[39m calc_action_time_gap(train_logs)\u001b[39m.\u001b[39mpivot(index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, columns\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel_1\u001b[39m\u001b[39m'\u001b[39m, values\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maction_time_gap\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m test_action_time_gap_feats \u001b[39m=\u001b[39m calc_action_time_gap(test_logs)\u001b[39m.\u001b[39mpivot(index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, columns\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel_1\u001b[39m\u001b[39m'\u001b[39m, values\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maction_time_gap\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_action_time_gap_feats, test_action_time_gap_feats\n",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/Silver Bullet.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalc_action_time_gap\u001b[39m(comb_df):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     action_time_gap_df \u001b[39m=\u001b[39m comb_df\u001b[39m.\u001b[39;49mcopy()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     action_time_gap_df[\u001b[39m'\u001b[39m\u001b[39mup_time_shift1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m action_time_gap_df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mup_time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshift(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/Silver%20Bullet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     action_time_gap_df[\u001b[39m'\u001b[39m\u001b[39maction_time_gap\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m action_time_gap_df[\u001b[39m'\u001b[39m\u001b[39mdown_time\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m action_time_gap_df[\u001b[39m'\u001b[39m\u001b[39mup_time_shift1\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LazyFrame' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "\"\"\" # WORD STATS ON ESSAY\n",
    "def diverse_stats(series, prefix):\n",
    "    if isinstance(series, list):\n",
    "        series = pd.Series([len(item) for item in series])\n",
    "\n",
    "    stats = {\n",
    "        # f'{prefix}_count': series.count(),\n",
    "        f'{prefix}_mean': series.mean(),\n",
    "        f'{prefix}_std': series.std(),\n",
    "        f'{prefix}_max': series.max(),\n",
    "        f'{prefix}_median': series.median(),\n",
    "        # f'{prefix}_sum': series.sum(),\n",
    "        # f'{prefix}_last': series.iloc[-1] if not series.empty else None,\n",
    "        # f'{prefix}_q1': series.quantile(0.25),\n",
    "        # f'{prefix}_q3': series.quantile(0.75),\n",
    "        # f'{prefix}_iqr': series.quantile(0.75) - series.quantile(0.25),\n",
    "        # f'{prefix}_min': series.min(),\n",
    "        # f'{prefix}_first': series.iloc[0] if not series.empty else None,\n",
    "        # f'{prefix}_sem': series.sem(),\n",
    "        f'{prefix}_skew': series.skew(),\n",
    "        f'{prefix}_kurt': series.kurtosis(),\n",
    "        # f'{prefix}_range': series.max() - series.min(),\n",
    "    }\n",
    "    return pd.Series(stats)\n",
    "\n",
    "\n",
    "def process_feats_action_time_gap(train_logs, test_logs):\n",
    "    def calc_action_time_gap(comb_df):\n",
    "        action_time_gap_df = comb_df.copy()\n",
    "        action_time_gap_df['up_time_shift1'] = action_time_gap_df.groupby('id')['up_time'].shift(1)\n",
    "        action_time_gap_df['action_time_gap'] = action_time_gap_df['down_time'] - action_time_gap_df['up_time_shift1']\n",
    "\n",
    "        grouped = action_time_gap_df.groupby('id')['action_time_gap']\n",
    "        return grouped.apply(lambda x: diverse_stats(x, 'action_time_gap')).reset_index()\n",
    "\n",
    "    train_action_time_gap_feats = calc_action_time_gap(train_logs).pivot(index='id', columns='level_1', values='action_time_gap').reset_index()\n",
    "    test_action_time_gap_feats = calc_action_time_gap(test_logs).pivot(index='id', columns='level_1', values='action_time_gap').reset_index()\n",
    "\n",
    "    return train_action_time_gap_feats, test_action_time_gap_feats\n",
    "\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv').collect().to_pandas()\n",
    "tr_feats, ts_feats = process_feats_action_time_gap(train_logs, test_logs)\n",
    "\n",
    "train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "missing_cols = set(tr_feats.columns) - set(ts_feats.columns)\n",
    "\n",
    "for col in missing_cols:\n",
    "    test_feats[col] = np.nan \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f85ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\\nFEAT_STORE_DIR = 'feat_store_combined'\\ntrain_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\\ntrain_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\\ntest_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv') \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from m5_sb_models import lgb_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "FEATS_DIR = 'silver_bullet'\n",
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "\n",
    "train_feats= pd.read_pickle(f'{FEATS_DIR}/train_feats.pkl')\n",
    "test_feats = pd.read_pickle(f'{FEATS_DIR}/test_feats.pkl')\n",
    "\n",
    "\"\"\" INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\n",
    "FEAT_STORE_DIR = 'feat_store_combined'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv') \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569b45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def rate_of_change_feats(train_logs, test_logs):\n",
    "\n",
    "    feats = []\n",
    "    time_agg = 10\n",
    "    for logs in [train_logs, test_logs]:\n",
    "\n",
    "        # Helper functions\n",
    "        def q1(x):\n",
    "            return x.quantile(0.25)\n",
    "        def q3(x):\n",
    "            return x.quantile(0.75)\n",
    "\n",
    "        AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "\n",
    "        df = train_logs[['id', 'down_time', 'word_count']].copy()\n",
    "        df['down_time_sec'] = df['down_time'] / 1000\n",
    "        df['time_bin'] = df['down_time_sec'].apply(lambda x: time_agg * (x // time_agg))\n",
    "        grp_df = df.groupby(['id', 'time_bin'])['word_count'].max().reset_index()\n",
    "\n",
    "        word_count_diff = grp_df.groupby('id')['word_count'].diff().fillna(0)\n",
    "        time_bin_diff = grp_df.groupby('id')['time_bin'].diff().fillna(0)\n",
    "        grp_df['rate_of_change'] = (word_count_diff / time_bin_diff).fillna(0)\n",
    "\n",
    "        stats = grp_df[['id','rate_of_change']].groupby(['id']).agg(AGGREGATIONS)\n",
    "        stats.columns = ['_'.join(x) for x in stats.columns]\n",
    "        feats.append(stats)\n",
    "\n",
    "    return feats[0], feats[1]\n",
    "\n",
    "train_roc, test_roc = rate_of_change_feats(train_logs, test_logs)\n",
    "\n",
    "train_feats = train_feats.merge(train_roc, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_roc, on='id', how='left') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4574fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Mapping >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE over 50: 0.616633. Std 0.8211\n",
      "RMSE by fold 0.616477. Std 0.013895680767961415\n"
     ]
    }
   ],
   "source": [
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv(data_path + 'train_scores.csv')\n",
    "train = train_feats.merge(train_scores, on='id', how='left')\n",
    "test = test_feats.copy()\n",
    "\n",
    "test_preds, valid_preds, final_rmse, cv_rmse  = lgb_pipeline(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1aa54",
   "metadata": {},
   "source": [
    "Final RMSE over 50: 0.616986. Std 0.8204\n",
    "RMSE by fold 0.616855. Std 0.012827626391203101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb35d1",
   "metadata": {
    "papermill": {
     "duration": 0.004305,
     "end_time": "2023-12-06T06:43:56.426590",
     "exception": false,
     "start_time": "2023-12-06T06:43:56.422285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6934ff20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T06:43:56.438293Z",
     "iopub.status.busy": "2023-12-06T06:43:56.437798Z",
     "iopub.status.idle": "2023-12-06T06:46:44.581107Z",
     "shell.execute_reply": "2023-12-06T06:46:44.579963Z"
    },
    "papermill": {
     "duration": 168.15235,
     "end_time": "2023-12-06T06:46:44.583943",
     "exception": false,
     "start_time": "2023-12-06T06:43:56.431593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 165\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Learning and Evaluation >\n"
     ]
    }
   ],
   "source": [
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv(data_path + 'train_scores.csv')\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = get_essay_df(test_logs)\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "test_ids = test_feats['id'].values\n",
    "testin_x = test_feats.drop(['id'], axis=1)\n",
    "\n",
    "print('< Learning and Evaluation >')\n",
    "param = {'n_estimators': 1024,\n",
    "         'learning_rate': 0.005,\n",
    "         'metric': 'rmse',\n",
    "         'random_state': 42,\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 0,}\n",
    "solution = LGBMRegressor(**param)\n",
    "y_pred   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) \n",
    "\n",
    "sub = pd.DataFrame({'id': test_ids, 'score': y_pred})\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.983197,
   "end_time": "2023-12-06T06:46:45.642128",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-06T06:43:48.658931",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
