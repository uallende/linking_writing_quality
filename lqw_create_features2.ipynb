{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m4_kaggle_feats import *\n",
    "from m5_sb_models import lgb_pipeline\n",
    "import polars as pl\n",
    "import os, warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_STORE = 'feature_store'\n",
    "\n",
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n",
    "test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n",
    "train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')\n",
    "train_scores = train_scores.collect().to_pandas()\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'metric': 'rmse',\n",
    "    'reg_alpha': 0.0031, \n",
    "    'reg_lambda': 0.001, \n",
    "    'colsample_bytree': 0.8,  \n",
    "    'subsample_freq': 1,  \n",
    "    'subsample': 0.75,  \n",
    "    'learning_rate': 0.017, \n",
    "    'num_leaves': 19, \n",
    "    'min_child_samples': 46,\n",
    "    'n_estimators': 250,\n",
    "    'verbosity': -1\n",
    "    }\n",
    "\n",
    "xgb_param={\n",
    "'reg_alpha': 0.00087,\n",
    "'reg_lambda': 2.5428,\n",
    "'colsample_bynode': 0.78390,\n",
    "'subsample': 0.89942, \n",
    "'eta': 0.04730, \n",
    "'max_depth': 3, \n",
    "'n_estimators': 350,\n",
    "'eval_metric': 'rmse',\n",
    "'device': 'cuda',\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 1000, \n",
    "    'learning_rate': 0.1, \n",
    "    'depth': 6, \n",
    "    'loss_function': 'RMSE', \n",
    "    'od_wait': 20, \n",
    "    'od_type': 'Iter', \n",
    "    'verbose': False, \n",
    "    'metric_period': 50, \n",
    "    'eval_metric': 'RMSE', \n",
    "    'bagging_temperature': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Events counts features >\n",
      "< punctuations features >\n",
      "< text chaanges counts features >\n",
      "< Input text change features >\n",
      "< Idle time features >\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "< Essays sentences feats >\n",
      "< Essays sentences feats >\n",
      "< Essays paragraphs feats >\n",
      "< Essays paragraphs feats >\n",
      "< Essays word feats >\n",
      "< Essays word feats >\n"
     ]
    }
   ],
   "source": [
    "file_name =  'down_events_counts'\n",
    "tr, ts = down_events_counts(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'punctuations'\n",
    "tr, ts = punctuations(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'text_changes_counts'\n",
    "tr, ts = text_changes_counts(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'count_of_activities'\n",
    "tr, ts = count_of_activities(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'input_text_change_feats'\n",
    "tr, ts = input_text_change_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'create_pauses'\n",
    "tr, ts = create_pauses(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'action_time_gap'\n",
    "tr, ts = action_time_gap(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'cursor_position_change_gap'\n",
    "tr, ts = cursor_position_change_gap(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'word_count_change_gap'\n",
    "tr, ts = word_count_change_gap(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'categorical_stats'\n",
    "tr, ts = categorical_stats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'action_time_stats'\n",
    "tr, ts = action_time_stats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'cursor_position_stats'\n",
    "tr, ts = cursor_position_stats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name =  'word_count_stats'\n",
    "tr, ts = word_count_stats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "\n",
    "train_essays = get_essay_df(train_logs.collect().to_pandas())\n",
    "test_essays = get_essay_df(test_logs.collect().to_pandas())\n",
    "file_name =  'word_feats'\n",
    "file_name = 'essay_sentences'\n",
    "tr = sent_feats(train_essays)\n",
    "ts = sent_feats(test_essays)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'essay_paragraphs'\n",
    "tr = parag_feats(train_essays)\n",
    "ts = parag_feats(test_essays)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'essay_words'\n",
    "tr = word_feats(train_essays)\n",
    "ts = word_feats(test_essays)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting round 0 of training feats\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/lqw_create_features2.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw_create_features2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     train_feats \u001b[39m=\u001b[39m train_feats\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw_create_features2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     test_feats \u001b[39m=\u001b[39m test_feats[train_feats\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw_create_features2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     test_preds, valid_preds, final_rmse, cv_rm \u001b[39m=\u001b[39m lgb_pipeline(train_feats, test_feats, lgb_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw_create_features2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     mean_rmse\u001b[39m.\u001b[39mappend(final_rmse)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/lqw_create_features2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining... \u001b[39m\u001b[39m{\u001b[39;00mtr_feats_cand\u001b[39m}\u001b[39;00m\u001b[39m. Score: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(mean_rmse)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Kaggle/linking-writing/m5_sb_models.py:54\u001b[0m, in \u001b[0;36mlgb_pipeline\u001b[0;34m(train, test, param, n_splits, iterations)\u001b[0m\n\u001b[1;32m     52\u001b[0m train_x, train_y, valid_x, valid_y \u001b[39m=\u001b[39m train_valid_split(x, y, train_index, valid_index)\n\u001b[1;32m     53\u001b[0m model \u001b[39m=\u001b[39m LGBMRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m \u001b[39m+\u001b[39m \u001b[39miter\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_x, train_y)\n\u001b[1;32m     56\u001b[0m valid_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(valid_x)\n\u001b[1;32m     57\u001b[0m test_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_x)\n",
      "File \u001b[0;32m~/miniconda3/envs/lwq/lib/python3.10/site-packages/lightgbm/sklearn.py:1092\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1077\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     init_model: Optional[Union[\u001b[39mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLGBMRegressor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1091\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1092\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1093\u001b[0m         X,\n\u001b[1;32m   1094\u001b[0m         y,\n\u001b[1;32m   1095\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1096\u001b[0m         init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[1;32m   1097\u001b[0m         eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m   1098\u001b[0m         eval_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m   1099\u001b[0m         eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[1;32m   1100\u001b[0m         eval_init_score\u001b[39m=\u001b[39;49meval_init_score,\n\u001b[1;32m   1101\u001b[0m         eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[1;32m   1102\u001b[0m         feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m   1103\u001b[0m         categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature,\n\u001b[1;32m   1104\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1105\u001b[0m         init_model\u001b[39m=\u001b[39;49minit_model\n\u001b[1;32m   1106\u001b[0m     )\n\u001b[1;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/lwq/lib/python3.10/site-packages/lightgbm/sklearn.py:885\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    882\u001b[0m evals_result: _EvalResultDict \u001b[39m=\u001b[39m {}\n\u001b[1;32m    883\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 885\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m    886\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    887\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[1;32m    888\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n\u001b[1;32m    889\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[1;32m    890\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m    891\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[1;32m    893\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    894\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m    895\u001b[0m )\n\u001b[1;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n\u001b[1;32m    898\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_best_iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster\u001b[39m.\u001b[39mbest_iteration\n",
      "File \u001b[0;32m~/miniconda3/envs/lwq/lib/python3.10/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[39m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lwq/lib/python3.10/site-packages/lightgbm/basic.py:3891\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3890\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3891\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3892\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   3893\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3895\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feat_list = os.listdir(FEAT_STORE)\n",
    "best_feats = []\n",
    "train_feats, test_feats = pd.DataFrame(), pd.DataFrame()\n",
    "best_rmse = float('inf')\n",
    "round = 0\n",
    "added_feats = []\n",
    "improved = True\n",
    "results = pd.DataFrame()\n",
    "list_train_feats = [feat for feat in feat_list if feat.startswith('train_')]\n",
    "\n",
    "while improved:\n",
    "    print(f'Starting round {round} of training feats')\n",
    "    improved = False\n",
    "    list_train_feats = [feat for feat in list_train_feats if feat not in added_feats]\n",
    "\n",
    "    for tr_feats_cand in list_train_feats:\n",
    "        ts_feats_cand = tr_feats_cand.replace('train', 'test')\n",
    "\n",
    "        tr_feats = pd.read_pickle(f'{FEAT_STORE}/{tr_feats_cand}')\n",
    "        ts_feats = pd.read_pickle(f'{FEAT_STORE}/{ts_feats_cand}')\n",
    "\n",
    "        existing_train_columns = set(train_feats.columns)\n",
    "        existing_test_columns = set(test_feats.columns)\n",
    "\n",
    "        if not(train_feats.empty & test_feats.empty):\n",
    "\n",
    "            train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "            test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "\n",
    "            if 'score' not in train_feats.columns:\n",
    "                train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "                \n",
    "            assert train_feats.shape[1] == test_feats.shape[1] + 1\n",
    "        else:\n",
    "            #print(f'feats empty - setting up train_feats')\n",
    "            train_feats = tr_feats.copy()\n",
    "            test_feats = ts_feats.copy()\n",
    "            train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "        \n",
    "        # print(f'Train feats cols {train_feats.columns}')\n",
    "        tr_cols = tr_feats.drop(columns=['id']).columns\n",
    "        ts_cols = ts_feats.drop(columns=['id']).columns\n",
    "        mean_rmse = []\n",
    "\n",
    "        for i in range(10):\n",
    "            train_feats = train_feats.sample(frac=1).reset_index(drop=True)\n",
    "            test_feats = test_feats[train_feats.drop('score',axis=1).columns]\n",
    "            test_preds, valid_preds, final_rmse, cv_rm = lgb_pipeline(train_feats, test_feats, lgb_params)\n",
    "            mean_rmse.append(final_rmse)\n",
    "\n",
    "        print(f'Training... {tr_feats_cand}. Score: {np.mean(mean_rmse):.4f}')\n",
    "        temp_res = {'feat_name': tr_feats_cand, 'RMSE': np.mean(mean_rmse)}\n",
    "        results = pd.concat([results, pd.DataFrame([temp_res])])\n",
    "\n",
    "        train_feats.drop(columns=tr_cols, inplace=True)\n",
    "        test_feats.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "    results = results.sort_values('RMSE', ascending=True)\n",
    "    top_score = results.head(1).RMSE.values[0]\n",
    "    top_feat = results.head(1).feat_name.values[0]\n",
    "\n",
    "    if top_score < best_rmse:\n",
    "        best_rmse = top_score\n",
    "        best_feat = top_feat\n",
    "        improved = True\n",
    "        print(f'Results improved!: Selected feat {top_feat} - score {top_score:.4f}')\n",
    "\n",
    "        ts_top_feat = top_feat.replace('train', 'test')\n",
    "        tr_feats = pd.read_pickle(f'{FEAT_STORE}/{top_feat}')\n",
    "        ts_feats = pd.read_pickle(f'{FEAT_STORE}/{ts_top_feat}')\n",
    "\n",
    "        if round > 0:\n",
    "            train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "            test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "        else:\n",
    "            train_feats = tr_feats\n",
    "            test_feats = ts_feats\n",
    "\n",
    "        added_feats.append(top_feat)\n",
    "        round += 1\n",
    "    else:\n",
    "        print('Training Over!')\n",
    "\n",
    "    list_train_feats = [feat for feat in list_train_feats if feat not in added_feats]\n",
    "    print(f'list_train_feats: {list_train_feats}')\n",
    "    print(f'added_feats_list: {added_feats}')\n",
    "    print(f'best feat: {top_feat}')\n",
    "\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Best Feature Set: {added_feats}\")\n",
    "best_feats.append(added_feats)\n",
    "added_feats = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
