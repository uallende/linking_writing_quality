{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m4_feats_polars import *\n",
    "from m5_sb_models import lgb_pipeline\n",
    "import polars as pl\n",
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_STORE = 'feature_store'\n",
    "data_path     = 'kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n",
    "test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n",
    "train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')\n",
    "train_scores = train_scores.collect().to_pandas()\n",
    "\n",
    "param = {'n_estimators': 1024,\n",
    "         'learning_rate': 0.005,\n",
    "         'metric': 'rmse',\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 0,}\n",
    "\n",
    "feat_list = os.listdir(FEAT_STORE)\n",
    "list_train_feats = [feat for feat in feat_list if feat.startswith('train')]\n",
    "#test_feats = [feat for feat in feat_list if feat.startswith('test')]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... train_event_id_stats.pkl\n",
      "Final RMSE over 50: 0.767516. Std 0.7233\n",
      "RMSE by fold 0.767447. Std 0.0103\n",
      "Training... train_categorical_nunique.pkl\n",
      "Final RMSE over 50: 0.907111. Std 0.5422\n",
      "RMSE by fold 0.906990. Std 0.0153\n",
      "Training... train_essay_sentences.pkl\n",
      "Final RMSE over 50: 0.666417. Std 0.8003\n",
      "RMSE by fold 0.666253. Std 0.0147\n",
      "Training... train_count_vectorise_bigrams.pkl\n",
      "Final RMSE over 50: 0.726059. Std 0.7497\n",
      "RMSE by fold 0.725895. Std 0.0156\n",
      "Training... train_action_time_by_activity.pkl\n",
      "Final RMSE over 50: 0.828619. Std 0.6658\n",
      "RMSE by fold 0.828407. Std 0.0184\n",
      "Training... train_word_counts_rate_of_change.pkl\n",
      "Final RMSE over 50: 0.711108. Std 0.7715\n",
      "RMSE by fold 0.710849. Std 0.0192\n",
      "Training... train_word_count_stats.pkl\n",
      "Final RMSE over 50: 0.734109. Std 0.7505\n",
      "RMSE by fold 0.734021. Std 0.0114\n",
      "Training... train_pauses.pkl\n",
      "Final RMSE over 50: 0.735167. Std 0.7415\n",
      "RMSE by fold 0.735035. Std 0.0136\n",
      "Training... train_events_counts.pkl\n",
      "Final RMSE over 50: 0.655458. Std 0.7958\n",
      "RMSE by fold 0.655358. Std 0.0114\n",
      "Training... train_input_change.pkl\n",
      "Final RMSE over 50: 0.756265. Std 0.7426\n",
      "RMSE by fold 0.756086. Std 0.0164\n",
      "Training... train_count_vectorise.pkl\n",
      "Final RMSE over 50: 0.669951. Std 0.7928\n",
      "RMSE by fold 0.669869. Std 0.0104\n",
      "Training... train_events_counts_rate_of_change.pkl\n",
      "Final RMSE over 50: 0.787355. Std 0.6968\n",
      "RMSE by fold 0.787234. Std 0.0138\n",
      "Training... train_count_of_activities.pkl\n",
      "Final RMSE over 50: 0.730908. Std 0.7576\n",
      "RMSE by fold 0.730743. Std 0.0158\n",
      "Training... train_time_based_cursor_pos_stats.pkl\n",
      "Final RMSE over 50: 0.731573. Std 0.7546\n",
      "RMSE by fold 0.731359. Std 0.0172\n",
      "Training... train_p_burst.pkl\n",
      "Final RMSE over 50: 0.761138. Std 0.7345\n",
      "RMSE by fold 0.760984. Std 0.0155\n",
      "Training... train_action_time_stats.pkl\n",
      "Final RMSE over 50: 0.965764. Std 0.4542\n",
      "RMSE by fold 0.965647. Std 0.0148\n",
      "Training... train_cursor_pos_stats.pkl\n",
      "Final RMSE over 50: 0.742722. Std 0.7545\n",
      "RMSE by fold 0.742555. Std 0.0159\n",
      "Training... train_essay_words.pkl\n",
      "Final RMSE over 50: 0.681552. Std 0.8016\n",
      "RMSE by fold 0.681396. Std 0.0144\n",
      "Training... train_essay_paragraphs.pkl\n",
      "Final RMSE over 50: 0.677635. Std 0.7962\n",
      "RMSE by fold 0.677495. Std 0.0136\n",
      "Training... train_r_burst.pkl\n",
      "Final RMSE over 50: 0.948473. Std 0.5243\n",
      "RMSE by fold 0.948408. Std 0.0111\n",
      "Training... train_essay_sentences.pkl\n",
      "Final RMSE over 50: 0.666417. Std 0.8003\n",
      "RMSE by fold 0.666253. Std 0.0147\n",
      "Training... train_count_vectorise.pkl\n",
      "Final RMSE over 50: 0.669951. Std 0.7928\n",
      "RMSE by fold 0.669869. Std 0.0104\n",
      "Training... train_essay_paragraphs.pkl\n",
      "Final RMSE over 50: 0.677635. Std 0.7962\n",
      "RMSE by fold 0.677495. Std 0.0136\n",
      "Training... train_essay_words.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/Projects/Kaggle/linking-writing/SPF_create_features.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_create_features.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m tr_cols \u001b[39m=\u001b[39m train_feats\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_create_features.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m ts_cols \u001b[39m=\u001b[39m test_feats\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_create_features.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m test_preds, valid_preds, final_rmse, cv_rm \u001b[39m=\u001b[39m lgb_pipeline(train_feats, test_feats, param)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_create_features.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m temp_res \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfeat_name\u001b[39m\u001b[39m'\u001b[39m:tr_feats_cand, \u001b[39m'\u001b[39m\u001b[39mRMSE\u001b[39m\u001b[39m'\u001b[39m: final_rmse}\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/Projects/Kaggle/linking-writing/SPF_create_features.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([results, pd\u001b[39m.\u001b[39mDataFrame([temp_res])])\n",
      "File \u001b[0;32m~/Projects/Kaggle/linking-writing/m5_sb_models.py:36\u001b[0m, in \u001b[0;36mlgb_pipeline\u001b[0;34m(train, test, param, n_splits, iterations)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m i, (train_index, valid_index) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(skf\u001b[39m.\u001b[39msplit(x, y\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m))):\n\u001b[1;32m     34\u001b[0m     train_x, train_y, valid_x, valid_y \u001b[39m=\u001b[39m train_valid_split(x, y, train_index, valid_index)\n\u001b[0;32m---> 36\u001b[0m     model\u001b[39m.\u001b[39;49mfit(train_x, train_y)\n\u001b[1;32m     37\u001b[0m     valid_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(valid_x)\n\u001b[1;32m     38\u001b[0m     test_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_x)\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1034\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     init_model: Optional[Union[\u001b[39mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLGBMRegressor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1050\u001b[0m         X,\n\u001b[1;32m   1051\u001b[0m         y,\n\u001b[1;32m   1052\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1053\u001b[0m         init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[1;32m   1054\u001b[0m         eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m   1055\u001b[0m         eval_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m   1056\u001b[0m         eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[1;32m   1057\u001b[0m         eval_init_score\u001b[39m=\u001b[39;49meval_init_score,\n\u001b[1;32m   1058\u001b[0m         eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[1;32m   1059\u001b[0m         feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m   1060\u001b[0m         categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature,\n\u001b[1;32m   1061\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1062\u001b[0m         init_model\u001b[39m=\u001b[39;49minit_model\n\u001b[1;32m   1063\u001b[0m     )\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/sklearn.py:842\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    839\u001b[0m evals_result: _EvalResultDict \u001b[39m=\u001b[39m {}\n\u001b[1;32m    840\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m    843\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    844\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[1;32m    845\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n\u001b[1;32m    846\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[1;32m    847\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m    848\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    849\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[1;32m    850\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    851\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m    852\u001b[0m )\n\u001b[1;32m    854\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_best_iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster\u001b[39m.\u001b[39mbest_iteration\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[39m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lrp/lib/python3.10/site-packages/lightgbm/basic.py:3658\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3657\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3658\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   3660\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3662\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_feats, test_feats = pd.DataFrame(), pd.DataFrame()\n",
    "results = pd.DataFrame()\n",
    "best_rmse = 0.8\n",
    "round = 0\n",
    "\n",
    "for k in range(len(list_train_feats)):\n",
    "    for i in range(round, len(list_train_feats)):\n",
    "        tr_feats_cand = list_train_feats[i]\n",
    "        ts_feats_cand = tr_feats_cand.replace('train', 'test')\n",
    "        tr_feats = pd.read_pickle(f'{FEAT_STORE}/{tr_feats_cand}')\n",
    "        ts_feats = pd.read_pickle(f'{FEAT_STORE}/{ts_feats_cand}')\n",
    "\n",
    "        if not(train_feats.empty & test_feats.empty):\n",
    "            train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "            test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "            assert train_feats.shape[1]==test_feats.shape[1]+1\n",
    "\n",
    "        else:\n",
    "            train_feats = tr_feats\n",
    "            test_feats = ts_feats\n",
    "            train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "        print(f'Training... {tr_feats_cand}')\n",
    "        tr_cols = train_feats.columns\n",
    "        ts_cols = test_feats.columns\n",
    "        test_preds, valid_preds, final_rmse, cv_rm = lgb_pipeline(train_feats, test_feats, param)\n",
    "        temp_res = {'feat_name':tr_feats_cand, 'RMSE': final_rmse}\n",
    "        results = pd.concat([results, pd.DataFrame([temp_res])])\n",
    "        train_feats.drop(columns=tr_cols, inplace=True)\n",
    "        test_feats.drop(columns=ts_cols, inplace=True)\n",
    "        #train_feats, test_feats = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    results = results.sort_values('RMSE', ascending=True)\n",
    "    best_feat = results.loc[round].feat_name\n",
    "    best_rmse = results.loc[round].RMSE\n",
    "    list_train_feats = list(results.feat_name)\n",
    "    round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... train_event_id_stats.pkl\n",
      "Final RMSE over 50: 0.767516. Std 0.7233\n",
      "RMSE by fold 0.767447. Std 0.0103\n",
      "Training... train_categorical_nunique.pkl\n",
      "Final RMSE over 50: 0.907111. Std 0.5422\n",
      "RMSE by fold 0.906990. Std 0.0153\n",
      "Training... train_essay_sentences.pkl\n",
      "Final RMSE over 50: 0.666417. Std 0.8003\n",
      "RMSE by fold 0.666253. Std 0.0147\n",
      "Training... train_count_vectorise_bigrams.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty DataFrames\n",
    "train_feats, test_feats = pd.DataFrame(), pd.DataFrame()\n",
    "results = pd.DataFrame()\n",
    "best_rmse = float('inf')\n",
    "round = 0\n",
    "used_features = set()\n",
    "improved = True\n",
    "\n",
    "while improved:\n",
    "    improved = False\n",
    "    for i in range(round, len(list_train_feats)):\n",
    "        tr_feats_cand = list_train_feats[i]\n",
    "\n",
    "        # Skip if this feature set has already been used\n",
    "        if tr_feats_cand in used_features:\n",
    "            continue\n",
    "\n",
    "        ts_feats_cand = tr_feats_cand.replace('train', 'test')\n",
    "        tr_feats = pd.read_pickle(f'{FEAT_STORE}/{tr_feats_cand}')\n",
    "        ts_feats = pd.read_pickle(f'{FEAT_STORE}/{ts_feats_cand}')\n",
    "\n",
    "        # Keep track of columns before adding new features\n",
    "        existing_train_columns = set(train_feats.columns)\n",
    "        existing_test_columns = set(test_feats.columns)\n",
    "\n",
    "        if not(train_feats.empty & test_feats.empty):\n",
    "            train_feats = train_feats.merge(tr_feats, on='id', how='left')\n",
    "            test_feats = test_feats.merge(ts_feats, on='id', how='left')\n",
    "            assert train_feats.shape[1] == test_feats.shape[1] + 1\n",
    "        else:\n",
    "            train_feats = tr_feats\n",
    "            test_feats = ts_feats\n",
    "            train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "\n",
    "        print(f'Training... {tr_feats_cand}')\n",
    "        tr_cols = train_feats.columns\n",
    "        ts_cols = test_feats.columns\n",
    "        test_preds, valid_preds, final_rmse, cv_rm = lgb_pipeline(train_feats, test_feats, param)\n",
    "        temp_res = {'feat_name': tr_feats_cand, 'RMSE': final_rmse}\n",
    "        results = pd.concat([results, pd.DataFrame([temp_res])])\n",
    "\n",
    "        # Remove recently added features\n",
    "        train_feats.drop(columns=tr_cols, inplace=True)\n",
    "        test_feats.drop(columns=ts_cols, inplace=True)\n",
    "\n",
    "        # Add the current feature set to the used features\n",
    "        used_features.add(tr_feats_cand)\n",
    "\n",
    "    results = results.sort_values('RMSE', ascending=True)\n",
    "    if results.loc[round].RMSE < best_rmse:\n",
    "        best_rmse = results.loc[round].RMSE\n",
    "        best_feat = results.loc[round].feat_name\n",
    "        improved = True\n",
    "\n",
    "    round += 1\n",
    "    # Update list_train_feats to only include unused features\n",
    "    list_train_feats = [feat for feat in list_train_feats if feat not in used_features]\n",
    "\n",
    "# Final results\n",
    "print(f\"Best RMSE: {best_rmse}\")\n",
    "print(f\"Best Feature Set: {best_feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Input text change features >\n",
      "< Action time by activities >\n",
      "< Events counts features >\n",
      "< Word counts rate of change features >\n",
      "< Categorical # unique values features >\n",
      "< word changes stats >\n",
      "< Count of events feats >\n",
      "< Cursor changes features >\n",
      "< Cursor changes based on time >\n",
      "< P-burst features >\n",
      "< R-burst features >\n",
      "< event_id rate of change >\n",
      "< Idle time features >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:04<00:00, 610.13it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3084.80it/s]\n",
      "100%|██████████| 2471/2471 [00:03<00:00, 640.53it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3256.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Essays word feats >\n",
      "< Essays word feats >\n",
      "< Essays sentences feats >\n",
      "< Essays sentences feats >\n",
      "< Essays paragraphs feats >\n",
      "< Essays paragraphs feats >\n"
     ]
    }
   ],
   "source": [
    "file_name = 'input_change'\n",
    "tr, ts = input_text_change_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'count_of_activities'\n",
    "tr, ts = count_of_activities(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'action_time_by_activity'\n",
    "tr, ts = action_time_by_activity(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'events_counts'\n",
    "tr, ts = events_counts(train_logs, test_logs,)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'word_counts_rate_of_change'\n",
    "tr, ts = rate_of_change_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'categorical_nunique'\n",
    "tr, ts = categorical_nunique(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'word_count_stats'\n",
    "tr, ts = words_stats_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'event_id_stats'\n",
    "tr, ts = events_stats_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'action_time_stats'\n",
    "tr, ts = action_time_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'cursor_pos_stats'\n",
    "tr, ts = cursor_stats_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'time_based_cursor_pos_stats'\n",
    "tr, ts = time_based_cursor_stats_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'p_burst'\n",
    "tr, ts = p_burst_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'r_burst'\n",
    "tr, ts = r_burst_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'events_counts_rate_of_change'\n",
    "tr, ts = rate_of_change_events(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "# file_name = 'word_count_acceleration'\n",
    "# tr, ts = wc_acceleration_feats(train_logs, test_logs)\n",
    "# tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "# ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'pauses'\n",
    "tr, ts = create_pauses(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "\n",
    "train_logs = train_logs.collect().to_pandas()\n",
    "test_logs = test_logs.collect().to_pandas()\n",
    "file_name = 'count_vectorise'\n",
    "tr, ts = countvectorize_one_one(train_logs, test_logs)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'count_vectorise_bigrams'\n",
    "tr, ts = countvectorize_one_one(train_logs, test_logs)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'essay_words'\n",
    "tr = word_feats(train_essays)\n",
    "ts = word_feats(test_essays)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'essay_sentences'\n",
    "tr = sent_feats(train_essays)\n",
    "ts = sent_feats(test_essays)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'essay_paragraphs'\n",
    "tr = parag_feats(train_essays)\n",
    "ts = parag_feats(test_essays)\n",
    "tr.to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Action time by activities >\n"
     ]
    }
   ],
   "source": [
    "file_name = 'p_burst'\n",
    "tr, ts = p_burst_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')\n",
    "file_name = 'r_burst'\n",
    "tr, ts = r_burst_feats(train_logs, test_logs)\n",
    "tr.collect().to_pandas().to_pickle(f'{FEAT_STORE}/train_{file_name}.pkl')\n",
    "ts.collect().to_pandas().to_pickle(f'{FEAT_STORE}/test_{file_name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
