{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from m4_feats_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'kaggle/input/linking-writing-processes-to-writing-quality'\n",
    "FEAT_STORE_DIR = 'feature_store'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tmp_down_time_mean</th>\n",
       "      <th>tmp_down_time_std</th>\n",
       "      <th>tmp_down_time_min</th>\n",
       "      <th>tmp_down_time_max</th>\n",
       "      <th>tmp_down_time_last</th>\n",
       "      <th>tmp_down_time_first</th>\n",
       "      <th>tmp_down_time_sem</th>\n",
       "      <th>tmp_down_time_median</th>\n",
       "      <th>tmp_down_time_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>tmp_cursor_position_sum</th>\n",
       "      <th>tmp_word_count_mean</th>\n",
       "      <th>tmp_word_count_std</th>\n",
       "      <th>tmp_word_count_min</th>\n",
       "      <th>tmp_word_count_max</th>\n",
       "      <th>tmp_word_count_last</th>\n",
       "      <th>tmp_word_count_first</th>\n",
       "      <th>tmp_word_count_sem</th>\n",
       "      <th>tmp_word_count_median</th>\n",
       "      <th>tmp_word_count_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>8.436548e+05</td>\n",
       "      <td>395112.665961</td>\n",
       "      <td>0</td>\n",
       "      <td>1797351</td>\n",
       "      <td>1797351</td>\n",
       "      <td>0</td>\n",
       "      <td>7813.679400</td>\n",
       "      <td>887190.0</td>\n",
       "      <td>2157225252</td>\n",
       "      <td>...</td>\n",
       "      <td>1818445</td>\n",
       "      <td>128.116152</td>\n",
       "      <td>76.498372</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1.512819</td>\n",
       "      <td>132.0</td>\n",
       "      <td>327593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>4.882323e+05</td>\n",
       "      <td>384959.404177</td>\n",
       "      <td>0</td>\n",
       "      <td>1758219</td>\n",
       "      <td>1758219</td>\n",
       "      <td>0</td>\n",
       "      <td>7771.013336</td>\n",
       "      <td>377050.0</td>\n",
       "      <td>1198122181</td>\n",
       "      <td>...</td>\n",
       "      <td>1904809</td>\n",
       "      <td>182.714751</td>\n",
       "      <td>97.763090</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.973502</td>\n",
       "      <td>186.0</td>\n",
       "      <td>448382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>8.240508e+05</td>\n",
       "      <td>489500.796565</td>\n",
       "      <td>0</td>\n",
       "      <td>1766778</td>\n",
       "      <td>1766778</td>\n",
       "      <td>0</td>\n",
       "      <td>7611.375322</td>\n",
       "      <td>755141.0</td>\n",
       "      <td>3408274006</td>\n",
       "      <td>...</td>\n",
       "      <td>3025946</td>\n",
       "      <td>194.772727</td>\n",
       "      <td>108.935068</td>\n",
       "      <td>0</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>0</td>\n",
       "      <td>1.693860</td>\n",
       "      <td>193.0</td>\n",
       "      <td>805580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>7.440880e+05</td>\n",
       "      <td>385205.014399</td>\n",
       "      <td>0</td>\n",
       "      <td>1362999</td>\n",
       "      <td>1362999</td>\n",
       "      <td>0</td>\n",
       "      <td>9765.334758</td>\n",
       "      <td>806845.5</td>\n",
       "      <td>1157800969</td>\n",
       "      <td>...</td>\n",
       "      <td>844188</td>\n",
       "      <td>103.618895</td>\n",
       "      <td>61.882250</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>1.568777</td>\n",
       "      <td>108.5</td>\n",
       "      <td>161231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>6.348842e+05</td>\n",
       "      <td>405576.409034</td>\n",
       "      <td>0</td>\n",
       "      <td>1583920</td>\n",
       "      <td>1583920</td>\n",
       "      <td>0</td>\n",
       "      <td>8061.699636</td>\n",
       "      <td>608118.0</td>\n",
       "      <td>1606891904</td>\n",
       "      <td>...</td>\n",
       "      <td>1518729</td>\n",
       "      <td>125.082971</td>\n",
       "      <td>77.255054</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>1.535610</td>\n",
       "      <td>113.0</td>\n",
       "      <td>316585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>7.136349e+05</td>\n",
       "      <td>503882.020411</td>\n",
       "      <td>0</td>\n",
       "      <td>1769114</td>\n",
       "      <td>1769114</td>\n",
       "      <td>0</td>\n",
       "      <td>7319.568976</td>\n",
       "      <td>712628.0</td>\n",
       "      <td>3381915633</td>\n",
       "      <td>...</td>\n",
       "      <td>3667989</td>\n",
       "      <td>256.353661</td>\n",
       "      <td>118.093794</td>\n",
       "      <td>0</td>\n",
       "      <td>461</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>1.715472</td>\n",
       "      <td>297.0</td>\n",
       "      <td>1214860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>8.202342e+05</td>\n",
       "      <td>512744.745940</td>\n",
       "      <td>0</td>\n",
       "      <td>1777392</td>\n",
       "      <td>1777392</td>\n",
       "      <td>0</td>\n",
       "      <td>10048.025509</td>\n",
       "      <td>726672.5</td>\n",
       "      <td>2135889912</td>\n",
       "      <td>...</td>\n",
       "      <td>2661493</td>\n",
       "      <td>223.013057</td>\n",
       "      <td>126.627934</td>\n",
       "      <td>0</td>\n",
       "      <td>438</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>2.481470</td>\n",
       "      <td>227.5</td>\n",
       "      <td>580726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>1.205533e+06</td>\n",
       "      <td>514320.848199</td>\n",
       "      <td>0</td>\n",
       "      <td>1935791</td>\n",
       "      <td>1935791</td>\n",
       "      <td>0</td>\n",
       "      <td>9293.100430</td>\n",
       "      <td>1483043.0</td>\n",
       "      <td>3692547571</td>\n",
       "      <td>...</td>\n",
       "      <td>4009729</td>\n",
       "      <td>157.589292</td>\n",
       "      <td>61.236111</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1.106456</td>\n",
       "      <td>201.0</td>\n",
       "      <td>482696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>5.566335e+05</td>\n",
       "      <td>334477.976640</td>\n",
       "      <td>0</td>\n",
       "      <td>1488450</td>\n",
       "      <td>1488450</td>\n",
       "      <td>0</td>\n",
       "      <td>5874.366278</td>\n",
       "      <td>554027.0</td>\n",
       "      <td>1804605942</td>\n",
       "      <td>...</td>\n",
       "      <td>3866542</td>\n",
       "      <td>205.917027</td>\n",
       "      <td>118.473905</td>\n",
       "      <td>0</td>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "      <td>0</td>\n",
       "      <td>2.080732</td>\n",
       "      <td>205.0</td>\n",
       "      <td>667583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>1.037117e+06</td>\n",
       "      <td>581107.759299</td>\n",
       "      <td>0</td>\n",
       "      <td>2030338</td>\n",
       "      <td>2030338</td>\n",
       "      <td>0</td>\n",
       "      <td>9659.672066</td>\n",
       "      <td>1097373.0</td>\n",
       "      <td>3753327248</td>\n",
       "      <td>...</td>\n",
       "      <td>2132220</td>\n",
       "      <td>105.105278</td>\n",
       "      <td>67.890428</td>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>1.128533</td>\n",
       "      <td>96.0</td>\n",
       "      <td>380376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  tmp_down_time_mean  tmp_down_time_std  tmp_down_time_min  \\\n",
       "0     001519c8        8.436548e+05      395112.665961                  0   \n",
       "1     0022f953        4.882323e+05      384959.404177                  0   \n",
       "2     0042269b        8.240508e+05      489500.796565                  0   \n",
       "3     0059420b        7.440880e+05      385205.014399                  0   \n",
       "4     0075873a        6.348842e+05      405576.409034                  0   \n",
       "...        ...                 ...                ...                ...   \n",
       "2466  ffb8c745        7.136349e+05      503882.020411                  0   \n",
       "2467  ffbef7e5        8.202342e+05      512744.745940                  0   \n",
       "2468  ffccd6fd        1.205533e+06      514320.848199                  0   \n",
       "2469  ffec5b38        5.566335e+05      334477.976640                  0   \n",
       "2470  fff05981        1.037117e+06      581107.759299                  0   \n",
       "\n",
       "      tmp_down_time_max  tmp_down_time_last  tmp_down_time_first  \\\n",
       "0               1797351             1797351                    0   \n",
       "1               1758219             1758219                    0   \n",
       "2               1766778             1766778                    0   \n",
       "3               1362999             1362999                    0   \n",
       "4               1583920             1583920                    0   \n",
       "...                 ...                 ...                  ...   \n",
       "2466            1769114             1769114                    0   \n",
       "2467            1777392             1777392                    0   \n",
       "2468            1935791             1935791                    0   \n",
       "2469            1488450             1488450                    0   \n",
       "2470            2030338             2030338                    0   \n",
       "\n",
       "      tmp_down_time_sem  tmp_down_time_median  tmp_down_time_sum  ...  \\\n",
       "0           7813.679400              887190.0         2157225252  ...   \n",
       "1           7771.013336              377050.0         1198122181  ...   \n",
       "2           7611.375322              755141.0         3408274006  ...   \n",
       "3           9765.334758              806845.5         1157800969  ...   \n",
       "4           8061.699636              608118.0         1606891904  ...   \n",
       "...                 ...                   ...                ...  ...   \n",
       "2466        7319.568976              712628.0         3381915633  ...   \n",
       "2467       10048.025509              726672.5         2135889912  ...   \n",
       "2468        9293.100430             1483043.0         3692547571  ...   \n",
       "2469        5874.366278              554027.0         1804605942  ...   \n",
       "2470        9659.672066             1097373.0         3753327248  ...   \n",
       "\n",
       "      tmp_cursor_position_sum  tmp_word_count_mean  tmp_word_count_std  \\\n",
       "0                     1818445           128.116152           76.498372   \n",
       "1                     1904809           182.714751           97.763090   \n",
       "2                     3025946           194.772727          108.935068   \n",
       "3                      844188           103.618895           61.882250   \n",
       "4                     1518729           125.082971           77.255054   \n",
       "...                       ...                  ...                 ...   \n",
       "2466                  3667989           256.353661          118.093794   \n",
       "2467                  2661493           223.013057          126.627934   \n",
       "2468                  4009729           157.589292           61.236111   \n",
       "2469                  3866542           205.917027          118.473905   \n",
       "2470                  2132220           105.105278           67.890428   \n",
       "\n",
       "      tmp_word_count_min  tmp_word_count_max  tmp_word_count_last  \\\n",
       "0                      0                 256                  255   \n",
       "1                      0                 323                  320   \n",
       "2                      0                 404                  404   \n",
       "3                      0                 206                  206   \n",
       "4                      0                 252                  252   \n",
       "...                  ...                 ...                  ...   \n",
       "2466                   0                 461                  273   \n",
       "2467                   0                 438                  438   \n",
       "2468                   0                 201                  201   \n",
       "2469                   0                 413                  413   \n",
       "2470                   0                 241                  240   \n",
       "\n",
       "      tmp_word_count_first  tmp_word_count_sem  tmp_word_count_median  \\\n",
       "0                        0            1.512819                  132.0   \n",
       "1                        0            1.973502                  186.0   \n",
       "2                        0            1.693860                  193.0   \n",
       "3                        0            1.568777                  108.5   \n",
       "4                        0            1.535610                  113.0   \n",
       "...                    ...                 ...                    ...   \n",
       "2466                     0            1.715472                  297.0   \n",
       "2467                     0            2.481470                  227.5   \n",
       "2468                     0            1.106456                  201.0   \n",
       "2469                     0            2.080732                  205.0   \n",
       "2470                     0            1.128533                   96.0   \n",
       "\n",
       "      tmp_word_count_sum  \n",
       "0                 327593  \n",
       "1                 448382  \n",
       "2                 805580  \n",
       "3                 161231  \n",
       "4                 316585  \n",
       "...                  ...  \n",
       "2466             1214860  \n",
       "2467              580726  \n",
       "2468              482696  \n",
       "2469              667583  \n",
       "2470              380376  \n",
       "\n",
       "[2471 rows x 46 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agg_fe_df(logs):\n",
    "    df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "        ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum']) # what about quantiles, IQR, etc.\n",
    "    \n",
    "    df.columns = ['_'.join(x) for x in df.columns]\n",
    "    df = df.add_prefix(\"tmp_\")\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "agg_fe_df(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:13<00:00,  2.23s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 14767.92it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 13680.04it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2474/2474 [00:00<00:00, 12759.27it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 14329.16it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:00<00:00, 14084.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1276244/657098511.py:216: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/tmp/ipykernel_1276244/657098511.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_1276244/657098511.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/tmp/ipykernel_1276244/657098511.py:219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    }
   ],
   "source": [
    "train_ids = train_logs.id\n",
    "test_ids = test_logs.id\n",
    "\n",
    "logs = pd.concat([train_logs, test_logs], axis=0)\n",
    "logs = normalise_up_down_times(logs)\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "feats = preprocessor.make_feats(logs)\n",
    "nan_cols = feats.columns[feats[feats['id'].isin(train_ids)].isna().any()].tolist()\n",
    "feats = feats.drop(columns=nan_cols)\n",
    "\n",
    "essays = getEssays(logs)\n",
    "sent_feats = compute_sentence_aggregations(essays)\n",
    "par_feats = compute_paragraph_aggregations(essays)\n",
    "word_feats = create_word_length_features(essays, 'essay', 'id', 'essay_words')\n",
    "vector_feats = countvectorize_one_one(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:04<00:00, 590.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_0</th>\n",
       "      <th>tok_1</th>\n",
       "      <th>tok_2</th>\n",
       "      <th>tok_3</th>\n",
       "      <th>tok_4</th>\n",
       "      <th>tok_5</th>\n",
       "      <th>tok_6</th>\n",
       "      <th>tok_7</th>\n",
       "      <th>tok_8</th>\n",
       "      <th>tok_9</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_17</th>\n",
       "      <th>tok_18</th>\n",
       "      <th>tok_19</th>\n",
       "      <th>tok_20</th>\n",
       "      <th>tok_21</th>\n",
       "      <th>tok_22</th>\n",
       "      <th>tok_23</th>\n",
       "      <th>tok_24</th>\n",
       "      <th>tok_25</th>\n",
       "      <th>tok_26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>88</td>\n",
       "      <td>67</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>61</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>55</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>102</td>\n",
       "      <td>51</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>80</td>\n",
       "      <td>72</td>\n",
       "      <td>66</td>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tok_0  tok_1  tok_2  tok_3  tok_4  tok_5  tok_6  tok_7  tok_8  tok_9  \\\n",
       "0        53     46     36     37     21     19     13     19      8      4   \n",
       "1        61     88     67     42     23     12      5      1      4      3   \n",
       "2        64     70     61     49     29     37     31     32      6     20   \n",
       "3        44     31     43     19     22     23      2      2      3      1   \n",
       "4        49     61     53     29     13     15      8      4      6      9   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2466     49     39     55     37     19     26     13     11      3      4   \n",
       "2467     74     78    102     51     34     30     23      8      3      0   \n",
       "2468     42     44     26     25     24     11      7     10      2      5   \n",
       "2469     80     72     66     53     37     40     13     13     12     15   \n",
       "2470     47     33     34     18     33     17     11      9      8     15   \n",
       "\n",
       "      ...  tok_17  tok_18  tok_19  tok_20  tok_21  tok_22  tok_23  tok_24  \\\n",
       "0     ...       0       0       0       0       0       0       0       0   \n",
       "1     ...       0       0       0       0       0       0       0       0   \n",
       "2     ...       0       0       0       0       0       0       0       0   \n",
       "3     ...       0       0       0       0       0       0       0       0   \n",
       "4     ...       0       0       0       0       0       0       0       0   \n",
       "...   ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "2466  ...       0       0       0       0       0       0       0       0   \n",
       "2467  ...       0       0       0       0       0       0       0       0   \n",
       "2468  ...       0       0       0       0       0       0       0       0   \n",
       "2469  ...       0       0       0       0       0       0       0       0   \n",
       "2470  ...       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      tok_25  tok_26  \n",
       "0          0       0  \n",
       "1          0       0  \n",
       "2          0       0  \n",
       "3          0       0  \n",
       "4          0       0  \n",
       "...      ...     ...  \n",
       "2466       0       0  \n",
       "2467       0       0  \n",
       "2468       0       0  \n",
       "2469       0       0  \n",
       "2470       0       0  \n",
       "\n",
       "[2471 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countvectorize_one_one(logs):\n",
    "\n",
    "    essays = getEssays(logs)\n",
    "    c_vect = CountVectorizer(ngram_range=(1, 1))\n",
    "    toks = c_vect.fit_transform(essays['essay']).todense()\n",
    "    toks_df = pd.DataFrame(columns = [f'tok_{i}' for i in range(toks.shape[1])], data=toks)\n",
    "    toks_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return toks_df\n",
    "\n",
    "# train_feats = pd.read_pickle('feature_store/base_feats/train_base_feats_1.pkl')\n",
    "# test_feats = pd.read_pickle('feature_store/base_feats/test_base_feats_1.pkl')\n",
    "countvectorize_one_one(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2474/2474 [00:04<00:00, 572.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2474, 27) (2474,)\n"
     ]
    }
   ],
   "source": [
    "def countvectorize_one_one(train_logs, test_logs, train_feats, test_feats):\n",
    "\n",
    "    tr_ids = train_feats.id\n",
    "    tst_ids = test_feats.id\n",
    "    tr_ts_logs = pd.concat([train_logs, test_logs], axis=0)\n",
    "    tr_ts_feats = pd.concat([train_feats['id'], test_feats['id']], axis=0).reset_index(drop=True)\n",
    "\n",
    "    essays = getEssays(tr_ts_logs)\n",
    "    c_vect = CountVectorizer(ngram_range=(1, 1))\n",
    "    toks = c_vect.fit_transform(essays['essay']).todense()\n",
    "    toks_df = pd.DataFrame(columns = [f'tok_{i}' for i in range(toks.shape[1])], data=toks)\n",
    "    toks_df.reset_index(drop=True, inplace=True)\n",
    "    print(toks_df.shape, tr_ts_feats.shape)\n",
    "\n",
    "    tr_ts_feats = pd.concat([tr_ts_feats, toks_df], axis=1)\n",
    "\n",
    "    train_feats = tr_ts_feats[tr_ts_feats['id'].isin(tr_ids)]\n",
    "    test_feats = tr_ts_feats[tr_ts_feats['id'].isin(tst_ids)]\n",
    "\n",
    "    return train_feats, test_feats\n",
    "\n",
    "train_feats = pd.read_pickle('feature_store/base_feats/train_base_feats_1.pkl')\n",
    "test_feats = pd.read_pickle('feature_store/base_feats/test_base_feats_1.pkl')\n",
    "train_, test_ = countvectorize_one_one(train_logs, test_logs, train_feats, test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id_max</th>\n",
       "      <th>up_time_max</th>\n",
       "      <th>action_time_max</th>\n",
       "      <th>action_time_min</th>\n",
       "      <th>action_time_mean</th>\n",
       "      <th>action_time_std</th>\n",
       "      <th>action_time_quantile</th>\n",
       "      <th>action_time_sem</th>\n",
       "      <th>action_time_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_17</th>\n",
       "      <th>tok_18</th>\n",
       "      <th>tok_19</th>\n",
       "      <th>tok_20</th>\n",
       "      <th>tok_21</th>\n",
       "      <th>tok_22</th>\n",
       "      <th>tok_23</th>\n",
       "      <th>tok_24</th>\n",
       "      <th>tok_25</th>\n",
       "      <th>tok_26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 445 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, event_id_max, up_time_max, action_time_max, action_time_min, action_time_mean, action_time_std, action_time_quantile, action_time_sem, action_time_sum, action_time_skew, action_time_kurt, activity_nunique, down_event_nunique, up_event_nunique, text_change_nunique, cursor_position_nunique, cursor_position_max, cursor_position_quantile, cursor_position_sem, cursor_position_mean, word_count_nunique, word_count_max, word_count_quantile, word_count_sem, word_count_mean, action_time_gap1_max, action_time_gap1_min, action_time_gap1_mean, action_time_gap1_std, action_time_gap1_quantile, action_time_gap1_sem, action_time_gap1_sum, action_time_gap1_skew, action_time_gap1_kurt, cursor_position_change1_max, cursor_position_change1_mean, cursor_position_change1_std, cursor_position_change1_quantile, cursor_position_change1_sem, cursor_position_change1_sum, cursor_position_change1_skew, cursor_position_change1_kurt, word_count_change1_max, word_count_change1_mean, word_count_change1_std, word_count_change1_quantile, word_count_change1_sem, word_count_change1_sum, word_count_change1_skew, word_count_change1_kurt, action_time_gap2_max, action_time_gap2_min, action_time_gap2_mean, action_time_gap2_std, action_time_gap2_quantile, action_time_gap2_sem, action_time_gap2_sum, action_time_gap2_skew, action_time_gap2_kurt, cursor_position_change2_max, cursor_position_change2_mean, cursor_position_change2_std, cursor_position_change2_quantile, cursor_position_change2_sem, cursor_position_change2_sum, cursor_position_change2_skew, cursor_position_change2_kurt, word_count_change2_max, word_count_change2_mean, word_count_change2_std, word_count_change2_quantile, word_count_change2_sem, word_count_change2_sum, word_count_change2_skew, word_count_change2_kurt, action_time_gap3_max, action_time_gap3_min, action_time_gap3_mean, action_time_gap3_std, action_time_gap3_quantile, action_time_gap3_sem, action_time_gap3_sum, action_time_gap3_skew, action_time_gap3_kurt, cursor_position_change3_max, cursor_position_change3_mean, cursor_position_change3_std, cursor_position_change3_quantile, cursor_position_change3_sem, cursor_position_change3_sum, cursor_position_change3_skew, cursor_position_change3_kurt, word_count_change3_max, word_count_change3_mean, word_count_change3_std, word_count_change3_quantile, word_count_change3_sem, word_count_change3_sum, word_count_change3_skew, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 445 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'count_vectorized_bigrams'\n",
    "\n",
    "train_.to_pickle(f'{FEAT_STORE_DIR}/train_{file_name}.pkl')\n",
    "test_.to_pickle(f'{FEAT_STORE_DIR}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:29<00:00,  2.72s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12514.69it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13338.93it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 9903.07it/s] \n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13499.24it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13254.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:360: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:362: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:363: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 22.36it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 39568.91it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 28992.88it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 26944.14it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 37008.56it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 23087.91it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:360: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:362: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:363: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:04<00:00, 564.51it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2775.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2471, 375)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n",
    "train_feats = train_feats.drop(columns=nan_cols)\n",
    "test_feats = test_feats.drop(columns=nan_cols)\n",
    "\n",
    "train_, test_ = process_action_time_activity(train_logs, test_logs)\n",
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "\n",
    "train_essay = getEssays(train_logs)\n",
    "test_essay = getEssays(test_logs)\n",
    "train_ = create_word_length_features(train_essay, 'essay_words', 'id', 'words')\n",
    "test_ = create_word_length_features(test_essay, 'essay_words', 'id', 'words')\n",
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "\n",
    "train_sent_df = split_essays_into_sentences(train_essay)\n",
    "train_ = compute_sentence_aggregations(train_sent_df)\n",
    "test_ = compute_sentence_aggregations(split_essays_into_sentences(test_essay))\n",
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "\n",
    "train_paragraph_df = split_essays_into_paragraphs(train_essay)\n",
    "train_ = compute_paragraph_aggregations(train_paragraph_df)\n",
    "test_ = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essay))\n",
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "##### feat_1\n",
    "train_, test_ = process_feats_time_gap_activity(train_logs, test_logs)\n",
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "\n",
    "train_, test_ = process_feats_action_time_gap(train_logs, test_logs)\n",
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "##### feat_2\n",
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2471, 495)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = train_feats.merge(train_, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_, on='id', how='left')\n",
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats.to_pickle(f'{FEAT_STORE_DIR}/train_feats_2_1.pkl')\n",
    "test_feats.to_pickle(f'{FEAT_STORE_DIR}/test_feats_2_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:03<00:00, 630.93it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3413.70it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'essay_words_feats'\n",
    "\n",
    "train_essay = getEssays(train_logs)\n",
    "test_essay = getEssays(test_logs)\n",
    "train_ = create_word_length_features(train_essay, 'essay_words', 'id', 'words')\n",
    "test_ = create_word_length_features(test_essay, 'essay_words', 'id', 'words')\n",
    "\n",
    "train_, test_ = process_feats_action_time_gap(train_logs, test_logs)\n",
    "train_, test_ = process_feats_time_gap_activity(train_logs, test_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [01:24<00:00,  2.57s/it, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 14360.23it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 12737.11it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 11716.33it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 14317.88it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 13897.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:360: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:362: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:363: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering ratios data\n",
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 23.38it/s, column=word_count_change100, method=kurt]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 18808.54it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 38956.38it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 36054.19it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 31855.47it/s]\n",
      "/root/miniconda3/envs/lrp/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 38362.54it/s]\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:360: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:361: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:362: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
      "/root/Projects/Kaggle/linking-writing/m4_feats_functions.py:363: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Engineering ratios data\n"
     ]
    }
   ],
   "source": [
    "file_name = 'base_feats'\n",
    "\n",
    "preprocessor = Preprocessor(seed=42)\n",
    "train_feats = preprocessor.make_feats(train_logs)\n",
    "test_feats = preprocessor.make_feats(test_logs)\n",
    "nan_cols = train_feats.columns[train_feats.isna().any()].tolist()\n",
    "train_feats = train_feats.drop(columns=nan_cols)\n",
    "test_feats = test_feats.drop(columns=nan_cols)\n",
    "\n",
    "train_feats.to_pickle(f'{FEAT_STORE_DIR}/train_{file_name}.pkl')\n",
    "test_feats.to_pickle(f'{FEAT_STORE_DIR}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_essay_words = create_word_length_features(train_essay, 'essay_words', 'id', 'words')\n",
    "test_essay_words = create_word_length_features(test_essay, 'essay_words', 'id', 'words')\n",
    "train_essay_words.to_pickle(f'{FEAT_STORE_DIR}/train_{file_name}.pkl')\n",
    "test_essay_words.to_pickle(f'{FEAT_STORE_DIR}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'at_by_bucket'\n",
    "train_action_buckets, test_action_buckets = action_time_by_bucket_feats(train_logs, test_logs)\n",
    "train_action_buckets.to_pickle(f'{FEAT_STORE_DIR}/train_{file_name}.pkl')\n",
    "test_action_buckets.to_pickle(f'{FEAT_STORE_DIR}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'at_by_activ'\n",
    "train_at_by_act, test_at_by_act = process_action_time_activity(train_logs, test_logs)\n",
    "train_at_by_act.to_pickle(f'{FEAT_STORE_DIR}/train_{file_name}.pkl')\n",
    "test_at_by_act.to_pickle(f'{FEAT_STORE_DIR}/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'adj_eff_time'\n",
    "train_adj_eff_time, test_adj_eff_time = process_adjusted_eff_time(train_logs, test_logs)\n",
    "train_adj_eff_time.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_adj_eff_time.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2470/2470 [00:00<00:00, 4126.84it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'rep_cut'\n",
    "train_rep_cut, test_rep_cut = process_re_cut_essays(train_logs, test_logs)\n",
    "train_rep_cut.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_rep_cut.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'action_time_gap'\n",
    "train_at_gap, test_at_gap = process_feats_action_time_gap(train_logs, test_logs)\n",
    "train_at_gap.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_at_gap.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'action_time_gap_by_acti'\n",
    "train_feats, test_feats = process_feats_time_gap_activity(train_logs, test_logs)\n",
    "train_feats.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_feats.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'IKI'\n",
    "train_IKI = train_logs.groupby(['id']).apply(calculate_pause_features).reset_index()\n",
    "test_IKI = test_logs.groupby(['id']).apply(calculate_pause_features).reset_index()\n",
    "train_IKI.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_IKI.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'train_wc_chage'\n",
    "train_feats = create_feats_wc_change(train_logs)\n",
    "test_feats = create_feats_wc_change(test_logs)\n",
    "train_feats.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_feats.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'wpm_feats'\n",
    "train_feats, test_feats = wpm_feats(train_logs, test_logs)\n",
    "train_feats.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_feats.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 317/317 [00:00<00:00, 16520.80it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name = 'essay_paste_words'\n",
    "train_feats, test_feats = essay_paste_words(train_logs, test_logs)\n",
    "train_feats.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_feats.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:04<00:00, 578.22it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3821.11it/s]\n",
      "100%|██████████| 2471/2471 [00:04<00:00, 613.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train_essays = getEssays(train_logs)\n",
    "test_essays = getEssays(test_logs)\n",
    "\n",
    "# Sentence features for train dataset\n",
    "train_essays = getEssays(train_logs)\n",
    "train_sent_df = split_essays_into_sentences(train_essays)\n",
    "train_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n",
    "\n",
    "# Paragraph features for train dataset\n",
    "train_paragraph_df = split_essays_into_paragraphs(train_essays)\n",
    "train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n",
    "\n",
    "# Features for test dataset\n",
    "test_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\n",
    "test_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))\n",
    "\n",
    "file_name = 'essay_sen'\n",
    "train_sent_agg_df.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_sent_agg_df.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')\n",
    "\n",
    "file_name = 'essay_par'\n",
    "train_paragraph_agg_df.to_pickle(f'{FEAT_STORE_DIR}/train/train_{file_name}.pkl')\n",
    "test_paragraph_agg_df.to_pickle(f'{FEAT_STORE_DIR}/test/test_{file_name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
