{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T01:54:18.277262Z","iopub.status.busy":"2024-01-01T01:54:18.276898Z","iopub.status.idle":"2024-01-01T01:54:18.575204Z","shell.execute_reply":"2024-01-01T01:54:18.574069Z","shell.execute_reply.started":"2024-01-01T01:54:18.277228Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import polars as pl\n","import numpy as np\n","import re\n","from joblib import Parallel, delayed\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from scipy.stats import skew, kurtosis\n","from m4_feats_polars import *\n","from m5_sb_models import *\n","\n","pl.set_random_seed(42)\n","\n","def amend_event_id_order(train_logs, test_logs):\n","    fixed_logs = []\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","        logs = logs.sort(pl.col(['id', 'down_time']))\n","        logs = logs.with_columns(pl.col('event_id').cumcount().over('id'))\n","        fixed_logs.append(logs)\n","\n","    return fixed_logs[0], fixed_logs[1]\n","\n","def normalise_up_down_times(train_logs, test_logs):\n","    new_logs = []\n","    for logs in [train_logs, test_logs]:\n","        min_down_time = logs.group_by('id').agg(pl.min('down_time').alias('min_down_time'))\n","        logs = logs.join(min_down_time, on='id', how='left')\n","        logs = logs.with_columns([(pl.col('down_time') - pl.col('min_down_time')).alias('normalised_down_time')])\n","        logs = logs.with_columns([(pl.col('normalised_down_time') + pl.col('action_time')).alias('normalised_up_time')])\n","        logs = logs.drop(['min_down_time', 'down_time', 'up_time'])\n","        logs = logs.rename({'normalised_down_time': 'down_time', 'normalised_up_time': 'up_time'})\n","        new_logs.append(logs.sort(['id','event_id'], descending=[True,True]))\n","    return new_logs[0], new_logs[1]\n","\n","# Helper functions\n","def q1(x):\n","    return x.quantile(0.25)\n","def q3(x):\n","    return x.quantile(0.75)\n","\n","def down_time_padding(train_logs, test_logs, time_agg):\n","\n","    data = []\n","    for logs in [train_logs, test_logs]:\n","    # bin original logs\n","        logs_binned = logs.clone()\n","        logs_binned = logs_binned.with_columns((pl.col('down_time') / 1000).alias('down_time_sec'))\n","        logs_binned = logs_binned.with_columns(((pl.col('down_time_sec') // time_agg) * time_agg).alias('time_bin'))\n","\n","        grp_binned = logs_binned.group_by(['id', 'time_bin']).agg(pl.max('word_count'),\n","                                                                  pl.count('event_id'),\n","                                                                  pl.max('cursor_position'))\n","        grp_binned = grp_binned.with_columns(pl.col('time_bin').cast(pl.Float64))\n","        grp_binned = grp_binned.sort([pl.col('id'), pl.col('time_bin')])\n","\n","        # get max down_time value from logs\n","        max_logs = logs.clone()\n","        max_down_time = max_logs.group_by(['id']).agg(pl.max('down_time') / 1000)\n","\n","        padding_dataframes = []\n","        max_down_time = max_down_time.collect()\n","\n","        # Iterate over each row in max_down_time_df\n","        for row in max_down_time.rows():\n","            id_value, max_time_value = row[0], row[1]  # Access by index\n","            time_steps = list(np.arange(0, max_time_value + time_agg, time_agg))\n","\n","            # Create padding DataFrame with the correct types\n","            padding_df = pl.DataFrame({\n","                'id': [str(id_value)] * len(time_steps),\n","                'time_bin': time_steps\n","            })\n","\n","            padding_dataframes.append(padding_df)\n","\n","        pad_df = pl.concat(padding_dataframes).lazy()\n","        grp_df = pad_df.join(grp_binned.lazy(), on=['id', 'time_bin'], how='left')\n","        grp_df = grp_df.sort([pl.col('id'), pl.col('time_bin')])\n","        grp_df = grp_df.with_columns(pl.col(['word_count','event_id','cursor_position']).fill_null(strategy=\"forward\").over('id'))\n","        data.append(grp_df)\n","\n","    return data[0], data[1]\n","\n","def countvectorize_one_one(train_essays, test_essays):\n","    print(\"< Count vectorize one-grams >\")\n","    \n","    tr_len = train_essays.shape[0]\n","    ids = pd.concat([train_essays.id, test_essays.id], axis=0).reset_index(drop=True)\n","    essays = pd.concat([train_essays, test_essays], axis=0).reset_index(drop=True)\n","    c_vect = CountVectorizer(ngram_range=(1, 1))\n","    toks = c_vect.fit_transform(essays['essay']).todense()\n","    toks = toks[:,:16]\n","    toks_df = pd.DataFrame(columns = [f'one_gram_tok_{i}' for i in range(toks.shape[1])], data=toks)\n","\n","    feats = pd.concat([ids, toks_df], axis=1)\n","\n","    tr_feats = feats.loc[:tr_len-1]\n","    ts_feats = feats.loc[tr_len:]\n","\n","    tr_feats = pl.DataFrame(tr_feats).lazy()\n","    ts_feats = pl.DataFrame(ts_feats).lazy()\n","    return tr_feats, ts_feats\n","\n","def countvectorize_two_one(train_essays, test_essays):\n","    print(\"< Count vectorize bi-grams >\")\n","    data = []\n","    tr_len = train_essays.shape[0]\n","    ids = pd.concat([train_essays.id, test_essays.id], axis=0).reset_index(drop=True)\n","    essays = pd.concat([train_essays, test_essays], axis=0).reset_index(drop=True)\n","    c_vect = CountVectorizer(ngram_range=(2, 2))\n","    toks = c_vect.fit_transform(essays['essay']).todense()\n","    toks = toks[:,:16]\n","    toks_df = pd.DataFrame(columns = [f'bigram_tok_{i}' for i in range(toks.shape[1])], data=toks)\n","\n","    feats = pd.concat([ids, toks_df], axis=1)\n","\n","    tr_feats = feats.loc[:tr_len-1]\n","    ts_feats = feats.loc[tr_len:]\n","\n","    tr_feats = pl.DataFrame(tr_feats).lazy()\n","    ts_feats = pl.DataFrame(ts_feats).lazy()\n","    return tr_feats, ts_feats\n","\n","def input_text_change_feats(train_logs, test_logs):\n","    print(\"< Input text change features >\") # character level - not word level as opposed to essays feats\n","    feats = []\n","    for data in [train_logs, test_logs]:\n","        df = data.clone()\n","        temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n","        temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n","        temp = temp.with_columns(\n","                                    input_text_count = pl.col('text_change').list.len(),\n","                                    input_text_len_mean = pl.col('text_change').map_elements(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n","                                    input_text_len_max = pl.col('text_change').map_elements(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n","                                    input_text_len_std = pl.col('text_change').map_elements(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n","                                    input_text_len_median = pl.col('text_change').map_elements(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n","                                    input_text_len_skew = pl.col('text_change').map_elements(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n","        temp = temp.drop('text_change')\n","        feats.append(temp)\n","\n","    feats = [feat.collect() for feat in feats]\n","    missing_cols = set(feats[0].columns) - set(feats[1].columns)\n","\n","    for col in missing_cols:\n","        nan_series = pl.repeat(np.nan, n=len(feats[1])).alias(col)\n","        feats[1] = feats[1].with_columns(nan_series)\n","\n","    return feats[0].lazy(), feats[1].lazy()\n","\n","def count_of_activities(train_logs, test_logs):\n","    def count_by_values(df, colname, values):\n","        fts = df.select(pl.col('id').unique(maintain_order=True))\n","        for i, value in enumerate(values):\n","            tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n","            fts  = fts.join(tmp_df, on='id', how='left') \n","        return fts\n","\n","    feats = []\n","    activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","\n","    for logs in [train_logs, test_logs]:\n","        counts = logs.clone()\n","        counts = count_by_values(counts, 'activity', activities)\n","        feats.append(counts)\n","\n","    return feats[0], feats[1]\n","\n","def count_by_values(df, colname, values):\n","    fts = df.select(pl.col('id').unique(maintain_order=True))\n","    for i, value in enumerate(values):\n","        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n","        fts  = fts.join(tmp_df, on='id', how='left') \n","    return fts\n","\n","def action_time_by_activity(train_logs, test_logs):\n","    print(\"< Action time by activities >\")\n","    feats = []\n","    activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","\n","        stats = logs.filter(pl.col('activity').is_in(activities)) \\\n","            .group_by(['id', 'activity']) \\\n","            .agg(pl.sum('action_time').name.suffix('_sum')) \\\n","            .collect() \\\n","            .pivot(values='action_time_sum', index='id', columns='activity') \\\n","            .fill_null(0)\n","\n","        feats.append(stats)\n","\n","    missing_cols = set(feats[0].columns) - set(feats[1].columns)\n","\n","    for col in missing_cols:\n","        zero_series = pl.repeat(0, n=len(feats[1])).alias(col)\n","        feats[1] = feats[1].with_columns(zero_series)\n","    return feats[0].lazy(), feats[1].lazy()\n","\n","def down_events_counts(train_logs, test_logs, n_events=20):\n","    print(\"< Events counts features >\")\n","    logs = pl.concat([train_logs, test_logs], how = 'vertical')\n","    events = (logs\n","            .group_by(['down_event'])\n","            .agg(pl.count())\n","            .sort('count', descending=True)\n","            .head(n_events).collect()\n","            .select('down_event')\n","            .to_series().to_list())\n","\n","    tr_ids = train_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","    ts_ids = test_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","\n","    data = logs.clone()\n","    event_stats = (data\n","                    .filter(pl.col('down_event').is_in(events))\n","                    .group_by(['id', 'down_event'])\n","                    .agg(pl.count()).collect()\n","                    .pivot(values='count', index='id', columns='down_event')\n","                    ).fill_null(0).lazy()\n","    \n","    # Rename columns to a generic format\n","    cols = event_stats.columns[1:]  # Skip the 'id' column\n","    for i, col in enumerate(cols):\n","        event_stats = event_stats.rename({col: f'down_event_{i+1}'})\n","\n","    tr_feats = event_stats.filter(pl.col('id').is_in(tr_ids))\n","    ts_feats = event_stats.filter(pl.col('id').is_in(ts_ids))\n","\n","    return tr_feats, ts_feats\n","\n","def categorical_nunique(train_logs, test_logs):\n","    print(\"< Categorical # unique values features >\")    \n","    feats = []\n","\n","    for logs in [train_logs, test_logs]:\n","        data = logs.clone()\n","        temp  = data.group_by(\"id\").agg(\n","            pl.n_unique(['activity', 'down_event', 'text_change'])\n","            .name.suffix('_nunique'))\n","        feats.append(temp)\n","    \n","    return feats[0], feats[1]\n","\n","def word_count_stats_baseline(train_logs, test_logs):\n","    print(\"< word changes stats baseline>\")\n","    feats = []\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            word_count_baseline_sum = pl.col('word_count').sum(),\n","            word_count_baseline_mean = pl.col('word_count').mean(),\n","            word_count_baseline_std = pl.col('word_count').std(),\n","            word_count_baseline_max = pl.col('word_count').max(),\n","            word_count_baseline_q1 = pl.col('word_count').quantile(0.25),\n","            word_count_baseline_median = pl.col('word_count').median(),\n","            word_count_baseline_q3 = pl.col('word_count').quantile(0.75),\n","            word_count_baseline_kurt = pl.col('word_count').kurtosis(),\n","            word_count_baseline_skew = pl.col('word_count').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def word_count_time_based(train_logs, test_logs, time_agg=12):\n","    print(\"< word changes stats time based>\")\n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","\n","    for data in [tr_pad, ts_pad]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            word_count_sum = pl.col('word_count').sum(),\n","            word_count_mean = pl.col('word_count').mean(),\n","            word_count_std = pl.col('word_count').std(),\n","            word_count_max = pl.col('word_count').max(),\n","            word_count_q1 = pl.col('word_count').quantile(0.25),\n","            word_count_median = pl.col('word_count').median(),\n","            word_count_q3 = pl.col('word_count').quantile(0.75),\n","            word_count_kurt = pl.col('word_count').kurtosis(),\n","            word_count_skew = pl.col('word_count').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def word_counts_rate_of_change(train_logs, test_logs, time_agg=5):\n","    print(\"< Word counts rate of change features >\")\n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","\n","    for data in [tr_pad, ts_pad]:\n","        logs = data.clone()\n","        logs = logs.sort('id')\n","        logs = logs.with_columns([\n","            pl.col('word_count').diff().over('id').alias('word_count_diff'),\n","            pl.col('time_bin').diff().over('id').alias('time_bin_diff')\n","        ]).fill_nan(0)\n","\n","        logs = logs.with_columns(\n","            (pl.col('word_count_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('rate_of_change')).fill_nan(0)\n","\n","        # Aggregating\n","        stats = logs.group_by('id').agg([\n","            pl.col('rate_of_change').filter(pl.col('rate_of_change') == 0).count().alias('roc_zro_count'),\n","            pl.col('rate_of_change').filter(pl.col('rate_of_change') > 0).count().alias('pos_change_count'),\n","            pl.col('rate_of_change').filter(pl.col('rate_of_change') < 0).count().alias('neg_change_count'),\n","            pl.col('rate_of_change').count().alias('roc_count'),\n","            pl.col('rate_of_change').mean().alias('roc_mean'),\n","            pl.col('rate_of_change').std().alias('roc_std'),\n","            pl.col('rate_of_change').sum().alias('roc_sum'),\n","            pl.col('rate_of_change').max().alias('roc_max'),\n","            pl.col('rate_of_change').quantile(0.25).alias('roc_q1'),\n","            pl.col('rate_of_change').median().alias('roc_median'),\n","            pl.col('rate_of_change').quantile(0.75).alias('roc_q3'),\n","            pl.col('rate_of_change').kurtosis().alias('roc_kurt'),\n","            pl.col('rate_of_change').skew().alias('roc_skew'),\n","        ])\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def word_count_acceleration(train_logs, test_logs, time_agg=8):\n","    print(\"< word count acceleration >\")    \n","\n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","\n","    for logs in [tr_pad, ts_pad]:\n","\n","        grp_df = logs.clone()\n","        grp_df = grp_df.sort(['id', 'time_bin'])\n","\n","        grp_df = grp_df.with_columns([\n","            pl.col('word_count').diff().over('id').fill_null(0).alias('word_count_diff'),\n","            pl.col('time_bin').diff().over('id').fill_null(0).alias('time_bin_diff'),\n","        ])\n","\n","        grp_df = grp_df.with_columns(\n","            (pl.col('word_count_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('rate_of_change')\n","        )\n","\n","        grp_df = grp_df.with_columns(\n","            pl.col('rate_of_change').diff().over('id').fill_nan(0).alias('rate_of_change_diff')\n","        )\n","\n","        grp_df = grp_df.with_columns(\n","            (pl.col('rate_of_change_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('acceleration')\n","        )\n","        grp_df = grp_df.select(pl.col(['id', 'acceleration']))\n","\n","        stats = grp_df.group_by('id').agg(\n","            word_count_acc_zero = pl.col('acceleration').filter(pl.col('acceleration') == 0).count(),\n","            word_count_acc_pst = pl.col('acceleration').filter(pl.col('acceleration') > 0).count(),\n","            word_count_acc_neg = pl.col('acceleration').filter(pl.col('acceleration') < 0).count(),\n","            word_count_acc_sum = pl.col('acceleration').sum(),\n","            word_count_acc_mean = pl.col('acceleration').mean(),\n","            word_count_acc_std = pl.col('acceleration').std(),\n","            word_count_acc_max = pl.col('acceleration').max(),\n","            word_count_acc_q1 = pl.col('acceleration').quantile(0.25),\n","            word_count_acc_median = pl.col('acceleration').median(),\n","            word_count_acc_q3 = pl.col('acceleration').quantile(0.75),\n","            word_count_acc_kurt = pl.col('acceleration').kurtosis(),\n","            word_count_acc_skew = pl.col('acceleration').skew(),\n","        )\n","\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def events_counts_time_based(train_logs, test_logs, time_agg=5):\n","    print(\"< Count of events time based feats >\")\n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","\n","    for data in [tr_pad, ts_pad]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            eid_timeb_sum = pl.col('event_id').sum(),\n","            eid_timeb_mean = pl.col('event_id').mean(),\n","            eid_timeb_std = pl.col('event_id').std(),\n","            eid_timeb_max = pl.col('event_id').max(),\n","            eid_timeb_q1 = pl.col('event_id').quantile(0.25),\n","            eid_timeb_median = pl.col('event_id').median(),\n","            eid_timeb_q3 = pl.col('event_id').quantile(0.75),\n","            eid_timeb_kurt = pl.col('event_id').kurtosis(),\n","            eid_timeb_skew = pl.col('event_id').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def events_counts_baseline(train_logs, test_logs):\n","    print(\"< Count of events baseline feats >\")\n","    feats = []\n","\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            eid_bline_sum = pl.col('event_id').sum(),\n","            eid_bline_mean = pl.col('event_id').mean(),\n","            eid_bline_std = pl.col('event_id').std(),\n","            eid_bline_max = pl.col('event_id').max(),\n","            eid_bline_q1 = pl.col('event_id').quantile(0.25),\n","            eid_bline_median = pl.col('event_id').median(),\n","            eid_bline_q3 = pl.col('event_id').quantile(0.75),\n","            eid_bline_kurt = pl.col('event_id').kurtosis(),\n","            eid_bline_skew = pl.col('event_id').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def events_counts_rate_of_change(train_logs, test_logs, time_agg=3):\n","    print(\"< event_id rate of change >\")    \n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","    for data in [tr_pad, ts_pad]:\n","\n","        logs = data.clone()\n","        logs = logs.sort('id')\n","        logs = logs.with_columns([\n","            pl.col('event_id').diff().over('id').alias('event_id_diff'),\n","            pl.col('time_bin').diff().over('id').alias('time_bin_diff')\n","        ]).fill_nan(0)\n","\n","        logs = logs.with_columns(\n","            (pl.col('event_id_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('rate_of_change')).fill_nan(0)\n","\n","        # Aggregating\n","        stats = logs.group_by('id').agg(\n","            eid_roc_count_zr = pl.col('rate_of_change').filter(pl.col('rate_of_change') == 0).count(),\n","            eid_roc_count = pl.col('rate_of_change').count(),\n","            eid_roc_mean = pl.col('rate_of_change').mean(),\n","            eid_roc_std = pl.col('rate_of_change').std(),\n","            eid_roc_max = pl.col('rate_of_change').max(),\n","            eid_roc_q1 = pl.col('rate_of_change').quantile(0.25),\n","            eid_roc_median = pl.col('rate_of_change').median(),\n","            eid_roc_q3 = pl.col('rate_of_change').quantile(0.75),\n","            eid_roc_kurt = pl.col('rate_of_change').kurtosis(),\n","            eid_roc_skew = pl.col('rate_of_change').skew(),\n","        )\n","\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def events_counts_acceleration(train_logs, test_logs, time_agg=4):\n","    print(\"< events counts acceleration >\")    \n","\n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","\n","    for logs in [tr_pad, ts_pad]:\n","\n","        grp_df = logs.clone()\n","        grp_df = grp_df.sort(['id', 'time_bin'])\n","\n","        grp_df = grp_df.with_columns([\n","            pl.col('event_id').diff().over('id').fill_null(0).alias('event_id_diff'),\n","            pl.col('time_bin').diff().over('id').fill_null(0).alias('time_bin_diff'),\n","        ])\n","\n","        grp_df = grp_df.with_columns(\n","            (pl.col('event_id_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('rate_of_change')\n","        )\n","\n","        grp_df = grp_df.with_columns(\n","            pl.col('rate_of_change').diff().over('id').fill_nan(0).alias('rate_of_change_diff')\n","        )\n","\n","        grp_df = grp_df.with_columns(\n","            (pl.col('rate_of_change_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('acceleration')\n","        )\n","        grp_df = grp_df.select(pl.col(['id', 'acceleration']))\n","\n","        stats = grp_df.group_by('id').agg(\n","            evid_acc_zero = pl.col('acceleration').filter(pl.col('acceleration') == 0).count(),\n","            evid_acc_pst = pl.col('acceleration').filter(pl.col('acceleration') > 0).count(),\n","            evid_acc_neg = pl.col('acceleration').filter(pl.col('acceleration') < 0).count(),\n","            evid_acc_sum = pl.col('acceleration').sum(),\n","            evid_acc_mean = pl.col('acceleration').mean(),\n","            evid_acc_std = pl.col('acceleration').std(),\n","            evid_acc_max = pl.col('acceleration').max(),\n","            evid_acc_q1 = pl.col('acceleration').quantile(0.25),\n","            evid_acc_median = pl.col('acceleration').median(),\n","            evid_acc_q3 = pl.col('acceleration').quantile(0.75),\n","            evid_acc_kurt = pl.col('acceleration').kurtosis(),\n","            evid_acc_skew = pl.col('acceleration').skew(),\n","        )\n","\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def action_time_baseline_stats(train_logs, test_logs):\n","    print(\"< Action time baseline stats >\")\n","    feats = []\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            action_time_mean = pl.col('action_time').mean(),\n","            action_time_std = pl.col('action_time').std(),\n","            action_time_max = pl.col('action_time').max(),\n","            action_time_q1 = pl.col('action_time').quantile(0.25),\n","            action_time_median = pl.col('action_time').median(),\n","            action_time_q3 = pl.col('action_time').quantile(0.75),\n","            action_time_kurt = pl.col('action_time').kurtosis(),\n","            action_time_skew = pl.col('action_time').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def cursor_pos_baseline(train_logs, test_logs):\n","    print(\"< Cursor changes features >\")\n","    feats = []\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            action_time_mean = pl.col('cursor_position').mean(),\n","            action_time_std = pl.col('cursor_position').std(),\n","            action_time_max = pl.col('cursor_position').max(),\n","            action_time_q1 = pl.col('cursor_position').quantile(0.25),\n","            action_time_median = pl.col('cursor_position').median(),\n","            action_time_q3 = pl.col('cursor_position').quantile(0.75),\n","            action_time_kurt = pl.col('cursor_position').kurtosis(),\n","            action_time_skew = pl.col('cursor_position').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def cursor_pos_time_based(train_logs, test_logs, time_agg=3):\n","    print(\"< Cursor changes based on time >\")\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","    feats = []\n","    for data in [tr_pad, ts_pad]:\n","        logs = data.clone()\n","        stats = logs.group_by('id').agg(\n","            cursor_pos_mean = pl.col('cursor_position').mean(),\n","            cursor_pos_std = pl.col('cursor_position').std(),\n","            cursor_pos_max = pl.col('cursor_position').max(),\n","            cursor_pos_q1 = pl.col('cursor_position').quantile(0.25),\n","            cursor_pos_median = pl.col('cursor_position').median(),\n","            cursor_pos_q3 = pl.col('cursor_position').quantile(0.75),\n","            cursor_pos_kurt = pl.col('cursor_position').kurtosis(),\n","            cursor_pos_skew = pl.col('cursor_position').skew(),\n","        )\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def cursor_pos_rate_of_change(train_logs, test_logs, time_agg=10):\n","    print(\"< event_id rate of change >\")    \n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","    for data in [tr_pad, ts_pad]:\n","\n","        logs = data.clone()\n","        logs = logs.sort('id')\n","        logs = logs.with_columns([\n","            pl.col('cursor_position').diff().over('id').alias('cursor_pos_diff'),\n","            pl.col('time_bin').diff().over('id').alias('time_bin_diff')\n","        ]).fill_nan(0)\n","\n","        logs = logs.with_columns(\n","            (pl.col('cursor_pos_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('rate_of_change')).fill_nan(0)\n","\n","        # Aggregating\n","        stats = logs.group_by('id').agg(\n","            cursor_pos_roc_count_zr = pl.col('rate_of_change').filter(pl.col('rate_of_change') == 0).count(),\n","            cursor_pos_pst_change_count = pl.col('rate_of_change').filter(pl.col('rate_of_change') > 0).count(),\n","            cursor_pos_neg_change_count = pl.col('rate_of_change').filter(pl.col('rate_of_change') < 0).count(),\n","            cursor_pos_roc_count = pl.col('rate_of_change').count(),\n","            cursor_pos_roc_mean = pl.col('rate_of_change').mean(),\n","            cursor_pos_roc_std = pl.col('rate_of_change').std(),\n","            cursor_pos_roc_max = pl.col('rate_of_change').max(),\n","            cursor_pos_roc_q1 = pl.col('rate_of_change').quantile(0.25),\n","            cursor_pos_roc_median = pl.col('rate_of_change').median(),\n","            cursor_pos_roc_q3 = pl.col('rate_of_change').quantile(0.75),\n","            cursor_pos_roc_kurt = pl.col('rate_of_change').kurtosis(),\n","            cursor_pos_roc_skew = pl.col('rate_of_change').skew(),\n","        )\n","\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def cursor_pos_acceleration(train_logs, test_logs, time_agg=8):\n","    print(\"< cursor position acceleration >\")    \n","\n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    tr_pad, ts_pad = down_time_padding(tr_logs, ts_logs, time_agg)\n","\n","    for logs in [tr_pad, ts_pad]:\n","\n","        grp_df = logs.clone()\n","        grp_df = grp_df.sort(['id', 'time_bin'])\n","\n","        grp_df = grp_df.with_columns([\n","            pl.col('cursor_position').diff().over('id').fill_null(0).alias('cursor_position_diff'),\n","            pl.col('time_bin').diff().over('id').fill_null(0).alias('time_bin_diff'),\n","        ])\n","\n","        grp_df = grp_df.with_columns(\n","            (pl.col('cursor_position_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('rate_of_change')\n","        )\n","\n","        grp_df = grp_df.with_columns(\n","            pl.col('rate_of_change').diff().over('id').fill_nan(0).alias('rate_of_change_diff')\n","        )\n","\n","        grp_df = grp_df.with_columns(\n","            (pl.col('rate_of_change_diff') / pl.col('time_bin_diff')).fill_nan(0).alias('acceleration')\n","        )\n","        grp_df = grp_df.select(pl.col(['id', 'acceleration']))\n","\n","        stats = grp_df.group_by('id').agg(\n","            cursor_pos_acc_zero = pl.col('acceleration').filter(pl.col('acceleration') == 0).count(),\n","            cursor_pos_acc_pst = pl.col('acceleration').filter(pl.col('acceleration') > 0).count(),\n","            cursor_pos_acc_neg = pl.col('acceleration').filter(pl.col('acceleration') < 0).count(),\n","            cursor_pos_acc_sum = pl.col('acceleration').sum(),\n","            cursor_pos_acc_mean = pl.col('acceleration').mean(),\n","            cursor_pos_acc_std = pl.col('acceleration').std(),\n","            cursor_pos_acc_max = pl.col('acceleration').max(),\n","            cursor_pos_acc_q1 = pl.col('acceleration').quantile(0.25),\n","            cursor_pos_acc_median = pl.col('acceleration').median(),\n","            cursor_pos_acc_q3 = pl.col('acceleration').quantile(0.75),\n","            cursor_pos_acc_kurt = pl.col('acceleration').kurtosis(),\n","            cursor_pos_acc_skew = pl.col('acceleration').skew(),\n","        )\n","\n","        feats.append(stats)\n","    return feats[0], feats[1]\n","\n","def create_integrated_iki(logs):\n","\n","    logs = logs.with_columns(\n","        pl.col('action_time').diff()\n","        .over('id')\n","        .alias('iki')\n","        .fill_null(0)\n","    )\n","    logs = logs.with_columns(\n","        pl.col('action_time')\n","        .mean()\n","        .over('id')\n","        .alias('action_time_mean')\n","    )\n","    logs = logs.with_columns(\n","        (pl.col('iki') - pl.col('action_time'))\n","        .alias('mean_centering')\n","    )\n","    logs = logs.with_columns(\n","        pl.col('mean_centering')\n","        .cum_sum()\n","        .over('id')\n","        .alias('iki_integrated')\n","    )\n","\n","    logs = logs.select(pl.col(['id','iki_integrated']))\n","    return logs\n","\n","\n","def integrated_iki(train_logs, test_logs):\n","    print(\"integrated IKI\")    \n","    feats = []\n","\n","    for data in [train_logs, test_logs]:\n","        logs = data.clone()\n","        logs = create_integrated_iki(logs)\n","\n","        iki_stats = logs.group_by(['id']).agg(\n","                        iki_stats_count = pl.col('iki_integrated').count(),\n","                        iki_stats_mean = pl.col('iki_integrated').mean(),\n","                        iki_stats_sum = pl.col('iki_integrated').sum(),\n","                        iki_stats_std = pl.col('iki_integrated').std(),\n","                        iki_stats_max = pl.col('iki_integrated').max(),\n","                        iki_stats_min = pl.col('iki_integrated').min(),\n","                        iki_stats_median = pl.col('iki_integrated').median()\n","        )\n","        feats.append(iki_stats)\n","    return feats[0], feats[1]\n","\n","def calculate_fluctuations(iki_integrated, q, bin_sizes):\n","    Fq = np.zeros(len(bin_sizes))\n","    for i, s in enumerate(bin_sizes):\n","        segments = int(np.floor(len(iki_integrated) / s))\n","        rms = np.zeros(segments)\n","        for v in range(segments):\n","            segment = iki_integrated[v * s: (v + 1) * s]\n","            trend = np.polyfit(np.arange(s), segment, 1)  # linear fit (trend)\n","            detrended = segment - np.polyval(trend, np.arange(s))\n","            rms[v] = np.sqrt(np.mean(detrended ** 2))\n","        Fq[i] = (np.mean(rms ** q)) ** (1 / q) if q != 0 else np.exp(0.5 * np.mean(np.log(rms ** 2)))\n","    return Fq\n","\n","def mfdfla_for_series(series, q_values, bin_sizes):\n","    results = []\n","    for q in q_values:\n","        Fq_values = calculate_fluctuations(series, q, bin_sizes)\n","        results.extend(Fq_values)\n","    return results\n","\n","def process_group(series, q_values, bin_sizes):\n","    return mfdfla_for_series(series, q_values, bin_sizes)\n","\n","def calculate_selected_fluctuations_parallel(iki_integrated_df, q_values, bin_sizes, n_jobs=-1):\n","    grouped = iki_integrated_df.groupby('id')['iki_integrated']\n","    results = Parallel(n_jobs=n_jobs)(delayed(process_group)(group, q_values, bin_sizes) for name, group in grouped)\n","    feats = pd.DataFrame(results, index=[name for name, group in grouped])\n","    feats.reset_index(inplace=True)\n","    columns = ['id'] + [f'Fq_q{q}_bin{s}' for q in q_values for s in bin_sizes]\n","    feats.columns = columns\n","    return feats\n","\n","def fractal_stats(train_logs, test_logs):\n","\n","    feats = []\n","    q_values = np.linspace(-15, 15, 2)\n","    bin_sizes = [1500, 2000, 2500]\n","    for data in [train_logs, test_logs]:\n","        \n","        logs = data.clone()\n","        iki_df = create_integrated_iki(logs)\n","        iki_df = iki_df.collect().to_pandas()\n","        stats = calculate_selected_fluctuations_parallel(iki_df, q_values, bin_sizes)\n","        stats = pl.DataFrame(stats).lazy()\n","        feats.append(stats)\n","\n","    return feats[0], feats[1]\n","\n","def p_burst_feats(train_logs, test_logs, time_agg=2):\n","    print(\"< P-burst features >\")    \n","    feats=[]\n","    original_test_ids = test_logs.select('id').unique()  \n","    for logs in [train_logs, test_logs]:\n","        df=logs.clone()\n","\n","        temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n","        temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","        temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","        #temp = temp.filter(pl.col('activity').is_in(['Input']))\n","\n","        temp = temp.with_columns(pl.col('time_diff')< time_agg)\n","\n","        rle_grp = temp.with_columns(\n","            id_runs = pl.struct('time_diff','id').\n","            rle_id()).filter(pl.col('time_diff'))\n","\n","        p_burst = rle_grp.group_by(['id','id_runs']).count()\n","\n","        p_burst = p_burst.group_by(['id']).agg(\n","            p_burst_count = pl.col('count').count(),\n","            p_burst_mean = pl.col('count').mean(),\n","            p_burst_sum = pl.col('count').sum(),\n","            p_burst_std = pl.col('count').std(),\n","            p_burst_max = pl.col('count').max(),\n","            p_burst_min = pl.col('count').min(),\n","            p_burst_median = pl.col('count').median(),\n","            p_burst_skew = pl.col('count').skew(),\n","            p_burst_kurt = pl.col('count').kurtosis(),\n","            p_burst_q1 = pl.col('count').quantile(0.25),\n","            p_burst_q3 = pl.col('count').quantile(0.75),\n","\n","        )\n","        feats.append(p_burst)\n","\n","    # Check if the second dataframe (test_logs) is empty and fill with zeros if so\n","    if feats[1].collect().height == 0:\n","        zero_filled_df = original_test_ids.with_columns([pl.lit(0).alias(col) for col in feats[0].columns if col != 'id'])\n","        feats[1] = zero_filled_df\n","\n","    [feat.collect() for feat in feats]\n","    missing_cols = set(feats[0].columns) - set(feats[1].columns)\n","            \n","    for col in missing_cols:\n","        zero_series = pl.repeat(0, n=len(feats[1])).alias(col)\n","        feats[1] = feats[1].with_columns(zero_series)\n","\n","    return feats[0], feats[1]\n","\n","def r_burst_feats(train_logs, test_logs):\n","    print(\"< R-burst features >\")    \n","    feats = []\n","    tr_ids = pl.DataFrame({'id': train_logs.select(pl.col('id')).unique().collect().to_series().to_list()})\n","    ts_ids = pl.DataFrame({'id': test_logs.select(pl.col('id')).unique().collect().to_series().to_list()})\n","\n","\n","    for logs in [train_logs, test_logs]:\n","        df = logs.clone()\n","        temp = df.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n","        rle_grp = temp.with_columns(\n","            id_runs = pl.struct('activity', 'id').rle_id()\n","        ).filter(pl.col('activity'))\n","\n","        r_burst = rle_grp.group_by(['id', 'id_runs']).count()\n","        r_burst = r_burst.group_by(['id']).agg(\n","            r_burst_count = pl.col('count').count(),\n","            r_burst_mean = pl.col('count').mean(),\n","            r_burst_sum = pl.col('count').sum(),\n","            r_burst_std = pl.col('count').std(),\n","            r_burst_max = pl.col('count').max(),\n","            r_burst_min = pl.col('count').min(),\n","            r_burst_median = pl.col('count').median()\n","        )\n","        feats.append(r_burst.collect())\n","    feats[0] = tr_ids.join(feats[0], on='id', how='left').fill_null(0)\n","    feats[1] = ts_ids.join(feats[1], on='id', how='left').fill_null(0)\n","    return feats[0].lazy(), feats[1].lazy()\n","\n","def q1(x):\n","    return x.quantile(0.25)\n","def q3(x):\n","    return x.quantile(0.75)\n","\n","AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n","\n","def reconstruct_essay(currTextInput):\n","    essayText = \"\"\n","    for Input in currTextInput.values:\n","        if Input[0] == 'Replace':\n","            replaceTxt = Input[2].split(' => ')\n","            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","            continue\n","        if Input[0] == 'Paste':\n","            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","            continue\n","        if Input[0] == 'Remove/Cut':\n","            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n","            continue\n","        if \"M\" in Input[0]:\n","            croppedTxt = Input[0][10:]\n","            splitTxt = croppedTxt.split(' To ')\n","            valueArr = [item.split(', ') for item in splitTxt]\n","            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","            if moveData[0] != moveData[2]:\n","                if moveData[0] < moveData[2]:\n","                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n","                else:\n","                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n","            continue\n","        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n","    return essayText\n","\n","\n","def get_essay_df(df):\n","    df       = df[df.activity != 'Nonproduction']\n","    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n","    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n","    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n","    return essay_df\n","\n","\n","def word_feats(df):\n","    print(\"< Essays word feats >\")    \n","    essay_df = df\n","    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('word')\n","    df['word_len'] = df['word'].apply(lambda x: len(x))\n","    df = df[df['word_len'] != 0]\n","    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n","    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n","    word_agg_df['id'] = word_agg_df.index\n","    word_agg_df = word_agg_df.reset_index(drop=True)\n","    return word_agg_df\n","\n","\n","def sent_feats(df):\n","    print(\"< Essays sentences feats >\")    \n","    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('sent')\n","    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n","    # Number of characters in sentences\n","    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.sent_len!=0].reset_index(drop=True)\n","\n","    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n","                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n","    return sent_agg_df\n","\n","\n","def parag_feats(df):\n","    print(\"< Essays paragraphs feats >\")    \n","    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n","    df = df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n","    # Number of words in paragraphs\n","    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.paragraph_len!=0].reset_index(drop=True)\n","    \n","    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n","                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return paragraph_agg_df\n","\n","def product_to_keys(logs, essays):\n","    print('< product to keys >')\n","    feats = []\n","    for log, essay in zip(logs, essays):\n","        essay['product_len'] = essay.essay.str.len()\n","        tmp_df = log[log.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n","        essay = essay.merge(tmp_df, on='id', how='left')\n","        essay['product_to_keys'] = essay['product_len'] / essay['keys_pressed']\n","        feats.append(essay[['id', 'product_to_keys']])\n","        \n","    tr_feats = pl.DataFrame(feats[0]).lazy()\n","    ts_feats = pl.DataFrame(feats[1]).lazy()\n","    return tr_feats, ts_feats\n","    \n","\n","def get_keys_pressed_per_second(train_logs, test_logs):\n","    print('< get keys pressed per second >')\n","    feats = []\n","    for data in [train_logs, test_logs]:\n","        logs = data.copy()\n","        temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n","        temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n","        temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n","        temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n","        feats.append(temp_df[['id', 'keys_per_second']])\n","\n","    tr_feats = pl.DataFrame(feats[0]).lazy()\n","    ts_feats = pl.DataFrame(feats[1]).lazy()\n","    return tr_feats, ts_feats\n","\n","def create_pauses(train_logs, test_logs):\n","    print(\"< Idle time features >\")\n","    feats = []\n","    for logs in [train_logs, test_logs]:\n","        df = logs.clone()\n","        temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n","        temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","        temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","        temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n","                                        inter_key_median_lantency = pl.median('time_diff'),\n","                                        mean_pause_time = pl.mean('time_diff'),\n","                                        std_pause_time = pl.std('time_diff'),\n","                                        total_pause_time = pl.sum('time_diff'),\n","                                        pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n","                                        pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n","                                        pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n","                                        pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n","                                        pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n","        feats.append(temp)\n","    return feats[0], feats[1]\n","\n","def essay_sents_per_par(df):\n","    AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3]\n","    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n","    df = df.explode('paragraph')\n","    df = df[df['paragraph'].str.strip() != '']\n","    df['sent_per_par'] = df['paragraph'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n","    df = df.explode('sent_per_par')\n","    df = df[df['sent_per_par'].str.strip() != '']\n","    df['sent_per_par'] = df['sent_per_par'].apply(lambda x: x.replace('\\n','').strip())\n","    df = df.groupby(['id','paragraph'])['sent_per_par'].count().reset_index()\n","    df = df[df['paragraph'].str.strip() != ''].drop('paragraph', axis=1)\n","\n","    par_sent_df = df[['id','sent_per_par']].groupby(['id']).agg(AGGREGATIONS)\n","    par_sent_df.columns = ['_'.join(x) for x in par_sent_df.columns]\n","    par_sent_df['id'] = par_sent_df.index\n","    par_sent_df = par_sent_df.reset_index(drop=True)\n","    par_sent_df = par_sent_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n","    return par_sent_df\n","\n","def add_word_pauses(train_logs, test_logs):\n","    print(\"< added words pauses basic\")    \n","    feats = []\n","\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","\n","    for data in [tr_logs, ts_logs]:\n","        logs = data.clone()\n","        logs = logs.select(pl.col(['id','event_id','word_count','down_time','up_time','action_time']))\n","        logs = logs.with_columns(pl.col('word_count')\n","                    .diff()\n","                    .over('id')\n","                    .fill_null(1)\n","                    .alias('word_diff'))\n","\n","        logs = logs.with_columns(pl.col('down_time')\n","                    .diff()\n","                    .over('id','word_count')\n","                    .fill_null(0)\n","                    .alias('down_time_diff')) \n","\n","        word_pause = logs.filter(pl.col('word_diff')>0)\n","        word_pause = word_pause.group_by(['id']).agg(\n","                add_words_pause_count = pl.col('down_time_diff').count(),\n","                add_words_pause_mean = pl.col('down_time_diff').mean(),\n","                add_words_pause_sum = pl.col('down_time_diff').sum(),\n","                add_words_pause_std = pl.col('down_time_diff').std(),\n","                add_words_pause_median = pl.col('down_time_diff').median(),\n","                add_words_pause_max = pl.col('down_time_diff').max(),\n","                add_words_pause_q1 = pl.col('down_time_diff').quantile(0.25),\n","                add_words_pause_q3 = pl.col('down_time_diff').quantile(0.75),\n","                add_words_pause_kurt = pl.col('down_time_diff').kurtosis(),\n","                add_words_pause_skew = pl.col('down_time_diff').skew(),\n","        )\n","        feats.append(word_pause)\n","    return feats[0], feats[1]\n","\n","def remove_word_pauses(train_logs, test_logs):\n","    print(\"< removed words pauses basic\")    \n","    feats = []\n","\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","\n","    for data in [tr_logs, ts_logs]:\n","        logs = data.clone()\n","        logs = logs.select(pl.col(['id','event_id','word_count','down_time','up_time','action_time']))\n","        logs = logs.with_columns(pl.col('word_count')\n","                    .diff()\n","                    .over('id')\n","                    .fill_null(1)\n","                    .alias('word_diff'))\n","\n","        logs = logs.with_columns(pl.col('down_time')\n","                    .diff()\n","                    .over('id')\n","                    .fill_null(0)\n","                    .alias('down_time_diff')) \n","\n","        word_pause = logs.filter(pl.col('word_diff')<0)\n","        word_pause = word_pause.group_by(['id']).agg(\n","                rmv_words_pause_count = pl.col('down_time_diff').count(),\n","                rmv_words_pause_mean = pl.col('down_time_diff').mean(),\n","                rmv_words_pause_sum = pl.col('down_time_diff').sum(),\n","                rmv_words_pause_std = pl.col('down_time_diff').std(),\n","                rmv_words_pause_median = pl.col('down_time_diff').median(),\n","                rmv_words_pause_max = pl.col('down_time_diff').max(),\n","                rmv_words_pause_q1 = pl.col('down_time_diff').quantile(0.25),\n","                rmv_words_pause_q3 = pl.col('down_time_diff').quantile(0.75),\n","                rmv_words_pause_kurt = pl.col('down_time_diff').kurtosis(),\n","                rmv_words_pause_skew = pl.col('down_time_diff').skew(),\n","        )\n","        feats.append(word_pause)\n","    return feats[0], feats[1]\n","\n","def word_timings(train_logs, test_logs):\n","    print(\"< word timings advanced\")    \n","    feats = []\n","    tr_logs, ts_logs = normalise_up_down_times(train_logs, test_logs)\n","    for data in [tr_logs, ts_logs]:\n","\n","        logs = data.clone()\n","        logs = logs.sort(['id', 'event_id'])\n","        logs = logs.select(pl.col(['id','event_id','word_count','down_time','up_time','action_time']))\n","        logs = logs.with_columns(\n","            pl.cum_sum('action_time')\n","            .over(['id','word_count'])\n","            .alias('cum_sum_action_time_per_word')\n","            )\n","\n","        logs = logs.group_by(['id','word_count']).agg(\n","            pl.max('cum_sum_action_time_per_word')\n","            .alias('time_per_word'))\n","\n","        word_timings = logs.group_by(['id']).agg(\n","            word_timings_mean = pl.col('time_per_word').mean(),\n","            word_timings_sum = pl.col('time_per_word').sum(),\n","            word_timings_std = pl.col('time_per_word').std(),\n","            word_timings_median = pl.col('time_per_word').median(),\n","            words_timings_max = pl.col('time_per_word').max(),\n","            words_timings_q1 = pl.col('time_per_word').quantile(0.25),\n","            words_timings_q3 = pl.col('time_per_word').quantile(0.75),\n","            words_timings_kurt = pl.col('time_per_word').kurtosis(),\n","            words_timings_skew = pl.col('time_per_word').skew(),\n","        )\n","        feats.append(word_timings)\n","    return feats[0], feats[1]\n","\n","def word_wait_shift(train_logs, test_logs, shift=1):\n","    print('< word_wait_shift >')\n","    feats = []\n","    for data in [train_logs,test_logs]:\n","        logs = data.clone()\n","        logs = logs.group_by('id','word_count').agg(\n","            word_start_time = pl.col('down_time').min()).sort('id','word_count')\n","\n","        logs = logs.with_columns(pl.col('word_start_time').shift(shift).over('id').alias(f'shifted'))\n","        logs = logs.with_columns((pl.col('word_start_time') - pl.col('shifted')).alias(f'word_time_diff_{shift}'))\n","\n","        words_shifted = logs.group_by('id').agg(\n","                        pl.col(f'word_time_diff_{shift}').count().name.suffix('count'),\n","                        pl.col(f'word_time_diff_{shift}').mean().name.suffix('mean'),\n","                        pl.col(f'word_time_diff_{shift}').sum().name.suffix('sum'),\n","                        pl.col(f'word_time_diff_{shift}').std().name.suffix('std'),\n","                        pl.col(f'word_time_diff_{shift}').median().name.suffix('median'),\n","                        pl.col(f'word_time_diff_{shift}').max().name.suffix('max'),\n","                        pl.col(f'word_time_diff_{shift}').quantile(0.25).name.suffix('quantile_25'),\n","                        pl.col(f'word_time_diff_{shift}').quantile(0.75).name.suffix('quantile_75'),\n","                        pl.col(f'word_time_diff_{shift}').kurtosis().name.suffix('kurt'),\n","                        pl.col(f'word_time_diff_{shift}').skew().name.suffix('skew'),\n","        )\n","        feats.append(words_shifted)\n","    return feats[0], feats[1]\n","\n","def words_p_burst(train_logs, test_logs, time_agg=650):\n","\n","    logs = pl.concat([train_logs, test_logs], how = 'vertical')\n","\n","    all_ids = pl.DataFrame({'id': logs.select(pl.col('id')).unique().collect().to_series().to_list()}).lazy()\n","    tr_ids = train_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","    ts_ids = test_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","\n","    logs = logs.sort(['id', 'event_id'])\n","    logs = logs.select(pl.col(['id','event_id','word_count','down_time','up_time','action_time']))\n","\n","    logs = logs.with_columns(\n","        pl.col('down_time')\n","        .diff()\n","        .over('id')\n","        .fill_null(0)\n","        .alias('down_time_diff'))\n","\n","    logs = logs.with_columns(\n","        pl.cum_sum('down_time_diff')\n","        .over(['id','word_count'])\n","        .alias('time_per_word')\n","        )\n","\n","    logs = logs.group_by(['id','word_count']).agg(\n","        pl.last('time_per_word')).sort(['id','word_count'])\n","\n","    temp = logs.with_columns(pl.col('time_per_word') < time_agg)\n","\n","    rle_grp = temp.with_columns(\n","        id_runs = pl.struct('time_per_word','id').\n","        rle_id()).filter(pl.col('time_per_word'))\n","\n","    p_burst = rle_grp.group_by(['id','id_runs']).count()\n","\n","    p_burst = p_burst.group_by(['id']).agg(\n","        p_burst_count = pl.col('count').count(),\n","        p_burst_mean = pl.col('count').mean(),\n","        p_burst_sum = pl.col('count').sum(),\n","        p_burst_std = pl.col('count').std(),\n","        p_burst_max = pl.col('count').max(),\n","        p_burst_median = pl.col('count').median(),\n","        p_burst_skew = pl.col('count').skew(),\n","        p_burst_kurt = pl.col('count').kurtosis(),\n","        p_burst_q3 = pl.col('count').quantile(0.75),\n","\n","    )\n","\n","    p_burst = all_ids.join(p_burst, on='id', how='left')\n","\n","    tr = p_burst.filter(pl.col('id').is_in(tr_ids))\n","    ts = p_burst.filter(pl.col('id').is_in(ts_ids))\n","    return tr, ts\n","\n","def text_changes_counts(train_logs, test_logs):\n","    print(\"< text chaanges counts features >\")\n","    text_changes = ['\\n', ':', 'NoChange', '/', ' ', ';', '\\\\', '=']\n","    logs = pl.concat([train_logs, test_logs], how = 'vertical')\n","\n","    all_ids = pl.DataFrame({'id': logs.select(pl.col('id')).unique().collect().to_series().to_list()})\n","    tr_ids = train_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","    ts_ids = test_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","\n","    data = logs.clone()\n","    text_changes_stats = (data\n","                    .filter(pl.col('text_change').is_in(text_changes))\n","                    .group_by(['id', 'text_change'])\n","                    .agg(pl.count()).collect()\n","                    .pivot(values='count', index='id', columns='text_change')\n","                    ).fill_null(0)\n","\n","    text_changes_stats = all_ids.join(text_changes_stats,on='id',how='left')\n","    text_changes_stats = text_changes_stats.fill_null(0)  \n","\n","    # Rename columns to a generic format\n","    cols = text_changes_stats.columns[1:]  # Skip the 'id' column\n","    for i, col in enumerate(cols):\n","        text_changes_stats = text_changes_stats.rename({col: f'text_change{i+1}'})\n","\n","    tr_feats = text_changes_stats.filter(pl.col('id').is_in(tr_ids))\n","    ts_feats = text_changes_stats.filter(pl.col('id').is_in(ts_ids))\n","\n","    return tr_feats.lazy(), ts_feats.lazy()\n","\n","\n","def punctuations(train_logs, test_logs):\n","    print(\"< punctuations features >\")\n","    feats = []\n","    logs = pl.concat([train_logs, test_logs], how = 'vertical')\n","    punctuations = [':', '#', '%', '<', ')', '>', '+', '/', '(', '^', '_', ';', '@', '!', '$', '&', '*']\n","    all_ids = pl.DataFrame({'id': logs.select(pl.col('id')).unique().collect().to_series().to_list()})\n","    tr_ids = train_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","    ts_ids = test_logs.select(pl.col('id')).unique().collect().to_series().to_list()\n","\n","    data = logs.clone()\n","    event_stats = (data\n","                    .filter(pl.col('down_event').is_in(punctuations))\n","                    .group_by(['id', 'down_event'])\n","                    .agg(pl.count()).collect()\n","                    .pivot(values='count', index='id', columns='down_event')\n","                    ).fill_null(0)\n","\n","    event_stats = all_ids.join(event_stats,on='id',how='left')\n","    event_stats = event_stats.fill_null(0)\n","\n","    # Rename columns to a generic format\n","    cols = event_stats.columns[1:]  # Skip the 'id' column\n","    for i, col in enumerate(cols):\n","        event_stats = event_stats.rename({col: f'punctuation_{i+1}'})\n","\n","    tr_feats = event_stats.filter(pl.col('id').is_in(tr_ids))\n","    ts_feats = event_stats.filter(pl.col('id').is_in(ts_ids))\n","\n","    return tr_feats.lazy(), ts_feats.lazy()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T01:54:18.577261Z","iopub.status.busy":"2024-01-01T01:54:18.576884Z","iopub.status.idle":"2024-01-01T01:54:18.594782Z","shell.execute_reply":"2024-01-01T01:54:18.593460Z","shell.execute_reply.started":"2024-01-01T01:54:18.577227Z"},"trusted":true},"outputs":[],"source":["lgb_params = {\n","    'boosting_type': 'gbdt', \n","    'metric': 'rmse',\n","    'reg_alpha': 0.0031, \n","    'reg_lambda': 0.001, \n","    'colsample_bytree': 0.8,  \n","    'subsample_freq': 1,  \n","    'subsample': 0.75,  \n","    'learning_rate': 0.017, \n","    'num_leaves': 19, \n","    'min_child_samples': 46,\n","    'n_estimators': 350,\n","    'verbosity': -1\n","    }\n","\n","xgb_params = {\n","    'alpha': 1,\n","    'colsample_bytree': 0.8,\n","    'gamma': 1.5,\n","    'learning_rate': 0.05,\n","    'max_depth': 4,\n","    'min_child_weight': 10,\n","    'subsample': 0.8,\n","    'device': 'cuda',\n","    'n_estimators': 350 \n","    }\n","\n","catboost_params = {\n","    'iterations': 1000, \n","    'learning_rate': 0.1, \n","    'depth': 6, \n","    'loss_function': 'RMSE', \n","    'od_wait': 20, \n","    'od_type': 'Iter', \n","    'verbose': False, \n","    'metric_period': 50, \n","    'eval_metric': 'RMSE', \n","    'bagging_temperature': 0.2\n","}\n","\n","svr_params = {\n","    'C': 1.0, \n","    'cache_size': 200, \n","    'coef0': 0.0, \n","    'degree': 3, \n","    'epsilon': 0.1, \n","    'gamma': 'scale', \n","    'kernel': 'rbf', \n","    'max_iter': -1, \n","    'shrinking': True, \n","    'tol': 0.001, \n","    'verbose': False}\n","\n","ridge_params = {'alpha':110}\n","\n","data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n","train_logs    = pl.scan_csv(f'{data_path}/train_logs.csv')\n","test_logs    = pl.scan_csv(f'{data_path}/test_logs.csv')\n","train_scores = pl.scan_csv(f'{data_path}/train_scores.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T01:54:18.600448Z","iopub.status.busy":"2024-01-01T01:54:18.599074Z","iopub.status.idle":"2024-01-01T01:55:48.032489Z","shell.execute_reply":"2024-01-01T01:55:48.031438Z","shell.execute_reply.started":"2024-01-01T01:54:18.600397Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["< Events counts features >\n","< Count vectorize one-grams >\n","< Idle time features >\n","< cursor position acceleration >\n","< R-burst features >\n","< Categorical # unique values features >\n","< event_id rate of change >\n","< Word counts rate of change features >\n","< removed words pauses basic\n","< Count vectorize bi-grams >\n","< word_wait_shift >\n","< Essays paragraphs feats >\n","< Essays paragraphs feats >\n","< Essays sentences feats >\n","< Essays sentences feats >\n","< Essays word feats >\n","< Essays word feats >\n","train feats shape (2471, 177)\n"]}],"source":["# PANDAS FEATS\n","train_essays          = get_essay_df(train_logs.collect().to_pandas())\n","test_essays           = get_essay_df(test_logs.collect().to_pandas())\n","\n","tr_down_events_counts, ts_down_events_counts = down_events_counts(train_logs, test_logs)\n","tr_vect_one, ts_vect_one = countvectorize_one_one(train_essays, test_essays)\n","tr_pauses, ts_pauses = create_pauses(train_logs, test_logs)\n","tr_cursor_pos_acc, ts_cursor_pos_acc = cursor_pos_acceleration(train_logs, test_logs)\n","tr_r_burst, ts_r_burst = r_burst_feats(train_logs, test_logs)\n","tr_nuni, ts_nuni = categorical_nunique(train_logs, test_logs)\n","tr_e_counts_roc, ts_e_counts_roc = events_counts_rate_of_change(train_logs, test_logs, time_agg=3)\n","tr_wc_roc, ts_wc_roc = word_counts_rate_of_change(train_logs, test_logs)\n","tr_remove_pause, ts_remove_pause = remove_word_pauses(train_logs, test_logs)\n","tr_vect_two, ts_vect_two = countvectorize_two_one(train_essays, test_essays)\n","tr_word_wait, ts_word_wait = word_wait_shift(train_logs, test_logs)\n","\n","train_feats = tr_down_events_counts.join(tr_vect_one, on='id', how='left')\n","train_feats = train_feats.join(tr_pauses, on='id', how='left')\n","train_feats = train_feats.join(tr_cursor_pos_acc, on='id', how='left')\n","train_feats = train_feats.join(tr_r_burst, on='id', how='left')\n","train_feats = train_feats.join(tr_nuni, on='id', how='left')\n","train_feats = train_feats.join(tr_e_counts_roc, on='id', how='left')\n","train_feats = train_feats.join(tr_wc_roc, on='id', how='left')\n","train_feats = train_feats.join(tr_remove_pause, on='id', how='left')\n","train_feats = train_feats.join(tr_vect_two, on='id', how='left')\n","train_feats = train_feats.join(tr_word_wait, on='id', how='left')\n","\n","test_feats = ts_down_events_counts.join(ts_vect_one, on='id', how='left')\n","test_feats = test_feats.join(ts_pauses, on='id', how='left')\n","test_feats = test_feats.join(ts_cursor_pos_acc, on='id', how='left')\n","test_feats = test_feats.join(ts_r_burst, on='id', how='left')\n","test_feats = test_feats.join(ts_nuni, on='id', how='left')\n","test_feats = test_feats.join(ts_e_counts_roc, on='id', how='left')\n","test_feats = test_feats.join(ts_wc_roc, on='id', how='left')\n","test_feats = test_feats.join(ts_remove_pause, on='id', how='left')\n","test_feats = test_feats.join(ts_vect_two, on='id', how='left')\n","test_feats = test_feats.join(ts_word_wait, on='id', how='left')\n","\n","train_logs = train_logs.collect().to_pandas()\n","test_logs = test_logs.collect().to_pandas()\n","train_scores = train_scores.collect().to_pandas()\n","train_feats = train_feats.collect().to_pandas()\n","test_feats = test_feats.collect().to_pandas()\n","\n","train_feats           = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n","train_feats           = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n","train_feats           = train_feats.merge(word_feats(train_essays), on='id', how='left')\n","test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n","\n","train_feats           = train_feats.merge(train_scores, on=['id'], how='left')\n","print(f'train feats shape {train_feats.shape}')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T01:55:48.033930Z","iopub.status.busy":"2024-01-01T01:55:48.033636Z","iopub.status.idle":"2024-01-01T02:13:21.396885Z","shell.execute_reply":"2024-01-01T02:13:21.395679Z","shell.execute_reply.started":"2024-01-01T01:55:48.033903Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Final RMSE over 50: 0.606351. Std 0.8259\n","LGBM completed\n","Final RMSE over 50: 0.606361. Std 0.8245\n","XGB completed\n","Final RMSE over 50: 0.622354. Std 0.8438\n","Catboost completed\n"]}],"source":["test_preds_lgbm, valid_preds_lgbm, final_rmse_lgbm, model_lgbm = lgb_pipeline(train_feats, test_feats, lgb_params)\n","print(f'LGBM completed')\n","test_preds_xgb, valid_preds_xgb, final_rmse_xgb, model_xgb = xgb_pipeline(train_feats, test_feats, xgb_params)\n","print(f'XGB completed')\n","test_preds_cat, valid_preds_cat, final_rmse_cat, model_cat = catboost_pipeline(train_feats, test_feats, catboost_params)\n","print(f'Catboost completed')\n","test_preds_svr, valid_preds_svr, final_rmse_svr, model_svr = svr_pipeline(train_feats, test_feats)\n","print(f'SVR completed')\n","test_preds_ridge, valid_preds_ridge, final_rmse_ridge, model_ridge = ridge_pipeline(train_feats, test_feats, ridge_params)\n","print(f'Ridge completed')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T02:13:21.398747Z","iopub.status.busy":"2024-01-01T02:13:21.398398Z","iopub.status.idle":"2024-01-01T02:13:21.482416Z","shell.execute_reply":"2024-01-01T02:13:21.481184Z","shell.execute_reply.started":"2024-01-01T02:13:21.398715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Baseline RMSE with simple average: 0.602079556990036\n","Best RMSE: inf\n","Best Model Combination: None\n","Best Weights: None\n"]}],"source":["import numpy as np\n","import itertools\n","from sklearn.metrics import mean_squared_error\n","\n","models = {\n","    'xgboost': average_model_predictions(valid_preds_xgb),\n","    'lgbm': average_model_predictions(valid_preds_lgbm),\n","    'catboost': average_model_predictions(valid_preds_cat),\n","    'ridge': average_model_predictions(valid_preds_ridge),\n","    'svr': average_model_predictions(valid_preds_svr),\n","}\n","\n","true_values = train_scores.score.values\n","\n","simple_avg = np.mean(list(models.values()), axis=0)\n","baseline_rmse = mean_squared_error(true_values, simple_avg, squared=False)\n","print(f\"Baseline RMSE with simple average: {baseline_rmse}\")\n","\n","for L in range(1, len(models) + 1):\n","    for subset in itertools.combinations(models, L):\n","        model_subset = {model: models[model] for model in subset}\n","\n","        for weights in itertools.product(np.linspace(0.5, 1.0, 10), repeat=len(subset)):\n","            weighted_avg = calculate_weighted_avg(weights, model_subset)\n","            rmse = mean_squared_error(true_values, weighted_avg, squared=False)\n","            if rmse < best_rmse:\n","                best_rmse = rmse\n","                best_combination = subset\n","                best_weights = weights\n","\n","print(f\"Best RMSE: {best_rmse}\")\n","print(f\"Best Model Combination: {best_combination}\")\n","print(f\"Best Weights: {best_weights}\")\n","\n","# Averaging test predictions for each model\n","avg_test_preds_lgbm = average_test_predictions(test_preds_lgbm)\n","avg_test_preds_xgb = average_test_predictions(test_preds_xgb)\n","avg_test_preds_cat = average_test_predictions(test_preds_cat)\n","avg_test_preds_svr = average_test_predictions(test_preds_svr)\n","avg_test_preds_ridge = average_test_predictions(test_preds_ridge)\n","\n","# Dictionary of averaged test predictions\n","test_predictions = {\n","    'xgboost': avg_test_preds_xgb,\n","    'lgbm': avg_test_preds_lgbm,\n","    'catboost': avg_test_preds_cat,\n","    'svr': avg_test_preds_svr,\n","    'ridge': avg_test_preds_ridge\n","}\n","\n","blended_test_predictions = calculate_weighted_avg_for_test(best_weights, test_predictions)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T02:13:21.484418Z","iopub.status.busy":"2024-01-01T02:13:21.483914Z","iopub.status.idle":"2024-01-01T02:13:21.496983Z","shell.execute_reply":"2024-01-01T02:13:21.495591Z","shell.execute_reply.started":"2024-01-01T02:13:21.484372Z"},"trusted":true},"outputs":[],"source":["test_ids = test_feats.id\n","preds_simple_avg = np.mean(list(test_predictions.values()), axis=0)\n","sub = pd.DataFrame({'id': test_ids, 'score': preds_simple_avg})\n","sub.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-01T02:13:21.499832Z","iopub.status.busy":"2024-01-01T02:13:21.498986Z","iopub.status.idle":"2024-01-01T02:13:21.517594Z","shell.execute_reply":"2024-01-01T02:13:21.516189Z","shell.execute_reply.started":"2024-01-01T02:13:21.499787Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2222bbbb</td>\n","      <td>1.250472</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4444cccc</td>\n","      <td>1.351368</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0000aaaa</td>\n","      <td>1.699218</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id     score\n","0  2222bbbb  1.250472\n","1  4444cccc  1.351368\n","2  0000aaaa  1.699218"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["sub"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
